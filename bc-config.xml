<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<dict>
    <entry key="add_primary_key_to_header">
        <int>1</int>
    </entry>
    <entry key="blockstrategy">
        <dict>
            <entry key="mininterblockinterval">
                <int>1000</int>
            </entry>
            <entry key="name">
                <string>net.postchain.base.BaseBlockBuildingStrategy</string>
            </entry>
        </dict>
    </entry>
    <entry key="config_consensus_strategy">
        <string>HEADER_HASH</string>
    </entry>
    <entry key="configurationfactory">
        <string>net.postchain.gtx.GTXBlockchainConfigurationFactory</string>
    </entry>
    <entry key="gtx">
        <dict>
            <entry key="modules">
                <array>
                    <string>net.postchain.rell.module.RellPostchainModuleFactory</string>
                    <string>net.postchain.gtx.StandardOpsGTXModule</string>
                    <string>net.postchain.d1.icmf.IcmfSenderGTXModule</string>
                    <string>net.postchain.d1.icmf.IcmfReceiverGTXModule</string>
                    <string>net.postchain.eif.transaction.signerupdate.directorychain.SignerUpdateGTXModule</string>
                </array>
            </entry>
            <entry key="rell">
                <dict>
                    <entry key="compilerVersion">
                        <string>0.13.13</string>
                    </entry>
                    <entry key="moduleArgs">
                        <dict>
                            <entry key="common">
                                <dict>
                                    <entry key="allow_blockchain_dependencies">
                                        <int>0</int>
                                    </entry>
                                    <entry key="provider_quota_max_actions_per_day">
                                        <int>100</int>
                                    </entry>
                                </dict>
                            </entry>
                            <entry key="common.init">
                                <dict>
                                    <entry key="genesis_node">
                                        <array>
                                            <string>037434C8D4F2B7B7DE44E80486A814676DC3D898FD4488E10E1940B1C4C5837200</string>
                                            <string>system.chromaway.com</string>
                                            <int>9870</int>
                                            <string>https://system.chromaway.com:7740</string>
                                            <string>SE</string>
                                        </array>
                                    </entry>
                                    <entry key="initial_provider">
                                        <string>0319852651DB3ACA5D5DFDF71D8600566345FE1713BCBC7F2C1B6434B7A8C88C6F</string>
                                    </entry>
                                </dict>
                            </entry>
                            <entry key="proposal_blockchain.util">
                                <dict>
                                    <entry key="allowed_dapp_chain_gtx_modules">
                                        <array>
                                            <string>net.postchain.rell.module.RellPostchainModuleFactory</string>
                                            <string>net.postchain.gtx.StandardOpsGTXModule</string>
                                            <string>net.postchain.d1.icmf.IcmfSenderGTXModule</string>
                                            <string>net.postchain.d1.icmf.IcmfReceiverGTXModule</string>
                                            <string>net.postchain.d1.iccf.IccfGTXModule</string>
                                            <string>net.postchain.eif.EifGTXModule</string>
                                        </array>
                                    </entry>
                                    <entry key="allowed_dapp_chain_sync_exts">
                                        <array>
                                            <string>net.postchain.d1.icmf.IcmfReceiverSynchronizationInfrastructureExtension</string>
                                            <string>net.postchain.eif.EifSynchronizationInfrastructureExtension</string>
                                        </array>
                                    </entry>
                                    <entry key="max_block_size">
                                        <int>27262976</int>
                                    </entry>
                                    <entry key="max_config_path_depth">
                                        <int>10</int>
                                    </entry>
                                    <entry key="max_config_size">
                                        <int>5242880</int>
                                    </entry>
                                    <entry key="min_fast_revolt_status_timeout">
                                        <int>2000</int>
                                    </entry>
                                    <entry key="min_inter_block_interval">
                                        <int>1000</int>
                                    </entry>
                                </dict>
                            </entry>
                        </dict>
                    </entry>
                    <entry key="modules">
                        <array>
                            <string>management_chain_mainnet</string>
                        </array>
                    </entry>
                    <entry key="sources">
                        <dict>
                            <entry key="cm_api/module.rell">
                                <string>module;

import common.*;
import model.*;

/**
 * Cluster Management api used by postchain to get information about cluster anchoring chains
 */

struct cm_peer_info {
    pubkey: pubkey;
    api_url: text;
}

struct cm_cluster_info {
    name;
    anchoring_chain: byte_array;
    peers: list&lt;cm_peer_info&gt;;
}

query cm_get_cluster_info(name): cm_cluster_info {
    val cluster = require_cluster(name);
    val cac = require(cluster_anchoring_chain @? { cluster }, "Cluster anchoring chain not found for cluster " + name);
    val cluster_peer = cluster_node @* {
        cluster_node.cluster == cluster,
        cluster.name == name
    } (
        peer = .node.pubkey,
        peer_api_url = .node.api_url
    ); 
    val peers = list&lt;cm_peer_info&gt;();
    for (cp in cluster_peer) {
        peers.add(cm_peer_info(pubkey = cp.peer, api_url = cp.peer_api_url));
    }
    return cm_cluster_info(
        name = cluster.name,
        anchoring_chain = cac.blockchain.rid,
        peers = peers
    );
}

query cm_get_cluster_names(): list&lt;text&gt; {
    return cluster @* { .operational == true } ( .name );
}

query cm_get_cluster_blockchains(name): list&lt;byte_array&gt; {
    return (container_blockchain) @* { .container.cluster.name == name } ( .blockchain.rid );
}

// Returns signers of blockchain at specific height
// NB: doesn't take into account pending configurations
query cm_get_peer_info(brid: byte_array, height: integer): set&lt;pubkey&gt; {
    val bc = require_blockchain(brid);
    return set&lt;pubkey&gt;.from_gtv(
        get_signers_for_configuration(bc, height)
    );
}

query cm_get_blockchain_cluster(brid: byte_array): text {
    if (brid == chain_context.blockchain_rid) return clusters.system; // I am chain0

    val bc = require_blockchain(brid);
    return container_blockchain @ { bc } ( .container.cluster.name );
}

query cm_get_blockchain_api_urls(blockchain_rid: byte_array) {
    return (blockchain, container_blockchain, cluster_node) @*
    {
        blockchain.rid == blockchain_rid,
        container_blockchain.blockchain.rid == blockchain.rid,
        container_blockchain.container.cluster == cluster_node.cluster
    } (
        cluster_node.node.api_url
    );
}

query cm_get_cluster_anchoring_chains(): list&lt;byte_array&gt; {
    return cluster_anchoring_chain @* {} ( .blockchain.rid );
}

query cm_get_system_anchoring_chain(): byte_array? {
    if (system_anchoring_chain.rid.empty()) return null;

    return system_anchoring_chain.rid;
}

query cm_get_system_chains(): list&lt;byte_array&gt; {
    return blockchain @* { .system }.rid;
}</string>
                            </entry>
                            <entry key="common/anchoring_cluster.rell">
                                <string>
function set_cluster_anchoring_config(config: byte_array) {
    map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(config)); // Validate that config is a map
    cluster_anchoring_config.raw_config = config;
}

@extend(after_cluster_operational) function activate_cluster_anchoring_chain(cluster) {
    val config_map = map&lt;text,gtv&gt;.from_gtv(gtv.from_bytes(cluster_anchoring_config.raw_config));

    // feature toggle
    if (not(exists(cluster_anchoring_chain @? { cluster } (.blockchain))) and config_map.size() &gt; 0) {
        require(empty(cluster_anchoring_chain @? { cluster }), "Anchoring chain already exists for cluster " + cluster.name);

        val system_container = container @ { .name == system_container_name(cluster.name), cluster };
        val cluster_signers = cluster_node @* { cluster } (@sort .node.pubkey);
        val blockchain_name = blockchains.cluster_anchoring_prefix + cluster.name;
        config_map["cluster"] = cluster.name.to_gtv();
        val unique_config = config_map.to_gtv_pretty();

        val blockchain = add_blockchain(unique_config.to_bytes(), cluster_signers, blockchain_name, system_container, system = true, state = blockchain_state.RUNNING);
        
        create cluster_anchoring_chain(blockchain, cluster);

        if (cluster.name != clusters.system) {
            // Add replication of this chain to all system nodes
            val system_cluster_nodes = cluster_node @* { .cluster.name == clusters.system  } ( .node );
            for (node in system_cluster_nodes) {
                create blockchain_replica_node(blockchain, node);
            }
        }
    }

    // Trigger after cluster update no matter if we added anchoring or not
    after_cluster_updated(cluster);
}

@extend(before_system_container_removal) function remove_cluster_anchoring_chain(cluster) {
    val anchor = cluster_anchoring_chain @? { cluster } (.blockchain);
    if (exists(anchor)) {
        anchor.state = blockchain_state.REMOVED;
        delete container_blockchain @* { anchor };
        delete cluster_anchoring_chain @ { anchor };
        delete blockchain_replica_node @* { anchor };
        create inactive_blockchain(anchor, op_context.block_height);
    }
}

@extend(after_node_added_to_cluster) function replicate_cluster_anchor_chain_on_system_node(node, cluster) {
    if (cluster.name == clusters.system) {
        val cluster_anchoring_chains = cluster_anchoring_chain @* { .cluster.name != clusters.system } ( .blockchain );
        for (blockchain in cluster_anchoring_chains) {
            if (not(exists(blockchain_replica_node @? { blockchain, node }))) {
                create blockchain_replica_node(blockchain, node);
            }
        }
    }
}
</string>
                            </entry>
                            <entry key="common/blockchain.rell">
                                <string>val COMPRESSED_ROOTS_CONFIG_KEY = "compressed_roots";

function allow_blockchain_dependencies() = chain_context.args.allow_blockchain_dependencies;

function add_blockchain(
    base_configuration: byte_array,
    signers: list&lt;pubkey&gt;,
    name: text, 
    container,
    system: boolean = false,
    state: blockchain_state = blockchain_state.RUNNING
): blockchain {
    require(empty(blockchain @? { name, .state != blockchain_state.REMOVED }), "A blockchain with the same name already exists");
    val blockchain_rid = calculate_configuration_hash(base_configuration, signers);
    require(
        empty(blockchain @? { .rid == blockchain_rid }),
        "Another blockchain with same RID %s has previously been deployed to this network. Please ensure that the initial configuration is unique.".format(blockchain_rid)
    );
    val signer_bytes = signers.to_gtv().to_bytes();
    val blockchain = create blockchain(blockchain_rid, name, system, state);
    compress_and_store_configuration(blockchain, 0, base_configuration);
    create blockchain_configuration_signers(blockchain, 0, signer_bytes);
    create container_blockchain(container, blockchain);
    add_dependencies(base_configuration, blockchain_rid, 0);
    signal_signer_list_update(blockchain_rid, signer_bytes);
    return blockchain;
}

function add_dependencies(raw_config: byte_array, brid: byte_array, height: integer) {
    val config_map = map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(raw_config));
    if (config_map.contains("dependencies")) {
        require(allow_blockchain_dependencies(), "Blockchain dependencies are not allowed");
        val dependencies = list&lt;(text, byte_array)&gt;.from_gtv(config_map["dependencies"]);   // Returns e.g.  [brid0, brid22, ..]
        val blockchain = blockchain @ { brid };
        // In case this is a configuration override, delete any deps from overridden config
        delete blockchain_dependency @* { .me == blockchain, .height == height };
        val container = container_blockchain @ { blockchain }.container;
        for ((_, dependency_brid) in dependencies) {
            val dependency_blockchain = blockchain @? { dependency_brid };
            if (exists(dependency_blockchain)) {
                require(brid != dependency_brid, "Dependency brid mismatch: %s != %s".format(brid, dependency_brid));
                require(container_blockchain @ { dependency_blockchain }.container == container,
                    "Blockchain dependencies are only allowed within the same container");
                create blockchain_dependency(
                    me = blockchain,
                    dependent_on = dependency_blockchain,
                    height = height);
            }
        }
    }
}

function get_blockchain_signer_nodes(blockchain: blockchain): list&lt;node&gt; {
    return (cluster_node, container_blockchain) @* {
        container_blockchain.blockchain == blockchain,
        cluster_node.cluster == container_blockchain.container.cluster
    } ( .node );
}

function get_signers_for_configuration(blockchain, height: integer): gtv {
    val raw_signers = blockchain_configuration_signers @ { blockchain, .height &lt;= height }
        (@omit @sort_desc .height, .signers) limit 1;
    return gtv.from_bytes(raw_signers);
}

function is_alive(blockchain): boolean {
    return blockchain.state == blockchain_state.RUNNING or blockchain.state == blockchain_state.PAUSED;
}

function calculate_configuration_hash(base_config: byte_array, signers: list&lt;pubkey&gt;): byte_array {
    // Hash calculated on decompressed config
    val decompressed_config = decompress_configuration(base_config);
    val full_config = map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(decompressed_config));
    full_config["signers"] = signers.to_gtv();
    return full_config.to_gtv().hash();    
}

function get_blockchain_configuration(blockchain_rid: byte_array, height: integer):
    (base_config: byte_array, signers: list&lt;pubkey&gt;, config_hash: byte_array)?
{
    val bc = require_blockchain(blockchain_rid);

    val base_config = get_blockchain_configuration_data(bc, height);
    if (base_config != null) {
        // Checking that signers.size() is non-zero is done when population blockchain_configuration_signers.
        val signers = require(blockchain_configuration_signers @? { bc, .height &lt;= height }
            (@omit @sort_desc .height, list&lt;pubkey&gt;.from_gtv(gtv.from_bytes(.signers))) limit 1,
            "No signers configuration for blockchain %s for height %d".format(blockchain_rid, height)
        );
        return (
            base_config = base_config,
            signers = signers,
            config_hash = calculate_configuration_hash(base_config, signers)
        );
    } else {
        return null;
    }
}

function get_blockchain_configuration_data(blockchain, height: integer): byte_array? {
    // Find configuration height -- the latest height up to given height.
    // If conf_h exist, so does signer_h, thus no need to check both.
    val compressed_config = blockchain_configuration @? { blockchain, .height &lt;= height } (@omit @sort_desc .height, .data) limit 1;
    return if (compressed_config != null) decompress_configuration(compressed_config) else null;
}

function get_latest_blockchain_configuration_data(blockchain): (height: integer, config: byte_array) {
    val compressed_config = blockchain_configuration @ { blockchain } (@sort_desc .height, .data) limit 1;
    val decompressed_config = decompress_configuration(compressed_config.data);
    return (height = compressed_config.height, config = decompressed_config);
}

function get_pending_blockchain_configuration(blockchain, height: integer):
    list&lt;(base_config: byte_array, signers: list&lt;pubkey&gt;, minimum_height: integer)&gt; {
    val earliest_signer_update_height = pending_blockchain_configuration @? { blockchain, .signers_update }
        (@sort .minimum_height) limit 1;
    val include_to_height = if (earliest_signer_update_height == null) height else min(height, earliest_signer_update_height);
    return pending_blockchain_configuration @* { blockchain, .minimum_height &lt;= include_to_height } (
        base_config = decompress_configuration(.base_config),
        signers = list&lt;pubkey&gt;.from_gtv(gtv.from_bytes(.signers)),
        @sort minimum_height = .minimum_height
    );
}

function get_pending_blockchain_configuration_by_hash(blockchain, config_hash: byte_array):
    (base_config: byte_array, signers: list&lt;pubkey&gt;, minimum_height: integer)? = pending_blockchain_configuration @? { blockchain, config_hash } (
        base_config = decompress_configuration(.base_config),
        signers = list&lt;pubkey&gt;.from_gtv(gtv.from_bytes(.signers)),
        @sort minimum_height = .minimum_height
    );

function get_latest_pending_blockchain_configuration_data(blockchain):
    (minimum_height: integer, base_config: byte_array, signers: byte_array)? = pending_blockchain_configuration @? { blockchain } (
        @sort_desc .minimum_height,
        base_config = decompress_configuration(.base_config),
        signers = .signers
    ) limit 1;


struct compressed_root {
    path: list&lt;text&gt;;
    compressed_keys: list&lt;compressed_key&gt;;
}

struct compressed_key {
    content_key: text;
    content_hash: byte_array;
}

function decompress_root(root: map&lt;text, gtv&gt;, path: list&lt;text&gt;, compressed_keys: list&lt;compressed_key&gt;): map&lt;text, gtv&gt; {
    if (path.empty()) {
        val hashes_to_decompress = set&lt;byte_array&gt;();
        for (hash in compressed_keys @* {}.content_hash) {
            hashes_to_decompress.add(hash);
        }

        val resolved_parts = map&lt;byte_array, byte_array&gt;();
        for (compressed_part in compressed_blockchain_configuration_part @* { .hash in hashes_to_decompress }) {
            resolved_parts.put(compressed_part.hash, compressed_part.data);
        }
        for (compressed_key in compressed_keys) {
            root.put(compressed_key.content_key, gtv.from_bytes(resolved_parts[compressed_key.content_hash]));
        }
    } else {
        val next_level = path[0];
        val next_root = if (root.contains(next_level)) map&lt;text, gtv&gt;.from_gtv(root[next_level]) else map&lt;text, gtv&gt;();
        root[next_level] = decompress_root(next_root, path.sub(1), compressed_keys).to_gtv();
    }
    return root;
}

function decompress_configuration(configuration: byte_array): byte_array {
    val config_map = map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(configuration));
    if (not config_map.contains(COMPRESSED_ROOTS_CONFIG_KEY)) return configuration;

    val compressed_roots = list&lt;compressed_root&gt;.from_gtv_pretty(config_map[COMPRESSED_ROOTS_CONFIG_KEY]);
    for (root in compressed_roots) {
        decompress_root(config_map, root.path, root.compressed_keys);
    }

    config_map.remove(COMPRESSED_ROOTS_CONFIG_KEY);
    return config_map.to_gtv().to_bytes();
}

function compress_root(root: map&lt;text, gtv&gt;, path: list&lt;text&gt;, compressed_keys: list&lt;compressed_key&gt;): map&lt;text, gtv&gt; {
    if (path.empty()) {
        for (key_to_compress in root.keys()) {
            val content = root[key_to_compress];
            val hash = content.hash();
            if (not exists(compressed_blockchain_configuration_part @? { .hash == hash })) {
                create compressed_blockchain_configuration_part(hash = hash, data = content.to_bytes());
            }
            compressed_keys.add(compressed_key(key_to_compress, hash));
            root.remove(key_to_compress);
        }
    } else {
        val next_level = path[0];
        val next_root = if (root.contains(next_level)) map&lt;text, gtv&gt;.from_gtv(root[next_level]) else map&lt;text, gtv&gt;();
        root[next_level] = compress_root(next_root, path.sub(1), compressed_keys).to_gtv();
    }
    return root;
}

function compress_and_store_configuration(blockchain, height: integer, configuration: byte_array, overwrite: boolean = false) {
    val config_map = map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(configuration));
    if (not config_map.contains(COMPRESSED_ROOTS_CONFIG_KEY)) {
        store_configuration(blockchain, height, configuration, overwrite);
    } else {
        val compressed_roots = list&lt;compressed_root&gt;.from_gtv_pretty(config_map[COMPRESSED_ROOTS_CONFIG_KEY]);
        for (root in compressed_roots) {
            val new_compressed_keys = list&lt;compressed_key&gt;();
            compress_root(config_map, root.path, new_compressed_keys);
            root.compressed_keys.add_all(new_compressed_keys);
        }

        config_map[COMPRESSED_ROOTS_CONFIG_KEY] = compressed_roots.to_gtv();
        store_configuration(blockchain, height, config_map.to_gtv().to_bytes(), overwrite);
    }
}

function store_configuration(blockchain, height: integer, configuration: byte_array, overwrite: boolean = false) {
    if (overwrite and exists(blockchain_configuration @? { blockchain, height })) {
        update blockchain_configuration @ { blockchain, height } ( .data = configuration );
    } else {
        create blockchain_configuration(blockchain, height, configuration);
    }
}

function remove_blockchain(blockchain) {
    before_delete_blockchain(blockchain);
    blockchain.state = blockchain_state.REMOVED;
    delete container_blockchain @ { blockchain };
    delete blockchain_replica_node @* { blockchain };
    delete blockchain_dependency @* { .me == blockchain };
    delete inactive_blockchain @? { blockchain };
    create inactive_blockchain(blockchain, op_context.block_height);
}

@extend(before_remove_container) function remove_container_blockchains(container) {
    val container_blockchains = container_blockchain @* { container }.blockchain;
    for (blockchain in container_blockchains) {
        remove_blockchain(blockchain);
    }
}

@extendable function before_delete_blockchain(blockchain) {}
</string>
                            </entry>
                            <entry key="common/cluster.rell">
                                <string>function create_cluster_impl(me: provider, name, governor: voter_set, providers: list&lt;pubkey&gt;, cluster_creation_data) {
    require(empty(cluster @* { name }), "Cluster with name %s already exists".format(name));
    validate_entity_name(name);
    require(cluster_creation_data.cluster_units &gt; 0, "Cluster must have at least 1 cluster unit");
    require(cluster_creation_data.extra_storage &gt;= 0, "Extra storage must not be negative");
    val c = create cluster(
        name,
        governor,
        false,
        cluster_units = cluster_creation_data.cluster_units,
        extra_storage = cluster_creation_data.extra_storage
    );

    for (p_key in providers) {
        val provider = require_provider(p_key);
        require_node_access(provider);
        create cluster_provider(c, provider);
    }

    create_system_container(
        me,
        system_container_name(c.name),
        cluster = c,
        voter_set = governor
    );

    after_cluster_creation(me, c);
    return c;
}

function require_cluster_quotas(cluster, wanted_container_units: integer, wanted_extra_storage: integer) {
    require_cluster_container_units(cluster, wanted_container_units);
    require_cluster_extra_storage(cluster, wanted_extra_storage);
}

function require_cluster_container_units(cluster, wanted_container_units: integer) {
    if (wanted_container_units &gt; 0) {
        val available_container_units = get_available_container_units(cluster);
        require(wanted_container_units &lt;= available_container_units,
            "Cluster %s has %d available container units but wanted %d".format(cluster.name, available_container_units, wanted_container_units));
    }
}

function require_cluster_extra_storage(cluster, wanted_extra_storage: integer) {
    if (wanted_extra_storage &gt; 0) {
        val available_extra_storage = get_available_extra_storage(cluster);
        require(wanted_extra_storage &lt;= available_extra_storage,
            "Cluster %s has %d MiB available extra storage but wanted %d MiB".format(cluster.name, available_extra_storage, wanted_extra_storage));
    }
}

function get_available_extra_storage(cluster): integer {
    return cluster.extra_storage - get_used_extra_storage_for_cluster(cluster);
}

function get_available_container_units(cluster): integer {
    val max_container_units = get_max_container_units_for_cluster(cluster);
    val used_container_units = get_used_container_units(cluster);
    return max_container_units - used_container_units;
}

function get_max_container_units_for_cluster(cluster): integer {
    return standard_cluster_unit.container_units * cluster.cluster_units;
}

function get_used_container_units(cluster): integer {
    return container_resource_limit @ {
        .container.cluster == cluster,
        container_resource_limit_type.container_units
    } (@sum .value);
}

function get_minimum_cluster_units_for_current_container_units(cluster): integer {
    val currently_used_container_units = get_used_container_units(cluster);
    return currently_used_container_units / standard_cluster_unit.container_units + 1;
}

function get_minimum_extra_storage_for_clusters(clusters: list&lt;cluster&gt;): integer {
    return clusters @ {} (@sum .extra_storage);
}

function get_used_extra_storage_for_cluster(cluster): integer {
    return container_resource_limit @ {
        .container.cluster == cluster,
        container_resource_limit_type.extra_storage
    } (@sum .value);
}

function _add_replica_node_to_cluster_internal(cluster, node) {
    require_cluster_units_for_node(cluster, node);
    require_extra_storage_for_node(cluster, node);
    create cluster_replica_node(cluster, node);
    after_replica_node_added_to_cluster(node, cluster);
}

function _remove_replica_node_from_cluster_internal(cluster, node) {
    val crn = cluster_replica_node @? { cluster, node };
    if (exists(crn)) {
        after_replica_node_removed_from_cluster(node, cluster);
        delete crn;
    }
}

/**
 * When all providers have provided a node each, cluster goes operational and
 * stays operational even if a new provider is added to the cluster or
 * a provider disables its node (if it is not the last node of the cluster).
 */
function check_operational(cl: cluster) {
    if (not cl.operational) {
        val providers = cluster_provider @* { cl }.provider;
        val nodes = cluster_node @* { cl }.node;
        if (nodes.size() == providers.size()) {
            update cluster @ { .name == cl.name } (.operational = true);
            after_cluster_operational(cl);
        }
    }
}

/**
 * If a provider is part of that cluster, and if provider do not already have a node in this cluster,
 * add node as block signer to this cluster. blockchain_configuration_signers update is included.
 */
function add_node_to_cluster_internal(provider, node, cluster) {
    if (exists(cluster_node @? { cluster, node })) {
        log("Node %s already is part of cluster %s".format(node.pubkey, cluster.name));
        return;
    }

    if (exists(cluster_provider @* { cluster, provider })) {
        val provider_cluster_nodes = cluster_node @* { cluster, .node in node @* { provider } };
        require(empty(provider_cluster_nodes), "A provider can only provide one node to each cluster");
        _remove_replica_node_from_cluster_internal(cluster, node);
        require_cluster_units_for_node(cluster, node);
        require_extra_storage_for_node(cluster, node);
        create cluster_node(cluster, node);
        update_configuration_signers(cluster, null);
        // check if cluster now is operational, if so update the flag:
        check_operational(cluster);
        log("blockchain configuration signers are updated");
        after_node_added_to_cluster(node, cluster);
    } else {
        log("Provider %s is not a member of cluster %s".format(provider.pubkey, cluster.name));
    }
}

// Use this to update signers after a change in cluster_node table.
function update_configuration_signers(cluster, excluded_node: node?) {
    val signers = cluster_node @* { cluster } (@sort .node.pubkey);
    // do not write new configuration when size is 0 since it's impossible to recover from that
    require(signers.size() &gt; 0);

    val bcs = container_blockchain @* { .container.cluster == cluster } .blockchain;
    for (blockchain in bcs) {
        val is_chain0 = blockchain.rid == chain_context.blockchain_rid;
        if (is_chain0) {
            update_configuration_signers_chain0(blockchain, signers);
        } else {
            update_configuration_signers_regular(blockchain, signers, excluded_node?.pubkey);
        }
    }
}

function update_configuration_signers_chain0(blockchain, signers: list&lt;pubkey&gt;) {
    val height = op_context.block_height + 1; // NB: compute_blockchain_info_list()/get_cluster_node_blockchains() relies on this
    log("Signers update for chain0 at height %d: %s".format(height, signers));

    // make a base_config at `height` unique
    if (empty(blockchain_configuration @? { blockchain, height })) {
        val base_config = blockchain_configuration @? { blockchain, .height &lt; height } (@omit @sort_desc .height, .data) limit 1;
        val unique_base_config = make_config_unique(base_config!!);
        create blockchain_configuration(blockchain, height, unique_base_config);
        add_dependencies(unique_base_config, blockchain.rid, height);
    }

    // signers
    val bc_signers = blockchain_configuration_signers @? { blockchain, height };
    val signer_bytes = signers.to_gtv().to_bytes();
    if (bc_signers == null) {
        create blockchain_configuration_signers(blockchain, height, signer_bytes);
    } else {
        bc_signers.signers = signer_bytes;
    }

    signal_signer_list_update(chain_context.blockchain_rid, signer_bytes);
}

function update_configuration_signers_regular(blockchain, signers: list&lt;pubkey&gt;, excluded_signer: pubkey?) {
    val last_signers_config = blockchain_configuration_signers @? { blockchain } (@sort_desc .height, .signers) limit 1;

    if (last_signers_config == null) {
        // No initial signers config found, add new config as initial
        create blockchain_configuration_signers(blockchain, 0, signers.to_gtv().to_bytes());
        return;
    }

    val last_pending_config = get_latest_pending_blockchain_configuration_data(blockchain);
    val (minimum_height, base_config, last_signers) = if (last_pending_config != null) (
        last_signers_config.height.max(last_pending_config.minimum_height) + 1,
        last_pending_config.base_config,
        last_pending_config.signers
    ) else (
        last_signers_config.height + 1,
        get_latest_blockchain_configuration_data(blockchain).config,
        last_signers_config.signers
    );

    if (signers.to_gtv().to_bytes() == last_signers) {
        log("Signers update for chain %s not necessary, already %s".format(blockchain.rid, signers));
        return;
    }

    log("Signers update for chain %s at minimum height %d: %s".format(blockchain.rid, minimum_height, signers));
    val unique_base_config = make_config_unique(base_config);
    val config_hash = calculate_configuration_hash(unique_base_config, signers);
    create pending_blockchain_configuration(
        blockchain,
        minimum_height,
        config_hash = config_hash,
        base_config = unique_base_config,
        signers = signers.to_gtv().to_bytes(),
        signers_update = true
    );

    if (excluded_signer != null) {
        create signer_excluded_from_pending_configuration(
            blockchain,
            config_hash = config_hash,
            pubkey = excluded_signer
        );
    }
}

function require_cluster_available_for_removal(cluster) {
    require(cluster.name != clusters.system, "System cluster can't be deleted");
    require(
        empty(container @* { cluster, .system == false }),
        "Cluster %s is not empty and can't be deleted. Delete containers first".format(cluster.name)
    );
}

function get_cluster_for_blockchain(blockchain_rid: byte_array): cluster {
    return (container_blockchain, blockchain) @ { blockchain.rid == blockchain_rid, blockchain == container_blockchain.blockchain }
                 ( container_blockchain.container.cluster );
}

function remove_cluster_impl(cluster) {
    before_cluster_removal(cluster);
    delete cluster_node @* { cluster };
    delete cluster_replica_node @* { cluster };
    delete cluster_provider @* { cluster };
    delete cluster;
}

function number_of_nodes_in_cluster(cluster): integer = cluster_node @ { cluster } ( @sum 1 );

@extendable function before_cluster_removal(cluster) {}

@extendable function after_cluster_creation(provider, cluster) {}

@extendable function after_cluster_operational(cluster) {}

@extendable function after_cluster_updated(cluster) {}

@extendable function after_node_added_to_cluster(node, cluster) {}

@extendable function after_replica_node_added_to_cluster(node, cluster) {}

@extendable function after_node_removed_from_cluster(node, cluster?) {}

@extendable function after_replica_node_removed_from_cluster(node, cluster?) {}
</string>
                            </entry>
                            <entry key="common/configuration_failed.rell">
                                <string>@extend(receive_icmf_message) function receive_configuration_failed(sender: byte_array, topic: text, body: gtv) {
    if (topic != configuration_failed_topic) return;

    val message = configuration_failed.from_gtv(body);

    if (message.blockchain_rid == chain_context.blockchain_rid) {
        log("Received failed configuration from chain %s for chain0 at height %d to %s, ignoring"
            .format(sender, message.height, message.config_hash));
        return;
    }
    val bc = blockchain @? { message.blockchain_rid };
    if (bc == null) {
        log("Unknown blockchain " + message.blockchain_rid);
        return;
    }

    val cluster = get_cluster_for_blockchain(message.blockchain_rid);
    val anchoring_chain = cluster_anchoring_chain @? { cluster } (.blockchain);
    if (anchoring_chain == null) {
        log("No anchoring chain for cluster %s".format(cluster.name));
        return;
    }
    if (sender != anchoring_chain.rid and sender != system_anchoring_chain.rid) {
        log("Received failed configuration from chain %s, which is not anchor chain for cluster %s".format(sender, cluster.name));
        return;
    }

    val pending_configuration = pending_blockchain_configuration @?
        { bc, .config_hash == message.config_hash, .minimum_height &lt;= message.height };
    if (pending_configuration == null) {
        log("Configuration with hash %s and minimum_height&lt;=%d not found for chain %s".format(message.config_hash, message.height, message.blockchain_rid));
        return;
    }

    log("Deleting failed pending configuration with hash %s for chain %s".format(message.config_hash, message.blockchain_rid));

    update blockchain_configuration_update_attempt @? { .config_hash == message.config_hash }
        (state = blockchain_configuration_update_state.FAILED, applied_at_height = message.height);
    create faulty_blockchain_configuration(
        blockchain = bc,
        config_hash = message.config_hash,
        reported_at_height = message.height
    );

    // Retry signers update
    if (pending_configuration.signers_update) {
        log("Pending config %s with signer updates failed. Retrying.".format(pending_configuration.config_hash));
        val excluded_signer = signer_excluded_from_pending_configuration
                @? { bc, .config_hash == pending_configuration.config_hash };
        val new_signers = pending_configuration.signers;
        val unique_base_config = make_config_unique(get_latest_blockchain_configuration_data(bc).config);
        val config_hash = calculate_configuration_hash(
            unique_base_config,
            list&lt;byte_array&gt;.from_gtv(gtv.from_bytes(new_signers))
        );
        val minimum_height = pending_configuration.minimum_height;

        delete pending_configuration;
        create pending_blockchain_configuration(
            bc,
            minimum_height,
            config_hash = config_hash,
            base_config = unique_base_config,
            signers = new_signers,
            signers_update = true
        );

        if (excluded_signer != null) {
            create signer_excluded_from_pending_configuration(
                bc, config_hash = config_hash, pubkey = excluded_signer.pubkey
            );
            delete excluded_signer;
        }
    } else {
        delete pending_configuration;
    }
}
</string>
                            </entry>
                            <entry key="common/configuration_updated.rell">
                                <string>@extend(receive_icmf_message) function receive_configuration_updated(sender: byte_array, topic: text, body: gtv) {
    if (topic != configuration_updated_topic) return;

    val message = configuration_updated.from_gtv(body);

    if (message.blockchain_rid == chain_context.blockchain_rid) {
        log("Received updated configuration from chain %s for chain0 at height %d to %s, ignoring"
            .format(sender, message.height, message.config_hash));
        return;
    }
    val bc = blockchain @? { message.blockchain_rid };
    if (bc == null) {
        log("Unknown blockchain " + message.blockchain_rid);
        return;
    }

    val cluster = get_cluster_for_blockchain(message.blockchain_rid);
    val anchoring_chain = cluster_anchoring_chain @? { cluster } (.blockchain);
    if (anchoring_chain == null) {
        log("No anchoring chain for cluster %s".format(cluster.name));
        return;
    }
    if (sender != anchoring_chain.rid and sender != system_anchoring_chain.rid) {
        log("Received updated configuration from chain %s, which is not anchor chain for cluster %s".format(sender, cluster.name));
        return;
    }

    log("Received updated configuration from chain %s for chain %s at height %d to %s"
        .format(sender, message.blockchain_rid, message.height, message.config_hash));

    on_configuration_updated(message);

    val pending_configuration = pending_blockchain_configuration @?
        { bc, .config_hash == message.config_hash, .minimum_height &lt;= message.height };
    if (pending_configuration == null) {
        log("Configuration with hash %s and minimum_height&lt;=%d not found for chain %s".format(message.config_hash, message.height, message.blockchain_rid));
        return;
    }

    val pending_base_config = pending_configuration.base_config;
    val pending_signers = pending_configuration.signers;
    if (blockchain_configuration @? { bc, message.height } != null) {
        log("Configuration at height %d already exists for chain %s".format(message.height, message.blockchain_rid));
        return;
    }

    update blockchain_configuration_update_attempt @? { .config_hash == message.config_hash }
        (state = blockchain_configuration_update_state.SUCCESSFUL, applied_at_height = message.height);
    delete signer_excluded_from_pending_configuration @* { bc, .config_hash == pending_configuration.config_hash };
    delete pending_configuration;

    compress_and_store_configuration(bc, message.height, pending_base_config);

    val current_signer_config = blockchain_configuration_signers @ { bc } (@omit @sort_desc .height, $) limit 1;
    if (current_signer_config.signers != pending_signers) {
        if (current_signer_config.height != message.height) {
            create blockchain_configuration_signers(bc, message.height, pending_signers);
        } else {
            current_signer_config.signers = pending_signers;
        }
        signal_signer_list_update(message.blockchain_rid, pending_signers);
    }

    add_dependencies(pending_base_config, bc.rid, message.height);
}

@extendable function on_configuration_updated(message: configuration_updated) {}
</string>
                            </entry>
                            <entry key="common/container.rell">
                                <string>function create_system_container(me: provider, name, cluster, voter_set): container {
    val c = create container(name, cluster, voter_set, me, system = true);
    create container_resource_limit(c, container_resource_limit_type.container_units, system_container_defaults.container_units);
    create container_resource_limit(c, container_resource_limit_type.max_blockchains, system_container_defaults.max_blockchains);
    create container_resource_limit(c, container_resource_limit_type.extra_storage, system_container_defaults.extra_storage);
    return c;
}

function create_container_with_limits(me: provider, name, cluster, voter_set, container_units: integer, max_blockchains: integer, extra_storage: integer): container {
    require(container_units &gt; 0, "Container must have at least 1 container unit");
    require(extra_storage &gt;= 0, "Invalid value for extra_storage: %d, must be greater or equal to 0".format(extra_storage));
    require_cluster_quotas(cluster, container_units, extra_storage);
    val c = create container(name, cluster, voter_set, me);
    create container_resource_limit(c, container_resource_limit_type.container_units, container_units);
    create container_resource_limit(c, container_resource_limit_type.max_blockchains, max_blockchains);
    create container_resource_limit(c, container_resource_limit_type.extra_storage, extra_storage);
    return c;
}

function remove_container_impl(container) {
    before_remove_container(container);
    delete container_resource_limit @* { container };
    delete container;
}

@extendable function before_remove_container(container) {}

@extendable function is_container_available_for_removal(container): text? {
    return when {
        container.system -&gt; "System container can't be deleted";
        else -&gt; null;
    };
}

function require_container_available_for_removal(container) {
    val objections = is_container_available_for_removal(container);
    require(empty(objections), objections!!);
}

@extend(before_cluster_removal) function remove_system_container(cluster) {
    before_system_container_removal(cluster);
    remove_container_impl(container @ { cluster, .system == true });
}

@extendable function before_system_container_removal(cluster) {}

function create_container_impl(me: provider, name, cluster, consensus_threshold: integer, deployers: list&lt;pubkey&gt;, container_units: integer, max_blockchains: integer, extra_storage: integer): container {
    require(consensus_threshold &gt;= -1 and consensus_threshold &lt;= deployers.size(), "Invalid threshold");

    val vs_name = "container_" + name + "_deployer";
    require(empty(voter_set @? { vs_name }), "Voter set named %s already exists".format(vs_name));
    val vs = create_voter_set_internal(vs_name, consensus_threshold);
    for (deployer_key in deployers) {
        val deployer = require_provider(deployer_key);
        require(deployer.active, "Provider %s is not active".format(deployer.pubkey));
        create voter_set_member(vs, deployer);
    }

    return create_container_with_limits(me, name, cluster, vs, container_units, max_blockchains, extra_storage);
}

function remove_container_and_voter_set(container) {
    if (empty(is_container_available_for_removal(container))) {
        val vs = container.deployer;
        remove_container_impl(container);
    }
}

function get_current_container_resource_limits(container_name: text): map&lt;container_resource_limit_type, integer&gt; {
    var limits_map = map&lt;container_resource_limit_type, integer&gt;();
    val cur_limits_list = container_resource_limit @* { .container.name == container_name };
    for (l in cur_limits_list) {
        limits_map[l.container_resource_limit_type] = l.value;
    }
    return limits_map;
}

function get_container_limit_or_default(container, type: container_resource_limit_type, default: integer): integer {
    return container_resource_limit @? { container, .container_resource_limit_type == type } (.value) ?: default;
}

function require_container_is_not_full(container) {
    val max_blockchains = container_resource_limit @ { container, container_resource_limit_type.max_blockchains } (.value);
    if (max_blockchains &gt; 0) {
        val container_dapps_count = container_blockchain @* { container } (@sum 1)[0];
        require(container_dapps_count &lt; max_blockchains, "Can't add blockchain, container %s is full".format(container.name));
    }
}

function get_container_blockchain_count(container) {
    return container_blockchain @* { container } (@sum 1)[0];
}

function upgrade_container(container, container_units: integer, extra_storage: integer, max_blockchains: integer) {
    require_container_units(container, container_units);
    require_max_blockchains(container, max_blockchains);
    require_extra_storage(container, extra_storage);

    update container_resource_limit @ { container, container_resource_limit_type.container_units } ( container_units );
    update container_resource_limit @ { container, container_resource_limit_type.max_blockchains } ( max_blockchains );
    update container_resource_limit @ { container, container_resource_limit_type.extra_storage } ( extra_storage );
}

function require_container_units(container, new_container_units: integer) {
    val current_container_units = get_container_limit_or_default(container, container_resource_limit_type.container_units, standard_container_defaults.container_units);
    val available_container_units = get_available_container_units(container.cluster);
    val required_container_units = new_container_units - current_container_units;
    require(
        required_container_units &lt;= 0 or required_container_units &lt;= available_container_units,
        "Can not change container limits since container_units is too high for current cluster. Available container_units in cluster are %d but needed is %d".format(available_container_units, required_container_units)
    );
}

function require_max_blockchains(container, new_max_blockchains: integer) {
    val current_blockchain_count = get_container_blockchain_count(container);
    require(
        current_blockchain_count &lt;= new_max_blockchains,
        "Can not change container limits since max_blockchains is too low. Current number of blockchains is %d".format(current_blockchain_count)
    );
}

function require_extra_storage(container, new_extra_storage: integer) {
    val current_extra_storage = get_container_limit_or_default(container, container_resource_limit_type.extra_storage, standard_container_defaults.extra_storage);
    val available_extra_storage = get_available_extra_storage(container.cluster);
    val required_extra_storage = new_extra_storage - current_extra_storage;
    require(
        required_extra_storage &lt;= available_extra_storage,
        "Can not change container limits since extra_storage is too high. Available extra_storage is %d MiB but needed is %d MiB".format(available_extra_storage, required_extra_storage)
    );
}</string>
                            </entry>
                            <entry key="common/init.rell">
                                <string>module;

import ^.*;
import model.*;

struct module_args {
    initial_provider: pubkey;
    genesis_node: node_info; 
    // actions_per_day: integer = 100; consider adding this module arg
}

// This operation will check that provider table is empty and if so add the provider supplied as module argument. And
// enable this first system provider.
operation init(system_anchoring_config: byte_array?, cluster_anchoring_config: byte_array?) {
    require_is_signer(chain_context.args.initial_provider);

    val genesis_node = chain_context.args.genesis_node;
    require_pubkey(genesis_node.pubkey);

    if (system_anchoring_config != null) {
        require(cluster_anchoring_config, "System anchoring requires cluster anchoring");
    }

    initialize_module(
        chain_context.args.initial_provider,
        genesis_node,
        system_anchoring_config = system_anchoring_config ?: map&lt;text, gtv&gt;().to_gtv().to_bytes(),
        cluster_anchoring_config = cluster_anchoring_config ?: map&lt;text, gtv&gt;().to_gtv().to_bytes()
    );
    after_init();
}

@extendable function after_init() {}
</string>
                            </entry>
                            <entry key="common/module.rell">
                                <string>module;

import model.*;
import lib.icmf.*;
import lib.icmf.receiver.*;
import messaging.configuration_update_message.*;
import proposal.*;
import proposal_blockchain.*;
import roles.*;
import signer_list_update.*;

struct module_args {
    allow_blockchain_dependencies: boolean;
    provider_quota_max_actions_per_day: integer;
}

// Adds the initial provider as a system provider along with voter sets and clusters needed to start
function initialize_module(
    initial_provider: pubkey,
    genesis_node: node_info,
    system_anchoring_config: byte_array,
    cluster_anchoring_config: byte_array,
    majority_threshold: integer = 0,
    provider_quota_max_containers: integer = provider_quota_defaults.MAX_CONTAINERS
) {
    require(empty( provider @* {}), "Network has already been initialized");
    log("--------------------Initializing-Chain-0-----------------------");
    log("Creating provider quotas");
    _setup_provider_quotas(chain_context.args.provider_quota_max_actions_per_day, provider_quota_max_containers);

    log("Creating SYSTEM voter set");
    create_voter_set_internal(voter_sets.system, 1);
    /**
     * System provider create a system cluster which will include system nodes and has a system container
     * which will run the directory bc.
     */
    log("Creating SYSTEM_P voter set");
    val system_voter_set = create_voter_set_internal(voter_sets.system_p, majority_threshold);

    log("Creating initial provider with pubkey: " + initial_provider);
    register_and_enable_provider(
        provider_info(initial_provider),
        provider_tier.NODE_PROVIDER,
        cluster = null,
        voter_set = null,
        enabled_by_default = true
    );
    val provider = provider @ { initial_provider };

    log("Creating system cluster with SYSTEM_P as governor");
    set_cluster_anchoring_config(cluster_anchoring_config);
    val system_cluster = create_cluster_impl(
        provider,
        clusters.system,
        governor = system_voter_set,
        providers = list&lt;pubkey&gt;(),
        cluster_creation_data = cluster_creation_data()
       );

    enroll.system(provider);
    after_provider_updated(provider);

    val system_container = container @ { .cluster == system_cluster, .system };

    log("Adding directory chain to system container");
    val directory_chain_config = map&lt;text,gtv&gt;.from_gtv(chain_context.raw_config);
    val directory_chain_signers = list&lt;byte_array&gt;.from_gtv(directory_chain_config["signers"]);

    // Compress and save initial directory chain config
    val directory_chain_initial_config = map&lt;text,gtv&gt;.from_gtv(chain_context.raw_config);
    directory_chain_initial_config[COMPRESSED_ROOTS_CONFIG_KEY] = [compressed_root(
        path = ["gtx", "rell", "sources"],
        compressed_keys = []
    )].to_gtv_pretty();
    add_blockchain(directory_chain_initial_config.to_gtv().to_bytes(), directory_chain_signers, blockchains.directory_chain, system_container, true);

    val system_anchoring_config_map = map&lt;text,gtv&gt;.from_gtv(gtv.from_bytes(system_anchoring_config));
    if (system_anchoring_config_map.size() &gt; 0) {
        log("Adding system anchoring blockchain to system container");
        val system_anchoring_blockchain = add_blockchain(system_anchoring_config, directory_chain_signers, blockchains.system_anchoring, system_container, true);
        update system_anchoring_chain ( rid = system_anchoring_blockchain.rid );
    }

    require(directory_chain_signers.size() == 1, "Directory chain must have exactly one initial signer");
    val initial_signer_node_key = directory_chain_signers[0];
    require(initial_signer_node_key == genesis_node.pubkey, "Blockchain signer must match genesis node configuration");

    log("Adding initial signer node of directory chain to system cluster: " + initial_signer_node_key);
    val initial_node = create node(
        provider,
        initial_signer_node_key,
        host = genesis_node.host,
        port = genesis_node.port,
        api_url = genesis_node.api_url,
        last_updated = op_context.last_block_time,
        territory = genesis_node.territory
    );
    after_node_added(initial_node);

    create cluster_node(system_cluster, initial_node);
    after_node_added_to_cluster(initial_node, system_cluster);

    check_operational(system_cluster);
    log("---------------------------------------------------------------");
}

function _setup_provider_quotas(actions_per_day: integer, max_containers: integer) {
    // max_actions_per_day
    create provider_quota(tier = provider_tier.DAPP_PROVIDER, provider_quota_type.max_actions_per_day, value = actions_per_day);
    create provider_quota(tier = provider_tier.NODE_PROVIDER, provider_quota_type.max_actions_per_day, value = actions_per_day);
    // max_nodes
    create provider_quota(tier = provider_tier.DAPP_PROVIDER, provider_quota_type.max_nodes, value = provider_quota_defaults.MAX_NODES);
    create provider_quota(tier = provider_tier.NODE_PROVIDER, provider_quota_type.max_nodes, value = -1);
    // max_containers
    create provider_quota(tier = provider_tier.NODE_PROVIDER, provider_quota_type.max_containers, value = max_containers);
}
</string>
                            </entry>
                            <entry key="common/node.rell">
                                <string>function add_node_internal(provider, node_data: register_node_data, require_not_exists: boolean = true): node {
    require(empty(provider @? { node_data.pubkey }),
        "This pubkey is already registered as a provider: " + node_data.pubkey
    );
    require_provider_quota(provider, provider_quota_type.max_nodes);
    if (require_not_exists) {
        require(empty(node @? { node_data.pubkey }), "Node already exists: " + node_data.pubkey);
    }

    val clusters = require_clusters(node_data.clusters);
    val minimum_extra_storage = get_minimum_extra_storage_for_clusters(clusters);
    require(node_data.extra_storage &gt;= minimum_extra_storage,
        "Node needs a minimum of %d MiB extra storage to fulfill cluster(s) extra storage requirements.".format(minimum_extra_storage));

    validate_host(node_data.host);
    validate_url(node_data.api_url);
    validate_territory_code(node_data.territory);

    val node = node @? { node_data.pubkey } ?: create node (
        provider,
        node_data.pubkey,
        host = node_data.host,
        port = node_data.port,
        api_url = node_data.api_url,
        last_updated = op_context.last_block_time,
        cluster_units = node_data.cluster_units,
        territory = node_data.territory,
        extra_storage = node_data.extra_storage
    );
    after_node_added(node);

    for (cluster in clusters) {
        if (roles.has_node_access(provider)) {
            add_node_to_cluster_internal(provider, node, cluster);
        } else { // provider_tier.DAPP_PROVIDER
            _add_replica_node_to_cluster_internal(cluster, node);
        }
    }
    return node;
}

function require_node_provider(node_pubkey: pubkey, provider_pubkey: pubkey) {
    val provider = require(provider @? { provider_pubkey }, "Provider %s does not exist".format(provider_pubkey));
    require_provider_auth_with_rate_limit(provider);
    val node = require_node(node_pubkey);
    require(node.provider == provider, "Must be provider of node to update its state");
    return node;
}

function require_cluster_units_for_node(cluster, node) {
    val available_cluster_units = get_available_cluster_units_for_node(node);
    require(available_cluster_units &gt;= cluster.cluster_units,
        "Node %s has %d cluster unit(s). To support cluster %s %d more unit(s) are required."
            .format(node.pubkey, node.cluster_units, cluster.name, cluster.cluster_units - available_cluster_units));
}

function require_extra_storage_for_node(cluster, node) {
    val available_extra_storage = get_available_extra_storage_for_node(node);
    require(available_extra_storage &gt;= cluster.extra_storage,
        "Node %s has %d MiB extra storage. To support cluster %s %d more MiBs are required."
            .format(node.pubkey, node.extra_storage, cluster.name, cluster.extra_storage - available_extra_storage));
}

function get_available_cluster_units_for_node(node): integer {
    return node.cluster_units - get_used_cluster_units_for_node(node);
}

function get_used_cluster_units_for_node(node): integer {
    return (cluster_node, cluster) @ { node, cluster_node.cluster == cluster } ( @sum cluster.cluster_units ) +
        (cluster_replica_node, cluster) @ { node, cluster_replica_node.cluster == cluster } ( @sum cluster.cluster_units );
}

function get_available_extra_storage_for_node(node): integer {
    return node.extra_storage - get_used_extra_storage_for_node(node);
}

function get_used_extra_storage_for_node(node): integer {
    return (cluster_node, cluster) @ { node, cluster_node.cluster == cluster } ( @sum cluster.extra_storage ) +
        (cluster_replica_node, cluster) @ { node, cluster_replica_node.cluster == cluster } ( @sum cluster.extra_storage );
}

function get_cluster_count_for_node(node): integer {
    return cluster_node @ { node } ( @sum 1 ) + cluster_replica_node @ { node } ( @sum 1 );
}

@extendable function after_node_added(node) {}

@extendable function before_node_removal(node) {}

@extendable function after_node_updated(node) {}</string>
                            </entry>
                            <entry key="common/operations/common_operations_blockchain.rell">
                                <string>operation add_blockchain_replica(my_pubkey: pubkey, blockchain_rid: byte_array, node_pubkey: byte_array) {
    val provider = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(provider);
    val node = require_node(node_pubkey, false);
    val blockchain = require_active_blockchain(blockchain_rid);
    require(empty(blockchain_replica_node @? { blockchain, node }), "Node is already a replica of blockchain %s".format(node.pubkey, blockchain.rid));
    require(node.provider == provider, "It is only allowed to add own node as a blockchain replica");
    create blockchain_replica_node(blockchain, node);
}

operation remove_blockchain_replica(my_pubkey: pubkey, blockchain_rid: byte_array, node_pubkey: byte_array) {
    val provider = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(provider);
    val blockchain = require_active_blockchain(blockchain_rid);
    val node = require_node(node_pubkey);
    require(node.provider == provider, "It is only allowed to remove own blockchain replica node; provider:%s, node: %s".format(provider.pubkey, node.pubkey));
    delete blockchain_replica_node @? { blockchain, node };
}</string>
                            </entry>
                            <entry key="common/operations/common_operations_cluster.rell">
                                <string>/**
 * Adding a node to a cluster automatically makes it signer of all bc in this cluster.
 * Only one node per provider and cluster.
 */
operation add_node_to_cluster(my_pubkey: pubkey, node_pubkey: pubkey, cluster_name: text) {
    val provider = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(provider);
    val cluster = require_cluster(cluster_name);
    val node = require_node(node_pubkey, false);
    require(node.provider == provider, "It is only allowed to add own node to a cluster");
    add_node_to_cluster_internal(provider, node, cluster);
}

operation add_replica_node_to_cluster(my_pubkey: pubkey, node_pubkey: pubkey, cluster_name: text) {
    val provider = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(provider);
    val cluster = require_cluster(cluster_name);
    val node = require_node(node_pubkey, false);
    require(empty(cluster_replica_node @* { cluster, node }), "Node %s is already a replica of the cluster %s".format(node_pubkey, cluster_name));
    require(node.provider == provider, "It is only allowed to add own node as a container replica");
    require(empty(cluster_node @* { cluster, node }), "Node %s is a cluster node and can't be added to cluster %s as a replica node".format(node_pubkey, cluster_name));
    _add_replica_node_to_cluster_internal(cluster, node);
}

operation remove_replica_node_from_cluster(my_pubkey: pubkey, node_pubkey: pubkey, cluster_name: text) {
    val provider = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(provider);
    val cluster = require_cluster(cluster_name);
    val node = require_node(node_pubkey);
    require(exists(cluster_replica_node @* { cluster, node }), "Node %s is not a replica of the cluster %s".format(node_pubkey, cluster_name));
    require(node.provider == provider, "It is only allowed to remove own container replica node");
    _remove_replica_node_from_cluster_internal(cluster, node);
}</string>
                            </entry>
                            <entry key="common/operations/common_operations_node.rell">
                                <string>/**
 * @deprecated use register_node_with_node_data instead
 */
operation register_node(my_pubkey: pubkey, node_pubkey: pubkey, host: text, port: integer, api_url: text, clusters:list&lt;text&gt; = list&lt;text&gt;()) {
    register_node_impl_dep(my_pubkey, node_pubkey, host, port, api_url, clusters, "", 1);
}

/**
 * @deprecated use register_node_with_node_data instead
 */
operation register_node_with_units(my_pubkey: pubkey, node_pubkey: pubkey, host: text, port: integer, api_url: text, clusters:list&lt;text&gt; = list&lt;text&gt;(), cluster_units: integer) {
    register_node_impl_dep(my_pubkey, node_pubkey, host, port, api_url, clusters, "", cluster_units);
}

/**
 * @deprecated use register_node_with_node_data instead
 */
operation register_node_with_territory_and_units(my_pubkey: pubkey, node_pubkey: pubkey, host: text, port: integer, api_url: text, territory: text, cluster_units: integer, clusters:list&lt;text&gt; = list&lt;text&gt;()) {
    register_node_impl_dep(my_pubkey, node_pubkey, host, port, api_url, clusters, territory, cluster_units);
}

/**
 * @deprecated use register_node_with_node_data instead
 */
function register_node_impl_dep(my_pubkey: pubkey, node_pubkey: pubkey, host: text, port: integer, api_url: text, clusters:list&lt;text&gt; = list&lt;text&gt;(), territory: text, cluster_units: integer) {
    val node_data = register_node_data(node_pubkey, host = host, port, api_url = api_url,clusters=clusters, cluster_units = cluster_units, territory = territory, extra_storage = 0);
    register_node_impl(my_pubkey, node_data);
}

operation register_node_with_node_data(my_pubkey: pubkey, node_data: register_node_data) {
    register_node_impl(my_pubkey, node_data);
}

function register_node_impl(my_pubkey: pubkey, node_data: register_node_data) {
    log("------------- Register-Node: %s --------------".format(node_data.pubkey));
    val provider = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(provider);
    require_pubkey(node_data.pubkey);
    add_node_internal(provider, node_data);
    log("Added node information for node: " + node_data.pubkey);
    log("-----------------------------------");
}

/**
 * @deprecated use replace_node_with_node_data instead
 */
operation replace_node(my_pubkey: pubkey, old_node_key: pubkey, new_node_key: pubkey, new_host: text?, new_port: integer?, new_api_url: text?) {
    replace_node_with_units_impl(my_pubkey, old_node_key, new_node_key, new_host, new_port, new_api_url, new_cluster_units = 1, new_territory = null);
}

/**
 * @deprecated use replace_node_with_node_data instead
 */
operation replace_node_with_units(my_pubkey: pubkey, old_node_key: pubkey, new_node_key: pubkey, new_host: text?, new_port: integer?, new_api_url: text?, cluster_units: integer) {
    replace_node_with_units_impl(my_pubkey, old_node_key, new_node_key, new_host, new_port, new_api_url, new_cluster_units = cluster_units, new_territory = null);
}

/**
 * @deprecated use replace_node_with_node_data instead
 */
operation replace_node_with_units_and_territory(my_pubkey: pubkey, old_node_key: pubkey, new_node_key: pubkey, new_host: text?, new_port: integer?, new_api_url: text?, new_territory: text?, cluster_units: integer) {
    replace_node_with_units_impl(my_pubkey, old_node_key, new_node_key, new_host, new_port, new_api_url, new_cluster_units = cluster_units, new_territory = new_territory);
}

/**
 * @deprecated use replace_node_with_node_data instead
 */
function replace_node_with_units_impl(my_pubkey: pubkey, old_node_key: pubkey, new_node_key: pubkey, new_host: text?, new_port: integer?, new_api_url: text?, new_territory: text?, new_cluster_units: integer) {
    val node_data = replace_node_data(old_node_key, new_node_key, new_host, new_port, new_api_url, new_cluster_units, new_territory, new_extra_storage = 0);
    replace_node_with_node_data_impl(my_pubkey, node_data);
}

operation replace_node_with_node_data(my_pubkey: pubkey, node_data: replace_node_data) {
    replace_node_with_node_data_impl(my_pubkey, node_data);
}

function replace_node_with_node_data_impl(my_pubkey: pubkey, node_data: replace_node_data) {
    log("------------- Replace-Node: %s --------------".format(node_data.old_node_key));
    val me = require_is_provider_with_rate_limit(my_pubkey);
    require_is_signer(node_data.old_node_key);
    require_is_signer(node_data.new_node_key);
    val old_node = node @ { node_data.old_node_key };
    require(old_node.provider == me, "Node must be owned by provider");
    _require_replace_cluster_units(old_node, node_data);
    _require_replace_extra_storage(old_node, node_data);

    val cluster_names = cluster_node @* { old_node }.cluster.name;
    delete cluster_node @* { old_node };
    val clusters_replicated_by_node = cluster_replica_node @* { old_node } (.cluster);
    delete cluster_replica_node @* { old_node };
    val blockchains_replicated_by_node = blockchain_replica_node @* { old_node } (.blockchain);
    delete blockchain_replica_node @* { old_node };

    if (exists(node_data.new_host)) {
        validate_host(node_data.new_host!!);
    }
    if (exists(node_data.new_api_url)) {
        validate_url(node_data.new_api_url!!);
    }
    if (exists(node_data.new_territory)) {
        validate_territory_code(node_data.new_territory!!);
    }

    val register_node_data = register_node_data(
        node_data.new_node_key, 
        host = node_data.new_host ?: old_node.host, 
        port = node_data.new_port ?: old_node.port,
        api_url = node_data.new_api_url ?: old_node.api_url,
        clusters = cluster_names,
        cluster_units = node_data.new_cluster_units ?: old_node.cluster_units,
        territory = node_data.new_territory ?: old_node.territory,
        extra_storage = node_data.new_extra_storage ?: old_node.extra_storage);

    before_node_removal(old_node);
    delete old_node;
    val new_node = add_node_internal(me, register_node_data, require_not_exists = false);

    for (cl in clusters_replicated_by_node) {
        _add_replica_node_to_cluster_internal(cl, new_node);
    }

    for (blockchain in blockchains_replicated_by_node) {
        if (not(exists(blockchain_replica_node @? { blockchain, new_node }))) {
            create blockchain_replica_node(blockchain, new_node);
        }
    }

    log("---------------------------------------");
}

function _require_replace_cluster_units(old_node: node, node_data: replace_node_data) {
    val new_cluster_units = node_data.new_cluster_units;
    if (new_cluster_units != null) {
        val old_cluster_units_used = get_used_cluster_units_for_node(old_node);
        require(new_cluster_units &gt;= old_cluster_units_used, "Node must have at least %d cluster_units".format(old_cluster_units_used));
    }
}

function _require_replace_extra_storage(old_node: node, node_data: replace_node_data) {
    val new_extra_storage = node_data.new_extra_storage;
    if (new_extra_storage != null) {
        val old_extra_storage_used = get_used_extra_storage_for_node(old_node);
        require(new_extra_storage &gt;= old_extra_storage_used, "Node must have at least %d MiB extra storage".format(old_extra_storage_used));
    }
}    

operation enable_node(my_pubkey: pubkey, node_pubkey: pubkey) {
    log("------------- Enable-Node: %s --------------".format(node_pubkey));
    val provider = require_is_provider_with_rate_limit(my_pubkey);
    val node = require_node(node_pubkey);
    require(node.provider == provider, "Node must be owned by provider: " + node_pubkey);
    require(not(node.active), "Node is already active: " + node_pubkey);
    node.active = true;
    after_node_added(node);
    log("-----------------------------------");
}

operation disable_node(my_pubkey: pubkey, node_pubkey: pubkey) {
    val provider = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(provider);
    val node_to_disable = require_node(node_pubkey);

    if (provider != node_to_disable.provider) {
        require(roles.has_system_access(provider), "Non system provider is only allowed to disable own nodes");
        require(
            node_to_disable.provider.tier == provider_tier.NODE_PROVIDER and not(node_to_disable.provider.system),
            "System provider is only allowed to disable nodes of non system provider");
    }

    node_to_disable.active = false;

    // cluster nodes
    val clusters = cluster_node @* { node_to_disable } .cluster;
    delete cluster_node @* { node_to_disable };
    for (cl in clusters) {
        update_configuration_signers(cl, node_to_disable);
    }
    after_node_removed_from_cluster(node_to_disable, null);

    // cluster replica nodes
    delete cluster_replica_node @* { .node.pubkey == node_pubkey };
    after_replica_node_removed_from_cluster(node_to_disable, null);

    // blockchain replica nodes
    delete blockchain_replica_node @* { .node.pubkey == node_pubkey };
}

operation remove_node(my_pubkey: pubkey, node_pubkey: pubkey) {
    log("------------- Remove-Node: %s --------------".format(node_pubkey));
    val provider = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(provider);
    val node_to_remove = require_node(node_pubkey);
    require(provider == node_to_remove.provider, "It is only allowed to remove own nodes: " + node_pubkey);
    require(not(node_to_remove.active), "Can't remove active nodes: " + node_pubkey);
    before_node_removal(node_to_remove);
    delete node_to_remove;
    log("---------------------------------------");
}

/**
 * @deprecated use update_node_with_node_data instead
 */
operation update_node(my_pubkey: pubkey, node_pubkey: pubkey, host: text? = null, port: integer? = null, api_url: text? = null) {
    update_node_impl(my_pubkey, node_pubkey, host, port, api_url, null, null);
}

/**
 * @deprecated use update_node_with_node_data instead
 */
operation update_node_with_units(my_pubkey: pubkey, node_pubkey: pubkey, host: text? = null, port: integer? = null, api_url: text? = null, cluster_units: integer? = null) {
    update_node_impl(my_pubkey, node_pubkey, host, port, api_url, null, cluster_units);
}

/**
 * @deprecated use update_node_with_node_data instead
 */
operation update_node_with_territory_and_units(my_pubkey: pubkey, node_pubkey: pubkey, host: text? = null, port: integer? = null, api_url: text? = null, territory: text? = null, cluster_units: integer? = null) {
    update_node_impl(my_pubkey, node_pubkey, host, port, api_url, territory, cluster_units);
}

/**
 * @deprecated use update_node_with_node_data instead
 */
function update_node_impl(my_pubkey: pubkey, node_pubkey: pubkey, host: text?, port: integer?, api_url: text?, territory: text?, cluster_units: integer?) {
    val node_data = update_node_data(node_pubkey, host, port, api_url, cluster_units, territory, extra_storage = null);
    update_node_with_node_data_impl(my_pubkey, node_data);
}

operation update_node_with_node_data(my_pubkey: pubkey, node_data: update_node_data) {
    update_node_with_node_data_impl(my_pubkey, node_data);
}

function update_node_with_node_data_impl(my_pubkey: pubkey, node_data: update_node_data) {
    val node = require_node_provider(node_data.pubkey, my_pubkey);
    if (node_data.host != null) {
        validate_host(node_data.host!!);
        node.host = node_data.host!!;
    }
    if (node_data.port != null) node.port = node_data.port!!;
    if (node_data.api_url != null) {
        validate_url(node_data.api_url!!);
        node.api_url = node_data.api_url!!;
    }
    if (node_data.territory != null) {
        validate_territory_code(node_data.territory!!);
        node.territory = node_data.territory!!;
    }
    if (
        node_data.host != null or
        node_data.port != null or
        node_data.api_url != null or
        node_data.territory != null or
        node_data.cluster_units != null or
        node_data.extra_storage != null
    ) node.last_updated = op_context.last_block_time;
    val cluster_units = node_data.cluster_units;
    if (cluster_units != null) {
        require(cluster_units &gt; 0, "Node must have at least 1 cluster_unit");
        val needed_cluster_units = get_used_cluster_units_for_node(node);
        require(cluster_units &gt;= needed_cluster_units,
            "Can not update cluster units to %d since the node needs at least %d to support current clusters.".format(cluster_units, needed_cluster_units));
        node.cluster_units = cluster_units;
    }
    val extra_storage = node_data.extra_storage;
    if (extra_storage != null) {
        val needed_extra_storage = get_used_extra_storage_for_node(node);
        require(extra_storage &gt;= needed_extra_storage,
            "Can not update extra storage to %d MiB since the node needs at least %d MiB to support current clusters.".format(extra_storage, needed_extra_storage));
        node.extra_storage = extra_storage;
    }
    after_node_updated(node);
}</string>
                            </entry>
                            <entry key="common/operations/common_operations_provider.rell">
                                <string>/*
    The provider with type in line can (+) / can not (-) register a provider with the type in the column:

         | DP | NP
    -----|-----|-----
     DP |  +  |  -
     NP  |  +  |  -
     SP  |  +  |  +
*/
operation register_provider(my_pubkey: pubkey, pubkey, provider_tier) {
    val me = require_provider(my_pubkey);
    require_node_access(me);
    require_provider_auth_with_rate_limit(me);
    require_pubkey(pubkey);
    require(empty(provider @* { pubkey }), "Provider already exists: " + pubkey);

    register_and_enable_provider(
        provider_info(pubkey),
        provider_tier,
        cluster = null,
        voter_set = null,
        enabled_by_default = not(_is_node_provider(provider_tier))
    );
    if (_is_node_provider(provider_tier)) {
        require(roles.has_system_access(me), "Must be system provider to add a node provider");
        val provider = provider @ { pubkey };
        enroll.node(provider);
        after_provider_updated(provider);
    }
}

operation promote_node_provider(my_pubkey: pubkey, provider_pubkey: pubkey) { // TODO: Should be able to demote
    val me = require_is_provider_with_rate_limit(my_pubkey);
    require_is_system_provider(my_pubkey);
    val p = require_provider(provider_pubkey);
    require(p.tier != provider_tier.NODE_PROVIDER, "Provider already has this role");
    p.active = false;
    enroll.node(p);
    after_provider_updated(p);
}

operation transfer_action_points(from: pubkey, to: pubkey, amount: integer) {
    val _from = require_is_provider_with_rate_limit(from);
    val _to = require_provider(to);
    require(provider_rl_state @ { _from } .points &gt;= amount, "Not enough action points to transfer from.");
    require(amount &gt; 0, "Amount must be greater than 0");
    update provider_rl_state @ { _from } ( .points -= amount );
    update provider_rl_state @ { _to } ( .points += amount );
}

operation update_provider(my_pubkey: pubkey, name?, url: text?) {
    val me = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(me);
    if (exists(name)) {
        validate_metadata_text("name", name);
        me.name = name;
    }
    if (exists(url)) {
        validate_url(url);
        me.url = url;
    }
}
</string>
                            </entry>
                            <entry key="common/operations/module.rell">
                                <string>module;

import ^.*;
import model.*;</string>
                            </entry>
                            <entry key="common/provider.rell">
                                <string>function _is_node_provider(tier: provider_tier) = tier == provider_tier.NODE_PROVIDER;

@extendable function register_and_enable_provider(provider_info, provider_tier, cluster?, voter_set?, enabled_by_default: boolean = false) {
    require(empty(node @? { provider_info.pubkey }), "This pubkey is already registered as a node: " + provider_info.pubkey);
    val provider = create provider(
        provider_info.pubkey,
        name = provider_info.name,
        url = provider_info.url,
        active = enabled_by_default,
        tier = provider_tier
    );
    val max_actions_per_day = provider_quota @ {
        .tier == provider_tier,
        provider_quota_type.max_actions_per_day
    } .value;
    create provider_rl_state(provider, points = max_actions_per_day, last_update=op_context.last_block_time);
    if (exists(cluster)) {
        create cluster_provider(cluster, provider);
    }
    if (exists(voter_set)) {
        create voter_set_member(voter_set, provider);
    }
    after_provider_registered(provider, cluster);
}

// Recover provider's action points and consume one, if possible
function provider_rate_limit(provider) {
    val max_actions_per_day = provider_quota @ {
            .tier == provider.tier,
            provider_quota_type.max_actions_per_day
    } .value;

    // We recover max_actions_per_day in 24 hours, find time needed to recover 1 point
    val recovery_time = (86400 * 1000) / max_actions_per_day;
    val state = provider_rl_state @ { provider } (.points, .last_update);
    val time_delta = op_context.last_block_time - state.last_update;
    var got_points = 0;
    var update_time = state.last_update;

    if (time_delta &gt; recovery_time) {
        got_points = time_delta / recovery_time;
        // advance  update_time to a multiple of recovery_time to avoid wasting time
        update_time = state.last_update + got_points * recovery_time;
        if (got_points + state.points &gt; max_actions_per_day) {
            got_points = max_actions_per_day - state.points;
            // if user is at the maximum reset his timer
            update_time = op_context.last_block_time;
        }
    }

    require(state.points + got_points &gt; 0, "Provider has no points to spend: " + provider.pubkey);

    update provider_rl_state @ { provider } (
        .points += got_points - 1,
        .last_update = update_time
    );
}

function require_provider_quota(provider, quota_type: provider_quota_type) {
    val quota = provider_quota @ { provider.tier, quota_type } (.value);
    if (quota &gt; 0) {
        when (quota_type) {
            max_nodes -&gt; _require_provider_quota(quota, quota_type, node @ { provider } (@sum 1));
            max_containers -&gt; _require_provider_quota(quota, quota_type, container @ { provider, .system == false } (@sum 1));
        }
    }
}

function disable_provider(provider) {
    provider.active = false;

    update node @* { provider } ( .active = false );

    // cluster nodes
    val cluster_nodes = cluster_node @* { .node.provider == provider } ( .cluster, .node );
    delete cluster_node @* { .node.provider == provider };
    for (cn in cluster_nodes) {
        update_configuration_signers(cn.cluster, cn.node);
        after_node_removed_from_cluster(cn.node, cn.cluster);
    }

    val pending_proposals_with_provider_as_voter = (prop: proposal, vsm: voter_set_member) @* { prop.voter_set == vsm.voter_set, vsm.provider == provider, prop.state == proposal_state.PENDING } ( prop );

    // cluster replica nodes
    val cluster_replica_nodes = cluster_replica_node @* { .node.provider == provider } ( .cluster, .node );
    delete cluster_replica_node @* { .node.provider == provider };
    for (crn in cluster_replica_nodes) {
        after_replica_node_removed_from_cluster(crn.node, crn.cluster);
    }

    // blockchain replica nodes
    delete blockchain_replica_node @* { .node.provider == provider };
    // remove disabled provider from voter_set_member table
    delete voter_set_member @* { .provider == provider };
    // if provider is last to vote on proposals make sure the proposals are voted on
    for (prop in pending_proposals_with_provider_as_voter) {
        // We should delete any vote that the removed provider has cast on the pending proposal (for correctness)
        delete vote @? { prop, provider };
        try_to_apply_proposal(prop);
    }
    after_provider_updated(provider);
}

function enable_provider(provider) {
    provider.active = true;

    // If enabled provider is a system provider, update SYSTEM_P voter set
    if (roles.has_system_access(provider)) {
        val voter_set = system_p_voter_set();
        if (empty(voter_set_member @? { voter_set, provider })) {
            create voter_set_member(voter_set, provider);
        }
    }
    after_provider_updated(provider);
}

function _require_provider_quota(quota: integer, quota_type: provider_quota_type, count: integer) {
    require(count &lt; quota , "Provider quota exceeded: %s = %d".format(quota_type, quota));
}

@extendable function after_provider_registered(provider, cluster: cluster?) {}

@extendable function after_provider_updated(provider) {}</string>
                            </entry>
                            <entry key="common/queries/common_queries_anchoring.rell">
                                <string>
query get_cluster_anchoring_configuration(): byte_array {
    return cluster_anchoring_config.raw_config;
}
</string>
                            </entry>
                            <entry key="common/queries/common_queries_blockchain.rell">
                                <string>query get_blockchain(rid: byte_array) = blockchain @ { rid };

query get_blockchain_signers(blockchain_rid: byte_array): list&lt;(byte_array, text, integer, boolean, integer)&gt; {
    val blockchain = require_active_blockchain(blockchain_rid);
    return get_blockchain_signer_nodes(blockchain) @* {} (
        @sort _ = .pubkey,
        _ = .host,
        _ = .port,
        _ = .active,
        _ = .last_updated
    );
}

query get_blockchain_replicas(blockchain_rid: byte_array) {
    return blockchain_replica_node @* { .blockchain.rid == blockchain_rid } (
        @sort .node.pubkey,
        .node.host,
        .node.port,
        .node.active,
        .node.last_updated
    );
}

query get_blockchains(include_inactive: boolean): list&lt;struct&lt;blockchain&gt;&gt; {
    if (include_inactive) {
        return blockchain @* {} ($.to_struct());
    }
    return blockchain @* { .state == blockchain_state.RUNNING } ($.to_struct());
}

// Will return RUNNING and PAUSED if include_inactive == true
function get_blockchains_with_container(include_inactive: boolean) {
    if (include_inactive) {
        return (b: blockchain, c: container_blockchain) @* { c.blockchain == b } (blockchain=b, c.container) ;
    } else {
        return (b: blockchain, c: container_blockchain) @* { c.blockchain == b and b.state == blockchain_state.RUNNING } (blockchain = b, c.container);
    }
}

struct blockchain_info {
    rid: byte_array;
    name: text;
    state: blockchain_state;
    container: name?;
    cluster: name?;
    system: boolean?;
    is_foreign_importing: boolean?;
    is_moving: boolean?;
    is_unarchiving: boolean?;
}

query get_blockchain_info(rid: byte_array): blockchain_info? {
    val blockchain = blockchain @? { rid };
    if (blockchain != null) {
        val container = container_blockchain @? { blockchain } (.container);
        return blockchain_info(
            rid = blockchain.rid,
            name = blockchain.name,
            state = blockchain.state,
            container = container?.name,
            cluster = container?.cluster?.name,
            system = blockchain.system,
            is_foreign_importing = importing_foreign_blockchain @? { .blockchain_rid == rid } ??,
            is_moving = moving_blockchain @? { blockchain } ??,
            is_unarchiving = unarchiving_blockchain @? { blockchain } ??
        );
    } else {
        return null;
    }
}

// Will return RUNNING, PAUSED and REMOVED if include_inactive == true
query get_blockchain_info_list(include_inactive: boolean): list&lt;blockchain_info&gt; {
    var bc_infos = list&lt;(blockchain: blockchain, container: container?)&gt;();
    if (include_inactive) {
        val connected_blockchains = container_blockchain @* {} (.blockchain, .container);
        val removed_blockchains = blockchain @* {
            blockchain not in connected_blockchains @* {}.blockchain
        } (blockchain = blockchain, container = null);
        bc_infos.add_all(connected_blockchains);
        bc_infos.add_all(removed_blockchains);
    } else {
        bc_infos.add_all((b: blockchain, c: container_blockchain) @* { c.blockchain == b and b.state == blockchain_state.RUNNING } (blockchain = b, c.container));
    }

    val result = list&lt;blockchain_info&gt;();
    for (bc in bc_infos) {
        result.add(blockchain_info(
            rid = bc.blockchain.rid,
            name = bc.blockchain.name,
            state = bc.blockchain.state,
            container = bc.container?.name,
            cluster = bc.container?.cluster?.name,
            system = bc.blockchain.system,
            is_foreign_importing = importing_foreign_blockchain @? { .blockchain_rid == bc.blockchain.rid } ??,
            is_moving = moving_blockchain @? { bc.blockchain } ??,
            is_unarchiving = unarchiving_blockchain @? { bc.blockchain } ??
        ));
    }
    return result;
}

query get_blockchain_cluster(blockchain_rid: byte_array) = cm_get_blockchain_cluster(blockchain_rid);

query get_blockchain_api_urls(blockchain_rid: byte_array) = cm_get_blockchain_api_urls(blockchain_rid);

struct blockchain_signers_info {
    height: integer;
    signers: list&lt;pubkey&gt;;
}

query get_blockchain_signer_updates(blockchain_rid: byte_array, from_height: integer): list&lt;blockchain_signers_info&gt; {
    val blockchain = require_blockchain(blockchain_rid);
    return blockchain_configuration_signers @* { blockchain, .height &gt;= from_height }
        (@omit @sort .height, blockchain_signers_info(.height, list&lt;pubkey&gt;.from_gtv(gtv.from_bytes(.signers))));
}

query get_compressed_configuration_parts(configuration_part_hashes: set&lt;byte_array&gt;): list&lt;byte_array&gt; =
        compressed_blockchain_configuration_part @* { .hash in configuration_part_hashes }.hash;

query get_importing_foreign_blockchain_info(blockchain_rid: byte_array): struct&lt;importing_foreign_blockchain&gt;? =
        importing_foreign_blockchain @? { blockchain_rid } ($.to_struct());

query get_moving_blockchain_info(blockchain_rid: byte_array) = moving_blockchain @? { .blockchain.rid == blockchain_rid } (
    source_container = .source.name,
    destination_container = .destination.name,
    final_height = .final_height
);

query get_unarchiving_blockchain_info(blockchain_rid: byte_array) = unarchiving_blockchain @? { .blockchain.rid == blockchain_rid } (
    source_container = .source.name,
    destination_container = .destination.name,
    final_height = .final_height
);
</string>
                            </entry>
                            <entry key="common/queries/common_queries_cluster.rell">
                                <string>
query get_cluster(name) = cluster @ { name };

query get_clusters() {
    return cluster @* {} (
        name = .name,
        governor = .governance.name,
        operational = .operational
    );
}

struct cluster_data {
    name;
    governor: text;
    is_operational: boolean;
    cluster_units: integer?;
    extra_storage: integer?;
    number_of_nodes: integer?;
    container_units_available: integer?;
    extra_storage_available: integer?;
}

query get_cluster_data(name) {
    val cluster = require_cluster(name);
    return cluster_data(
        name = cluster.name,
        governor = cluster.governance.name,
        is_operational = cluster.operational,
        cluster_units = cluster.cluster_units,
        extra_storage = cluster.extra_storage,
        number_of_nodes = number_of_nodes_in_cluster(cluster),
        container_units_available = get_available_container_units(cluster),
        extra_storage_available = get_available_extra_storage(cluster)
    );
}

query get_operational_dapp_clusters(): list&lt;cluster_data&gt; {
    val counts = map(cluster_node @* { .cluster.operational == true, .cluster.name != clusters.system } (@group .cluster, @sum 1));
    return cluster @* { .operational == true, .name != clusters.system } (cluster_data(
        name = .name,
        governor = .governance.name,
        is_operational = .operational,
        cluster_units = .cluster_units,
        extra_storage = .extra_storage,
        number_of_nodes = counts.get_or_default($, 0),
        container_units_available = get_available_container_units($),
        extra_storage_available = get_available_extra_storage($)
    ));
}

query get_cluster_providers(name): list&lt;(name:text, pubkey:pubkey)&gt; {
    val cluster = require_cluster(name);
    return cluster_provider @* { cluster } (name = .provider.name, pubkey = .provider.pubkey);
}

query get_cluster_nodes(name) {
    val cluster = require_cluster(name);
    return cluster_node @* { cluster } (
        pubkey = .node.pubkey,
        host = .node.host,
        port = .node.port,
        api_url = .node.api_url,
        active = .node.active
    );
}

query get_cluster_replica_nodes(name) {
    val cluster = require_cluster(name);
    return cluster_replica_node @* { cluster } (
        pubkey = .node.pubkey,
        host = .node.host,
        port = .node.port,
        api_url = .node.api_url,
        active = .node.active
    );
}

query get_cluster_containers(cluster_name: text): list&lt;(name:text, deployer:text)&gt; {
    return container @* { .cluster.name == cluster_name } ( @sort name = .name, deployer = .deployer.name);
}

query get_cluster_blockchains(name) = cm_get_cluster_blockchains(name);

query get_cluster_api_urls(name) {
    return (cluster_node, cluster) @* { cluster.name == name and cluster == cluster_node.cluster } ( cluster_node.node.api_url );
}

</string>
                            </entry>
                            <entry key="common/queries/common_queries_container.rell">
                                <string>
query get_container(name) = container @ { name };

struct container_data {
    name: text;
    cluster: text;
    deployer: text;
    proposed_by_pubkey: byte_array;
    proposed_by_name: text;
    system: boolean;
    state: container_state?;
}

query get_container_data(name): container_data {
    val container = require_container(name);
    return container_data(
        name = container.name,
        cluster = container.cluster.name,
        deployer = container.deployer.name,
        proposed_by_pubkey = container.proposed_by.pubkey,
        proposed_by_name = container.proposed_by.name,
        system = container.system,
        state = container.state
    );
}

query get_container_blockchain(name) {
    val container = require_container(name);
    return container_blockchain @* { container } (
        rid = .blockchain.rid,
        name = .blockchain.name,
        system = .blockchain.system,
        state = .blockchain.state
    );
}

query get_containers(): list&lt;(cluster:text, name:text, deployer:text)&gt; {
    return container @* {} (@sort cluster = .cluster.name, @sort name = .name, deployer = .deployer.name);
}
</string>
                            </entry>
                            <entry key="common/queries/common_queries_node.rell">
                                <string>
query get_node(pubkey) = node @ { pubkey };

struct node_data {
    provider: pubkey;
    pubkey;
    active: boolean;
    host: text;
    port: integer;
    last_updated: timestamp;
    api_url: text;
    cluster_units: integer?;
    territory: text?;
    extra_storage: integer?;
}

query get_node_data(pubkey) {
    val node = require_node(pubkey);
    return node_data(
        provider = node.provider.pubkey,
        pubkey = node.pubkey,
        active = node.active,
        host = node.host,
        port = node.port,
        last_updated = node.last_updated,
        api_url = node.api_url,
        cluster_units = node.cluster_units,
        territory = node.territory,
        extra_storage = node.extra_storage
    );
}

query list_clusters_of_node(pubkey): list&lt;text&gt; {
    val node = require_node(pubkey);
    return cluster_node @* { node }.cluster.name;
}

query get_node_containers(pubkey): list&lt;(cluster:text, name:text, deployer:text)&gt; {
    val node = require_node(pubkey);
    val clusters = cluster_node @* { node } .cluster;
    val res = list&lt;(cluster:text, name:text, deployer:text)&gt;();
    for (cl in clusters) {
        val containers = container @* { cl } (@sort cluster = .cluster.name, @sort name = .name, deployer = .deployer.name);
        res.add_all(containers);
    }
    return res;
}

// deprecated
query get_nodes_with_provider() {
    return node @* {} (
        pubkey = .pubkey,
        node_active = .active,
        host = .host,
        port = .port,
        last_updated = .last_updated,
        name = .provider.name,
        provider_active = .provider.active,
        @sort provider = .provider.pubkey
    );
}

struct node_info {
    pubkey;
    host: text;
    port: integer;
    api_url: text;
    territory: text?;
}

query get_all_nodes(include_inactive: boolean) {
    val nodes = if (include_inactive) node @* {} else node @* { .active };
    return nodes @* {} (
        info = node_info(.pubkey, host = .host, port = .port, api_url = .api_url, territory = .territory),
        active = .active,
        last_updated = .last_updated,
        provider = .provider.to_struct()
    );
}
</string>
                            </entry>
                            <entry key="common/queries/common_queries_provider.rell">
                                <string>struct provider_node_data {
    pubkey;
    active: boolean;
    host: text;
    port: integer;
    api_url: text;
    last_updated: timestamp;
    territory: text?;
}

query get_nodes_by_provider(pubkey) {
    return node @* { provider @ { pubkey } } (
        provider_node_data(
            .pubkey,
            .active,
            .host,
            .port,
            .api_url,
            .last_updated,
            .territory
        )
    );
}

query get_provider_points(pubkey): integer {
    return provider_rl_state @ {provider @ { pubkey }} .points;
}

query get_provider_clusters(pubkey) : list&lt;text&gt; {
    return cluster_provider @* { provider @ {pubkey} }.cluster.name;
}

query get_provider_data (pubkey) {
    return provider @ { pubkey } ($.to_struct());
}

query get_all_providers(): list&lt;struct&lt;provider&gt;&gt; {
    return provider @* {} ($.to_struct());
}

query get_providers(tier: provider_tier, system: boolean, require_active: boolean): list&lt;struct&lt;provider&gt;&gt; {
    if (system) {
        require(tier == provider_tier.NODE_PROVIDER, "System providers are always node providers");
    }
    return  provider @* { 
        tier,
        if (system) .system else true, 
        if (require_active) .active else true 
     } ($.to_struct());
}

query get_provider_quotas(): list&lt;struct&lt;provider_quota&gt;&gt; {
    return provider_quota @* {} ($.to_struct());
}
</string>
                            </entry>
                            <entry key="common/queries/common_queries_voter_set.rell">
                                <string>query list_voter_sets() {
    return voter_set@* {} ( voter_set.to_struct() );
}

query get_voter_set_info(name): (name:text, threshold:integer, governor:text, members:list&lt;pubkey&gt;) {
    val v = require(voter_set @? { name }, "Voter set " + name + " not found");
    val governance = voter_set_governance @ { .voter_set == v };
    val members = voter_set_member @* { v } ( .provider.pubkey );
    return (
        name = v.name,
        threshold = v.threshold,
        governor = governance.governor.name,
        members = members
    );
}

query get_voter_set(name) = voter_set @ { name };

query get_voter_set_governor(name) {
    return voter_set_governance @ { .voter_set == voter_set @ { name } } .governor.name;
}

query get_voter_set_members(name) {
    return voter_set_member @* { voter_set @ {name} } (.provider.pubkey);
}

query get_voter_sets() {
    return (voter_set, voter_set_governance) @* { voter_set == voter_set_governance.voter_set } (name = voter_set.name, threshold = voter_set.threshold, @sort gorvernor = voter_set_governance.governor.name);
}</string>
                            </entry>
                            <entry key="common/queries/module.rell">
                                <string>module;

import ^.*;
import cm_api.*;
import model.*;

query get_summary() {
    return (
        providers = provider @{} (@sum 1),
        clusters = cluster @{} (@sum 1),
        containers = container @{} (@sum 1),
        voter_sets = voter_set @{} (@sum 1),
        nodes = node @{} (@sum 1),
        blockchains = blockchain @{} (@sum 1)
    );
}
</string>
                            </entry>
                            <entry key="common/require.rell">
                                <string>function require_is_signer(pubkey) {
    require(op_context.is_signer(pubkey), "Operation must be signed by " + pubkey);
}

function require_provider(pubkey) = require(provider @? { pubkey }, "Unknown provider " + pubkey);
function require_cluster(name) = require(cluster @? { name }, "Cluster %s not found".format(name));
function require_clusters(names: list&lt;text&gt;): list&lt;cluster&gt; {
    val clusters = cluster @* { .name in names };
    if (clusters.size() != names.size()) {
        val missing_clusters = list&lt;text&gt;();
        for (name in names) {
            val cl = clusters @? { name };
            if (cl == null) missing_clusters.add(name);
        }
        require(missing_clusters.empty(), "Clusters %s not found".format(missing_clusters.to_text()));
    }
    return clusters;
}
function require_node(pubkey, include_disabled: boolean = true) = require(node @? { pubkey, include_disabled or .active },
    "Node not found" + (if(include_disabled) "" else " or disabled") + ": " + pubkey);
function require_container(name) = require(container @? { name }, "Container %s not found".format(name));
function require_running_container(name) = require(container @? { name, .state == container_state.RUNNING }, "Container %s not found or is not running".format(name));
function require_voter_set(name) = require(voter_set @? { name }, "Voter set %s does not exist".format(name));
function require_active_blockchain(blockchain_rid: byte_array) =
    require_blockchain(
        blockchain_rid,
        allowed_states = [
            blockchain_state.RUNNING,
            blockchain_state.PAUSED,
            blockchain_state.IMPORTING,
            blockchain_state.UNARCHIVING,
        ]
    );
function require_blockchain(
    blockchain_rid: byte_array,
    allowed_states: list&lt;blockchain_state&gt;? = null
) = require(
    blockchain @? {
        blockchain_rid,
        allowed_states == null or allowed_states.size() == 0 or .state in allowed_states
    }, "Unknown blockchain " + blockchain_rid
);

function require_is_system_provider(pubkey) {
    val p = require_provider(pubkey);
    require_system_access(p);
    return p;
}

function require_system_p_member(provider) {
    require_voter_set_member(system_p_voter_set(), provider);
}

function require_cluster_governor(cluster, provider) {
    require_voter_set_member(cluster.governance, provider);
}

function require_voter_set_governor(voter_set, provider) {
    val governor = voter_set_governance @ { voter_set } .governor;
    require_voter_set_member(governor, provider);
}

function require_container_deployer(container, provider) {
    require(roles.has_deploy_access(provider, container), "Provider %s is not a deployer of container %s".format(provider.pubkey, container.name));
}

function require_voter_set_member(voter_set, provider) {
    require(exists(voter_set_member @* { voter_set, provider}), "Provider is not a member of voter set " + voter_set.name);
}

function require_is_provider_with_rate_limit(pubkey) {
    val provider = require_provider(pubkey);
    require_provider_auth_with_rate_limit(provider);
    return provider;
}

// This is the function we should use for auth
function require_provider_auth_with_rate_limit(provider) {
    require_is_signer(provider.pubkey);
    require(provider.active, "Provider is not enabled");
    provider_rate_limit(provider);
}

function require_pubkey(pubkey) {
    require(pubkey.size() == 33 or pubkey.size() == 65 or pubkey.size() == 1336, "Value is not pubkey: " + pubkey);
}
</string>
                            </entry>
                            <entry key="common/util.rell">
                                <string>function make_config_unique(config_data: byte_array): byte_array {
    val config_map = map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(config_data));

    val chain0_block_rid = if (op_context.block_height &gt;= 1)
        block @ { .block_height == op_context.block_height - 1 } ( .block_rid )
    else
        chain_context.blockchain_rid;
    config_map["chain0_last_block_rid"] = chain0_block_rid.to_gtv();
    config_map["chain0_block_height"] = op_context.block_height.to_gtv();
    config_map["chain0_tx_rid"] = op_context.transaction.tx_rid.to_gtv();
    config_map["chain0_op_index"] = op_context.op_index.to_gtv();

    return config_map.to_gtv().to_bytes();
}

function require_height_is_greater_or_equal_to_last_config_height(blockchain, height: integer) {
    val last_config_height = require(blockchain_configuration @? { blockchain } (@sort_desc .height) limit 1,
        "No configurations found for blockchain: " + blockchain.rid
    );
    require(height &gt;= last_config_height,
        "Given height %d must be greater than or equal to the last config height %d for blockchain %s"
            .format(height, last_config_height, blockchain.rid)
    );
}

function get_voter_set_members_sorted(name) {
    return voter_set_member @* { voter_set @ {name} } ( @sort .provider.pubkey );
}
</string>
                            </entry>
                            <entry key="common/validation.rell">
                                <string>function validate_entity_name(name: text) = require(
    name.matches("^[a-zA-Z0-9_]{0,64}$"),
    "Entity name can only contain letters, numerals, and underscores. Maximum allowed length is 64 characters."
);

function validate_metadata_text(parameter_name: text, value: text) = require(
    value.size() &lt;= 1000,
    "%s must not exceed 1000 characters".format(parameter_name)
);

function validate_url(url: text) = require(url.size() &lt;= 1000, "URL must not exceed 1000 characters");

function validate_host(host: text) = require(host.size() &lt;= 255, "Host must not exceed 255 characters");

function validate_territory_code(territory: text) = require(
    territory.empty() or territory.size() == 2,
    "Territory code must be two characters or empty"
);
</string>
                            </entry>
                            <entry key="constants/module.rell">
                                <string>module;

namespace blockchains {
    val directory_chain = "directory_chain";
    val system_anchoring = "system_anchoring";
    val evm_event_receiver_chain = "evm_event_receiver_chain";
    val evm_event_receiver_price_oracle_chain = "evm_event_receiver_price_oracle_chain";
    val evm_transaction_submitter_chain = "evm_transaction_submitter_chain";
    val cluster_anchoring_prefix = "cluster_anchoring_";
    val price_oracle_chain_name = "price_oracle_chain";
}
</string>
                            </entry>
                            <entry key="economy_chain_in_directory_chain/change_dapp_providers_state.rell">
                                <string>@extend(receive_icmf_message) function receive_change_dapp_providers_state(sender: byte_array, topic: text, body: gtv) {
    if (topic != change_dapp_providers_state_topic) return;

    if (sender != economy_chain.rid) {
        log("Sender %s is not the economy chain, ignoring".format(sender));
        return;
    }

    receive_change_dapp_providers_state_impl(body);
}

function receive_change_dapp_providers_state_impl(body: gtv) {
    val change_dapp_providers_state_message = change_dapp_providers_state_message.from_gtv(body);

    for (provider_key in change_dapp_providers_state_message.providers) {
        try_call(change_dapp_provider_state(
            provider_key,
            change_dapp_providers_state_message.enable,
            *
        ));
    }
}

function change_dapp_provider_state(provider_key: pubkey, enable: boolean) {
    val provider = require_provider(provider_key);
    require(provider.tier == provider_tier.DAPP_PROVIDER, "Provider %s is not a dapp provider".format(provider));

    if (enable) {
        enable_provider(provider);
    } else {
        disable_provider(provider);
    }
}
</string>
                            </entry>
                            <entry key="economy_chain_in_directory_chain/create_cluster.rell">
                                <string>@extend(receive_icmf_message) function receive_create_cluster(sender: byte_array, topic: text, body: gtv) {
    if (topic != create_cluster_topic) return;

    if (sender != economy_chain.rid) {
        log("Sender %s is not the economy chain, ignoring".format(sender));
        return;
    }

    receive_create_cluster_impl(body);
}

function receive_create_cluster_impl(body: gtv) {
    val message = create_cluster_message.from_gtv(body);

    val cluster_name = try_call(create_cluster_from_message(message, *));

   if (cluster_name == null)
        send_message(create_cluster_error_topic, create_cluster_error_message (
            name = message.name,
            error_message = "Unexpected error"
        ).to_gtv());
}

function create_cluster_from_message(
    message: create_cluster_message
): text {
    val proposer = require_provider(message.proposer_pubkey);
    val governor = require_voter_set(message.governor_voter_set_name);
    val node_provider_set = require_voter_set(message.voter_set_name);

    create_cluster_impl(proposer, message.name, governor, voter_set_member @* { node_provider_set }.provider.pubkey, cluster_creation_data(cluster_units = message.cluster_units, extra_storage = message.extra_storage));

    return message.name;
}
</string>
                            </entry>
                            <entry key="economy_chain_in_directory_chain/create_container.rell">
                                <string>@extend(receive_icmf_message) function receive_create_container(sender: byte_array, topic: text, body: gtv) {
    if (topic != create_container_topic) return;

    if (sender != economy_chain.rid) {
        log("Sender %s is not the economy chain, ignoring".format(sender));
        return;
    }

    receive_create_container_impl(body);
}

function receive_create_container_impl(body: gtv) {
    val message = create_container_message.from_gtv(body);

    val container_cluster_names = try_call(create_container_from_ticket(
        provider_pubkey = message.provider_pubkey,
        container_units = message.container_units,
        extra_storage = message.extra_storage,
        cluster_name = message.cluster_name,
        *));

    val error_message = if (container_cluster_names == null)
        "Unexpected error"
    else if (container_cluster_names.container_name.empty())
        "Unable to find suitable cluster"
    else null;
    send_message(ticket_container_result_topic, ticket_container_result_message(
        ticket_id = message.ticket_id,
        error_message = error_message,
        container_name = container_cluster_names?.container_name,
        cluster_name = container_cluster_names?.cluster_name
    ).to_gtv());
}

function create_container_from_ticket(
    provider_pubkey: pubkey,
    container_units: integer,
    extra_storage: integer,
    cluster_name: text
): (container_name: text, cluster_name: text) {

    require_pubkey(provider_pubkey);
    require(not cluster_name.empty(), "Cluster name must be specified");

    val provider = create_provider_if_not_exist(provider_pubkey);
    val cluster = cluster @? { .name == cluster_name, .name != clusters.system, .operational };

    if (
        cluster != null and
        get_available_container_units(cluster) &gt;= container_units and
        get_available_extra_storage(cluster) &gt;= extra_storage
    ) {
        val container = create_container_in_cluster(provider, cluster, container_units, extra_storage);
        return (container_name = container.name, cluster_name = cluster.name);
    }

    return (container_name = "", cluster_name = "");
}

function create_container_in_cluster(provider, cluster, container_units: integer, extra_storage: integer): container {
    val name = [cluster.to_gtv(), provider.pubkey.to_gtv(), op_context.transaction.tx_rid.to_gtv()].hash().to_hex();
    return create_container_impl(
        provider, name, cluster, 1, [provider.pubkey],
        container_units,
        standard_container_defaults.max_blockchains,
        extra_storage
    );
}</string>
                            </entry>
                            <entry key="economy_chain_in_directory_chain/directory_chain_sync.rell">
                                <string>@extend(after_provider_registered) function(provider, cluster: cluster?) {
    send_message(provider_update_topic, provider_update_message(
        pubkey = provider.pubkey,
        system = provider.system,
        tier = provider.tier.name,
        active = provider.active
    ).to_gtv());
}

@extend(after_provider_updated) function(provider) {
    send_message(provider_update_topic, provider_update_message(
        pubkey = provider.pubkey,
        system = provider.system,
        tier = provider.tier.name,
        active = provider.active
    ).to_gtv());
}

@extend(after_cluster_creation) function (provider, cluster) {
    update_cluster(cluster);
}

@extend(after_cluster_updated) function(cluster) {
    update_cluster(cluster);
}

function update_cluster(cluster) {
    send_message(cluster_update_topic, cluster_update_message(
        name = cluster.name,
        deleted = false,
        operational = cluster.operational,
        cluster_units = cluster.cluster_units,
        extra_storage = cluster.extra_storage,
        anchoring_chain = cluster_anchoring_chain @? { cluster }.blockchain.rid
    ).to_gtv());
}

@extend(before_cluster_removal) function(cluster) {
    send_message(cluster_update_topic, cluster_update_message(
        name = cluster.name,
        deleted = true,
        operational = cluster.operational,
        cluster_units = cluster.cluster_units,
        extra_storage = cluster.extra_storage,
        anchoring_chain = cluster_anchoring_chain @? { cluster }.blockchain.rid
    ).to_gtv());
}

@extend(after_node_added) function(node) {
    send_message(node_update_topic, node_update_message(
        provider_pubkey = node.provider.pubkey,
        pubkey = node.pubkey,
        active = node.active,
        territory = node.territory,
        cluster_units = node.cluster_units,
        extra_storage = node.extra_storage,
        deleted = false
    ).to_gtv());
}

@extend(after_node_updated) function(node) {
    send_message(node_update_topic, node_update_message(
        provider_pubkey = node.provider.pubkey,
        pubkey = node.pubkey,
        active = node.active,
        territory = node.territory,
        cluster_units = node.cluster_units,
        extra_storage = node.extra_storage,
        deleted = false
    ).to_gtv());
}

@extend(before_node_removal) function(node) {
    send_message(node_update_topic, node_update_message(
        provider_pubkey = node.provider.pubkey,
        pubkey = node.pubkey,
        active = node.active,
        territory = node.territory,
        cluster_units = node.cluster_units,
        extra_storage = node.extra_storage,
        deleted = true
    ).to_gtv());
}

@extend(after_node_added_to_cluster) function(node, cluster) {
    after_node_added_to_cluster_impl(node, cluster, false);
}

@extend(after_replica_node_added_to_cluster) function(node, cluster) {
    after_node_added_to_cluster_impl(node, cluster, true);
}

function after_node_added_to_cluster_impl(node, cluster, replica: boolean) {
    send_message(cluster_node_update_topic, cluster_node_update_message(
        name = cluster.name,
        pubkey = node.pubkey,
        replica_node = replica,
        deleted = false
    ).to_gtv());
}

@extend(after_node_removed_from_cluster) function(node, cluster?) {
    after_node_removed_from_cluster_impl(node, cluster, false);
}

@extend(after_replica_node_removed_from_cluster) function(node, cluster?) {
    after_node_removed_from_cluster_impl(node, cluster, true);
}

function after_node_removed_from_cluster_impl(node, cluster?, replica: boolean) {
    val name = if (exists(cluster)) cluster.name else null;
    send_message(cluster_node_update_topic, cluster_node_update_message(
        name = name,
        pubkey = node.pubkey,
        replica_node = replica,
        deleted = true
    ).to_gtv());
}

</string>
                            </entry>
                            <entry key="economy_chain_in_directory_chain/module.rell">
                                <string>module;

import common.*;
import common.init.*;
import model.*;
import ^.messaging.anchoring_api.*;
import ^.proposal_blockchain_move.*;
import ^.lib.icmf.*;
import ^.lib.icmf.receiver.*;
import ^.messaging.economy_chain.*;

val economy_chain_name = "economy_chain";

object economy_chain {
    mutable rid: byte_array = x"";
}

operation init_economy_chain(my_pubkey: pubkey, economy_chain_config: byte_array) {
    require_is_provider_with_rate_limit(my_pubkey);
    require_is_system_provider(my_pubkey);

    require(economy_chain.rid == x"", "Economy chain is already started");

    val nodes = cluster_node @* { system_cluster() } (@sort .node.pubkey);
    // do not write new configuration when size is 0 since it's impossible to recover from that
    require(nodes.size() &gt; 0, "System cluster must have at least one node");

    log("Adding economy chain to system container");
    val economy_blockchain = add_blockchain(economy_chain_config, nodes, economy_chain_name, system_container(), true);
    update economy_chain(rid = economy_blockchain.rid);
}

query get_economy_chain_rid(): byte_array? = if (economy_chain.rid == x"") null else economy_chain.rid;

function create_provider_if_not_exist(pubkey): provider {
    var provider = provider @? { pubkey };
    if (provider == null) {
        register_and_enable_provider(
            provider_info(pubkey),
            provider_tier.DAPP_PROVIDER,
            null,
            null,
            enabled_by_default = true
        );
        provider = provider @ { pubkey };
    }
    return provider;
}</string>
                            </entry>
                            <entry key="economy_chain_in_directory_chain/register_dapp_provider.rell">
                                <string>@extend(receive_icmf_message) function receive_register_dapp_provider(sender: byte_array, topic: text, body: gtv) {
    if (topic != register_dapp_provider_topic) return;

    if (sender != economy_chain.rid) {
        log("Sender %s is not the economy chain, ignoring".format(sender));
        return;
    }

    receive_register_dapp_provider_impl(body);
}

function receive_register_dapp_provider_impl(body: gtv) {
    val register_dapp_provider_message = register_dapp_provider_message.from_gtv(body);
    val container_voter_set = container @? { .name == register_dapp_provider_message.container_name }.deployer;

    try_call(register_and_enable_provider(
        provider_info(register_dapp_provider_message.new_provider),
        provider_tier.DAPP_PROVIDER,
        null,
        container_voter_set,
        true,
        *
    ));
}
</string>
                            </entry>
                            <entry key="economy_chain_in_directory_chain/restart_container.rell">
                                <string>@extend(receive_icmf_message) function receive_restart_container(sender: byte_array, topic: text, body: gtv) {
    if (topic != restart_container_topic) return;

    if (sender != economy_chain.rid) {
        log("Sender %s is not the economy chain, ignoring".format(sender));
        return;
    }

    receive_restart_container_impl(body);
}

function receive_restart_container_impl(body: gtv) {
    val message = restart_container_message.from_gtv(body);

    if (not try_call(restart_container(message.container_name, *))) {
        log("Unable to restart container %s".format(message.container_name));
    }
}

function restart_container(container_name: text) {
    val container = container @ { .name == container_name };
    container.state = container_state.RUNNING;
}</string>
                            </entry>
                            <entry key="economy_chain_in_directory_chain/stop_container.rell">
                                <string>@extend(receive_icmf_message) function receive_stop_container(sender: byte_array, topic: text, body: gtv) {
    if (topic != stop_container_topic) return;

    if (sender != economy_chain.rid) {
        log("Sender %s is not the economy chain, ignoring".format(sender));
        return;
    }

    receive_stop_container_impl(body);
}

function receive_stop_container_impl(body: gtv) {
    val message = stop_container_message.from_gtv(body);

    if (not try_call(stop_container(message.container_name, *))) {
        log("Unable to stop container %s".format(message.container_name));
    }
}

function stop_container(container_name: text) {
    val container = container @ { .name == container_name };
    container.state = container_state.STOPPED;
}</string>
                            </entry>
                            <entry key="economy_chain_in_directory_chain/upgrade_container.rell">
                                <string>@extend(receive_icmf_message) function receive_upgrade_container(sender: byte_array, topic: text, body: gtv) {
    if (topic != upgrade_container_topic) return;

    if (sender != economy_chain.rid) {
        log("Sender %s is not the economy chain, ignoring".format(sender));
        return;
    }

    receive_upgrade_container_impl(body);
}

entity ec_pending_last_anchored_heights_request {
    request_id: integer;
    upgrade_container_message: byte_array;
}

function receive_upgrade_container_impl(body: gtv) {
    val message = upgrade_container_message.from_gtv(body);

    val container = container @ { message.container_name };
    if (container.cluster.name == message.cluster_name) {
        val upgrade_result = try_call(check_and_upgrade_container(container, message, *));
        send_upgrade_container_result(message, if (upgrade_result) null else "Unable to upgrade container %s".format(message.container_name));
    } else {
        val error_message = start_creating_container_and_moving_blockchains_async(message);
        if (error_message??) {
            send_upgrade_container_result(message, "Unable to upgrade container due to: " + error_message);
        }
    }
}

function send_upgrade_container_result(
    message: upgrade_container_message,
    error_message: text?,
    dst_container_name: text? = null,
    dst_cluster_name: text? = null
) {
    send_message(
        ticket_container_result_topic,
        ticket_container_result_message(
            ticket_id = message.ticket_id,
            error_message = error_message,
            container_name = dst_container_name ?: message.container_name,
            cluster_name = dst_cluster_name ?: message.cluster_name
        ).to_gtv()
    );
}

function check_and_upgrade_container(container, message: upgrade_container_message) {
    upgrade_container(container, message.container_units, message.extra_storage, standard_container_defaults.max_blockchains);
    // In case container was stopped
    restart_container(message.container_name);
}

function start_creating_container_and_moving_blockchains_async(message: upgrade_container_message): text? {
    // validate dst cluster
    val dst_cluster = require_cluster(message.cluster_name);
    if (empty(cluster_node @* { dst_cluster })) {
        return "Cluster %s must have at least one node".format(message.cluster_name);
    }
    val src_container = container @? { message.container_name };
    if (empty(src_container)) {
        return "Container %s not found".format(message.container_name);
    }

    // fetching last_anchored_heights
    val cac = cluster_anchoring_chain @ { src_container.cluster } ( .blockchain.rid );
    val brids = container_blockchain @* { src_container, not .blockchain.system } ( .blockchain.rid );
    val request_id = request_last_anchored_heights(cac, brids);
    create ec_pending_last_anchored_heights_request(
        request_id, message.to_bytes()
    );

    return null;
}

@extend(on_last_anchored_heights_received) function last_anchored_heights_received(request_id: integer, anchored_blockchains: map&lt;byte_array, integer&gt;) {
    val pending_request = ec_pending_last_anchored_heights_request @? { request_id };
    if (empty(pending_request)) {
        log("Pending get_last_anchored_heights request not found: " + request_id);
        return;
    }

    val message = upgrade_container_message.from_bytes(pending_request.upgrade_container_message);

    // TODO: Retrieve error messages from `require()`:s
    val dst_container_cluster_names = try_call(
        move_container_blockchains_impl(
            message.container_name,
            message.cluster_name,
            anchored_blockchains,
            message.container_units,
            message.extra_storage,
            *
    ));

    val error_message = if (dst_container_cluster_names??) null else "Unable to upgrade container %s, check logs".format(message.container_name);
    send_upgrade_container_result(message, error_message, dst_container_cluster_names?.container_name, dst_container_cluster_names?.cluster_name);
}
</string>
                            </entry>
                            <entry key="evm_event_receiver/module.rell">
                                <string>module;

import common.*;
import model.*;

import messaging.blockchain_rid.*;
import lib.icmf.*;

import constants.*;

object evm_event_receiver_chain {
    mutable rid: byte_array = x"";
}

/**
 * Create and initialize the EVM event receiver system chain.
 * @param my_pubkey The pubkey of initializer, must be signing transaction and a system provider.
 * @param event_receiver_chain_config Event receiver chain configuration, see `postchain-eif` for more information.
 */
operation init_evm_event_receiver_chain(my_pubkey: pubkey, event_receiver_chain_config: byte_array) {
    require_is_provider_with_rate_limit(my_pubkey);
    require_is_system_provider(my_pubkey);

    require(evm_event_receiver_chain.rid == x"", "EVM event receiver chain is already started");

    val nodes = cluster_node @* { system_cluster() } (@sort .node.pubkey);
    // do not write new configuration when size is 0 since it's impossible to recover from that
    require(nodes.size() &gt; 0, "System cluster must have at least one node");

    log("Adding EVM event receiver chain to system container");
    val evm_event_receiver_blockchain = add_blockchain(
        event_receiver_chain_config, nodes, blockchains.evm_event_receiver_chain, system_container(), true);
    update evm_event_receiver_chain(rid = evm_event_receiver_blockchain.rid);
    send_message(blockchain_rid_topic, blockchain_rid(rid = evm_event_receiver_blockchain.rid, name = blockchains.evm_event_receiver_chain).to_gtv());
}

/**
* Get the EVM event receiver blockchain RID.
*
* @returns A bytearray of the blockchain RID.
*/
query get_evm_event_receiver_chain_rid(): byte_array? = if (evm_event_receiver_chain.rid == x"") null else evm_event_receiver_chain.rid;
</string>
                            </entry>
                            <entry key="evm_event_receiver_price_oracle/module.rell">
                                <string>module;

import common.*;
import model.*;

import messaging.blockchain_rid.*;
import lib.icmf.*;

import constants.*;

object evm_event_receiver_price_oracle_chain {
    mutable rid: byte_array = x"";
}

operation init_evm_event_receiver_price_oracle_chain(my_pubkey: pubkey, event_receiver_chain_config: byte_array) {
    require_is_provider_with_rate_limit(my_pubkey);
    require_is_system_provider(my_pubkey);

    require(evm_event_receiver_price_oracle_chain.rid == x"", "EVM event receiver price oracle chain is already started");

    val nodes = cluster_node @* { system_cluster() } ( @sort .node.pubkey );
    // do not write new configuration when size is 0 since it's impossible to recover from that
    require(nodes.size() &gt; 0, "System cluster must have at least one node");

    log("Adding EVM event receiver price oracle chain to system container");
    val blockchain = add_blockchain(
        event_receiver_chain_config, nodes, blockchains.evm_event_receiver_price_oracle_chain, system_container(), true);
    update evm_event_receiver_price_oracle_chain(rid = blockchain.rid);
    send_message(blockchain_rid_topic, blockchain_rid(rid = blockchain.rid, name = blockchains.evm_event_receiver_price_oracle_chain).to_gtv());
}

query get_evm_event_receiver_price_oracle_chain_rid(): byte_array? = if (evm_event_receiver_price_oracle_chain.rid == x"") null else evm_event_receiver_price_oracle_chain.rid;
</string>
                            </entry>
                            <entry key="evm_transaction_submitter/module.rell">
                                <string>module;

import common.*;
import model.*;

import messaging.blockchain_rid.*;
import lib.icmf.*;

import constants.*;

object evm_transaction_submitter_chain {
    mutable rid: byte_array = x"";
}

operation init_evm_transaction_submitter_chain(my_pubkey: pubkey, transaction_submitter_chain_config: byte_array) {
    require_is_provider_with_rate_limit(my_pubkey);
    require_is_system_provider(my_pubkey);

    require(evm_transaction_submitter_chain.rid == x"", "EVM transaction submitter chain is already started");

    val nodes = cluster_node @* { system_cluster() } (@sort .node.pubkey);
    // do not write new configuration when size is 0 since it's impossible to recover from that
    require(nodes.size() &gt; 0, "System cluster must have at least one node");

    log("Adding EVM transaction submitter chain to system container");
    val evm_transaction_submitter_blockchain = add_blockchain(transaction_submitter_chain_config, nodes, blockchains.evm_transaction_submitter_chain, system_container(), true);
    update evm_transaction_submitter_chain(rid = evm_transaction_submitter_blockchain.rid);
    send_message(blockchain_rid_topic, blockchain_rid(rid = evm_transaction_submitter_blockchain.rid, name = blockchains.evm_transaction_submitter_chain).to_gtv());
}

query get_evm_transaction_submitter_chain_rid(): byte_array? = if (evm_transaction_submitter_chain.rid == x"") null else evm_transaction_submitter_chain.rid;
</string>
                            </entry>
                            <entry key="features.rell">
                                <string>module;

@extendable function direct_cluster(): boolean = false;
@extendable function direct_container(): boolean = false;

query has_direct_cluster(): boolean = direct_cluster();

query has_direct_container(): boolean = direct_container();</string>
                            </entry>
                            <entry key="lib/icmf/constants.rell">
                                <string>module;

val ICMF_MESSAGE_MAX_SIZE = 16 * 1024 * 1024; // 16 MiB
val ICMF_TOPIC_GLOBAL_PREFIX = "G_";
val ICMF_TOPIC_LOCAL_PREFIX = "L_";</string>
                            </entry>
                            <entry key="lib/icmf/module.rell">
                                <string>module;

import .constants.*;

function send_message(topic: text, body: gtv) {
    val encoded_body = body.to_bytes();
    require(topic.starts_with(ICMF_TOPIC_GLOBAL_PREFIX) or topic.starts_with(ICMF_TOPIC_LOCAL_PREFIX),
        "Topic must start with " + ICMF_TOPIC_GLOBAL_PREFIX + " or " + ICMF_TOPIC_LOCAL_PREFIX);
    require(encoded_body.size() &lt; ICMF_MESSAGE_MAX_SIZE, "Message body too big, max size is %d bytes".format(ICMF_MESSAGE_MAX_SIZE));

    op_context.emit_event("icmf_message", (topic = topic, body = body).to_gtv_pretty());
}
</string>
                            </entry>
                            <entry key="lib/icmf/receiver.rell">
                                <string>module;

operation __icmf_message(sender: byte_array, topic: text, body: gtv) {
    receive_icmf_message(sender, topic, body);
}

@extendable function receive_icmf_message(sender: byte_array, topic: text, body: gtv) {}
</string>
                            </entry>
                            <entry key="management_chain_common/begin_block_handler.rell">
                                <string>import common.*;

operation __begin_block(height: integer) {
    before_begin_block(height);

    // bc == null until chain0 is initialized
    // if bc != null, and since it is __begin_block operation, then height &gt; 0
    val bc = blockchain @? { chain_context.blockchain_rid };
    if (empty(bc)) return;
    val expected_config = get_blockchain_configuration(chain_context.blockchain_rid, height);
    if (expected_config == null or expected_config.config_hash == chain_context.raw_config.hash()) return;

    log("Chain0 config for height %d changed from %s to %s and will be stored".format(
        height, expected_config.config_hash, chain_context.raw_config.hash()));
    val config_map = map&lt;text, gtv&gt;.from_gtv(chain_context.raw_config);
    config_map.remove_or_null("signers");
    val base_config = config_map.to_gtv().to_bytes();
    compress_and_store_configuration(bc, height, base_config, true);
    add_dependencies(base_config, chain_context.blockchain_rid, height);
}

@extendable function before_begin_block(height: integer) {}
</string>
                            </entry>
                            <entry key="management_chain_common/module.rell">
                                <string>module;
</string>
                            </entry>
                            <entry key="management_chain_mainnet/module.rell">
                                <string>module;

// All management_chain_directory1 imports except direct_cluster and direct_container (should not be part of mainnet)
import common.*;
import common.init.*;
import common.operations.*;
import common.queries.*;
import management_chain_common.*;
import nm_api.*;
import proposal.*;
import proposal_blockchain.*;
import proposal_blockchain_import.*;
import proposal_blockchain_move.*;
import proposal_cluster.*;
import proposal_cluster_anchoring.*;
import proposal_container.proposal_container_limits.*;
import proposal_provider.*;
import proposal_voter_set.*;
import version;
import features;

// Additional modules for mainnet
import economy_chain_in_directory_chain.*;
import evm_event_receiver.*;
import evm_event_receiver_price_oracle.*;
import evm_transaction_submitter.*;
import price_oracle.*;
</string>
                            </entry>
                            <entry key="messaging/anchoring_api.rell">
                                <string>module;

import lib.icmf.*;
import lib.icmf.receiver.*;

val get_last_anchored_heights_topic = ICMF_TOPIC_GLOBAL_PREFIX + "get_last_anchored_heights";
val last_anchored_heights_topic = ICMF_TOPIC_GLOBAL_PREFIX + "last_anchored_heights";

entity pending_request {
    key request_id: integer;
}

object request_counter {
    mutable counter: integer = 0;
}

function create_request(): pending_request {
    val request = create pending_request(request_counter.counter);
    request_counter.counter = request_counter.counter + 1;
    return request;
}

struct get_last_anchored_heights_message {
    request_id: integer;
    target_anchoring_chain: byte_array;
    blockchains: list&lt;byte_array&gt;;
}

struct last_anchored_heights_message {
    request_id: integer;
    anchored_blockchains: map&lt;byte_array, integer&gt;;
}

function request_last_anchored_heights(target_anchoring_chain: byte_array, blockchain_rids: list&lt;byte_array&gt;): integer {
    val request = create_request();

    send_message(
        get_last_anchored_heights_topic,
        get_last_anchored_heights_message(
            request_id = request.request_id,
            target_anchoring_chain = target_anchoring_chain,
            blockchains = blockchain_rids
        ).to_gtv()
    );

    return request.request_id;
}

@extend(receive_icmf_message) function receive_last_anchored_heights(sender: byte_array, topic: text, body: gtv) {
    if (topic != last_anchored_heights_topic) return;

    val message = last_anchored_heights_message.from_gtv(body);
    val pending_req = pending_request @? { message.request_id };
    if (empty(pending_req)) {
        log("get_last_anchored_heights request not found: " + message.request_id);
        return;
    }

    on_last_anchored_heights_received(message.request_id, message.anchored_blockchains);

    delete pending_req;
}

@extendable function on_last_anchored_heights_received(request_id: integer, anchored_blockchains: map&lt;byte_array, integer&gt;);
</string>
                            </entry>
                            <entry key="messaging/blockchain_rid.rell">
                                <string>module;

import lib.icmf.constants.*;

struct blockchain_rid {
    rid: byte_array;
    name: text;
}

val blockchain_rid_topic = ICMF_TOPIC_LOCAL_PREFIX + "blockchain_rid_topic";
</string>
                            </entry>
                            <entry key="messaging/configuration_update_message.rell">
                                <string>module;

import lib.icmf.constants.*;

val configuration_updated_topic = ICMF_TOPIC_GLOBAL_PREFIX + "configuration_updated";
val configuration_failed_topic = ICMF_TOPIC_GLOBAL_PREFIX + "configuration_failed";

struct configuration_updated {
    blockchain_rid: byte_array;
    height: integer;
    config_hash: byte_array;
}

struct configuration_failed {
    blockchain_rid: byte_array;
    height: integer;
    config_hash: byte_array;
}
</string>
                            </entry>
                            <entry key="messaging/economy_chain.rell">
                                <string>module;

import lib.icmf.constants.*;

val create_cluster_topic = ICMF_TOPIC_GLOBAL_PREFIX + "create_cluster";
val create_cluster_error_topic = ICMF_TOPIC_LOCAL_PREFIX + "create_cluster_error";

val create_container_topic = ICMF_TOPIC_GLOBAL_PREFIX + "create_container";
val stop_container_topic = ICMF_TOPIC_GLOBAL_PREFIX + "stop_container";
val restart_container_topic = ICMF_TOPIC_GLOBAL_PREFIX + "restart_container";
val upgrade_container_topic = ICMF_TOPIC_GLOBAL_PREFIX + "upgrade_container";

val register_dapp_provider_topic = ICMF_TOPIC_GLOBAL_PREFIX + "register_dapp_provider";
val change_dapp_providers_state_topic = ICMF_TOPIC_GLOBAL_PREFIX + "change_dapp_providers_state";

val ticket_container_result_topic = ICMF_TOPIC_LOCAL_PREFIX + "ticket_container_result";
val cluster_update_topic = ICMF_TOPIC_LOCAL_PREFIX + "cluster_update";
val cluster_node_update_topic = ICMF_TOPIC_LOCAL_PREFIX + "cluster_node_update";
val provider_update_topic = ICMF_TOPIC_LOCAL_PREFIX + "provider_update";
val node_update_topic = ICMF_TOPIC_LOCAL_PREFIX + "node_update";

struct create_cluster_message {
    name: text;
    governor_voter_set_name: text;
    voter_set_name: text;
    cluster_units: integer;
    extra_storage: integer;
    proposer_pubkey: byte_array;
}

struct create_cluster_error_message {
    name: text;
    error_message: text;
}

struct create_container_message {
    ticket_id: integer;
    provider_pubkey: pubkey;
    container_units: integer;
    extra_storage: integer;
    cluster_name: text;
}

struct upgrade_container_message {
    ticket_id: integer;
    container_name: text;
    container_units: integer;
    extra_storage: integer;
    cluster_name: text;
}

struct stop_container_message {
    container_name: text;
}

struct restart_container_message {
    container_name: text;
}

struct ticket_container_result_message {
    ticket_id: integer;
    error_message: text?;
    container_name: text? = null;
    cluster_name: text? = null;
}

struct cluster_update_message {
    name;
    deleted: boolean;
    operational: boolean;
    cluster_units: integer;
    extra_storage: integer;
    anchoring_chain: byte_array?;
}

struct cluster_node_update_message {
    name?;
    pubkey;
    replica_node: boolean;
    deleted: boolean;
}

struct provider_update_message {
    pubkey;
    system: boolean;
    tier: text;
    active: boolean;
}

struct cluster_provider_update_message {
    pairs: list&lt;(text, pubkey)&gt;;
    deleted: boolean;
}

struct node_update_message {
    provider_pubkey: pubkey;
    pubkey;
    active: boolean;
    territory: text;
    cluster_units: integer;
    extra_storage: integer;
    deleted: boolean;
}

struct register_dapp_provider_message {
    container_name: text;
    new_provider: pubkey;
}

struct change_dapp_providers_state_message {
    providers: list&lt;pubkey&gt;;
    enable: boolean;
}
</string>
                            </entry>
                            <entry key="messaging/signer_list_update.rell">
                                <string>module;

import lib.icmf.constants.*;

val signer_list_update_topic = ICMF_TOPIC_LOCAL_PREFIX + "signer_list_update";

struct signer_list_update_message {
    serial: integer;
    blockchain_rid: byte_array;
    signers: byte_array;
    confirmed_in_directory_at_height: integer; // Will be used for constructing proofs
}

</string>
                            </entry>
                            <entry key="model/anchoring_model.rell">
                                <string>object system_anchoring_chain {
    mutable rid: byte_array = x"";
}

object cluster_anchoring_config {
    mutable raw_config: byte_array = map&lt;text, gtv&gt;().to_gtv().to_bytes();
}

entity cluster_anchoring_chain {
    key blockchain, cluster;
}
</string>
                            </entry>
                            <entry key="model/blockchain_model.rell">
                                <string>enum blockchain_state {
    // active states: RUNNING, PAUSED, IMPORTING, UNARCHIVING
    // inactive states: REMOVED, ARCHIVED
    RUNNING, PAUSED, REMOVED, IMPORTING, ARCHIVED, UNARCHIVING
}

entity blockchain {
    key rid: byte_array; 
    mutable name: text;
    system: boolean = false;
    mutable state: blockchain_state = blockchain_state.RUNNING;
}

entity blockchain_configuration {
    key blockchain, height: integer;
    mutable data: byte_array; // base_config, without signers
}

entity compressed_blockchain_configuration_part {
    key hash: byte_array;
    data: byte_array;
}

entity pending_blockchain_configuration {
    key blockchain, minimum_height: integer;
    key blockchain, config_hash: byte_array;
    base_config: byte_array;
    signers: byte_array;
    signers_update: boolean = false;
}

entity signer_excluded_from_pending_configuration {
    key blockchain, config_hash: byte_array;
    pubkey;
}

entity faulty_blockchain_configuration {
    key blockchain, reported_at_height: integer;
    config_hash: byte_array;
}

entity blockchain_configuration_signers {
    key blockchain, height: integer;
    mutable signers: byte_array;
}

entity blockchain_replica_node { 
    key blockchain, node; 
}

/**
 * For blockchain dependency tracking. Dependency is not static.
 * Blockchain (me) might be dependent on different blockchains (dependent_on) at different heights.
 */
entity blockchain_dependency { 
    key me: blockchain, height: integer, dependent_on: blockchain; 
}

entity blockchain_configuration_options {
    key blockchain, height: integer;
    suppress_special_transaction_validation: boolean;
}

entity importing_foreign_blockchain {
    key blockchain_rid: byte_array;
    pubkey;
    host: text;
    port: integer;
    api_url: text;
    chain0_rid: byte_array;
    mutable final_height: integer = -1;
}

/**
 * @deprecated use inactive_blockchain instead
 */
entity removed_blockchain {
    key blockchain;
    height: integer;
}

entity inactive_blockchain {
    key blockchain;
    height: integer;
}

entity moving_blockchain {
    key blockchain;
    transaction = op_context.transaction;
    source: container;
    destination: container;
    mutable final_height: integer = -1;
    mutable remove_on_nodes: byte_array = x""; // gtv(list&lt;pubkey&gt;)
}

entity unarchiving_blockchain {
    key blockchain;
    transaction = op_context.transaction;
    source: container;
    destination: container;
    final_height: integer;
}
</string>
                            </entry>
                            <entry key="model/cluster_model.rell">
                                <string>/**
 * Governance semantics:
 *
 * Anybody can create a cluster, but it consumes an action point.
 *
 * A cluster defines a list of providers involved in it. Each provider is supposed to contribute one node.
 * Cluster is considered operational only once every provider specified in a cluster have provided a node.
 * Cluster composition (providers) can change, operational status does not change if new provider is added
 * or a provider disables its node (if it is not the last node of the cluster).
 *
 * Cluster composition can be changed by its 'governance' voter set, they can vote to add or remove providers
 *
 * a provider can add at most one node to a cluster he is a member of. he can also replace his node.
 *
 * Cluster's deployer can add containers, specifying their resource limits. He can change limits for existing containers.
 *
 * Container deployer can deploy, update and stop blockchains in the container.
 *
 *
 * Node can belong to multiple clusters. Although it is not recommended, but permitted.
 *
 * blockchain_signer_node records are not needed for blockchains which run on a cluster, as list
 * of signer nodes can be deduced from list of nodes of a cluster.
 *
 */

// node cluster
entity cluster {
    key name;
    governance: voter_set; // who controls the cluster
    mutable operational: boolean = false; // cluster goes operational when all providers provide a node
    mutable cluster_units: integer = 1; // how many container_units does the cluster support
    mutable extra_storage: integer = 0; // how much extra storage is allocated to the cluster
}

entity cluster_provider { 
    key cluster, provider; 
}

entity cluster_node { 
    key cluster, node; 
}

entity cluster_replica_node {
    key cluster, node;
}

object standard_cluster_unit {
    mutable container_units: integer = 16;
}

object standard_cluster_defaults {
    mutable cluster_units: integer = 1;
    mutable extra_storage: integer = 0;
}

struct cluster_creation_data {
    cluster_units: integer = standard_cluster_defaults.cluster_units;
    extra_storage: integer = standard_cluster_defaults.extra_storage;
}</string>
                            </entry>
                            <entry key="model/constants.rell">
                                <string>namespace containers {
    val system = "system";
    val system_suffix = "_system";
}

namespace clusters {
    val system = "system";
}

namespace voter_sets {
    val system = "SYSTEM";
    val system_p = "SYSTEM_P";
}

function system_container_name(cluster_name: text) =
    if (cluster_name == clusters.system)
        containers.system
    else
        cluster_name + containers.system_suffix;

function system_container() = container @ { system_container_name(clusters.system) };
function system_cluster() = cluster @ { .name == clusters.system };
function system_voter_set() = voter_set @ { voter_sets.system };

function system_p_voter_set() = voter_set @ { voter_sets.system_p };
</string>
                            </entry>
                            <entry key="model/container_model.rell">
                                <string>enum container_resource_limit_type {
    container_units,
    max_blockchains,
    extra_storage
}

enum container_state {
    RUNNING, STOPPED, MIGRATING
}

entity container {
    key name;
    index cluster;
    index deployer: voter_set; // who can deploy and update bcs in this container
    proposed_by: provider;
    system: boolean = false;
    mutable latest_bc_removal: timestamp = if (op_context.exists) op_context.last_block_time else -1;
    mutable state: container_state = container_state.RUNNING;
}

entity container_resource_limit {
    key container, container_resource_limit_type;
    mutable value: integer;
}

//Defines which container a blockchain belongs to
entity container_blockchain { 
    key container, blockchain; 
}

object standard_container_unit {
    mutable cpu: integer = 50; // Percent of cpus, i.e. 50 = 0.5 vCPU
    mutable ram: integer = 2048; // MiB
    mutable io_read: integer = 25; // MiB/s
    mutable io_write: integer = 20; // MiB/s
    mutable storage: integer = 16384; // MiB
}

object standard_container_defaults {
    mutable container_units: integer = 1;
    mutable max_blockchains: integer = 10;
    mutable extra_storage: integer = 0;
}

object system_container_defaults {
    mutable container_units: integer = 4;
    mutable max_blockchains: integer = -1;
    mutable extra_storage: integer = 0;
}

function empty_container_resource_limits() = map&lt;container_resource_limit_type, integer&gt;();
</string>
                            </entry>
                            <entry key="model/module.rell">
                                <string>module;

import constants.*;</string>
                            </entry>
                            <entry key="model/node_model.rell">
                                <string>entity node {
    index provider;
    key pubkey;
    mutable active: boolean = true;
    mutable host: text;
    mutable port: integer;
    mutable api_url: text;
    mutable last_updated: timestamp;
    mutable cluster_units: integer = 1;
    mutable territory: text = ""; // ISO 3166-1 alpha-2 code or empty string for undefined
    mutable extra_storage: integer = 0;
}

struct node_info {
    pubkey;
    host: text;
    port: integer;
    api_url: text;
    territory: text;
}

struct register_node_data {
    pubkey;
    host: text;
    port: integer;
    api_url: text;
    clusters: list&lt;text&gt; = list&lt;text&gt;();
    cluster_units: integer = 1;
    territory: text = "";
    extra_storage: integer = 0;
}

struct replace_node_data {
    old_node_key: pubkey;
    new_node_key: pubkey;
    new_host: text? = null;
    new_port: integer? = null;
    new_api_url: text? = null;
    new_cluster_units: integer?  = null;
    new_territory: text? = null;
    new_extra_storage: integer? = null;
}

struct update_node_data {
    pubkey;
    host: text? = null;
    port: integer? = null;
    api_url: text? = null;
    cluster_units: integer? = null;
    territory: text? = null;
    extra_storage: integer? = null;
}
</string>
                            </entry>
                            <entry key="model/provider_model.rell">
                                <string>enum provider_tier {
    /* Can add replica nodes to any cluster */
    DAPP_PROVIDER,
    /* Can add signer nodes to any non-system cluster */
    NODE_PROVIDER
}
/**
 * A provider is a participant of the mainnet eco system
 * - Add a replica node
 * - Add a provider (Without privileges)
 * - Update their own information
 * - Form a new voter set
 * - Create a cluster
 * - Vote on proposals concerning voter sets you belong to
 */
entity provider {
    key pubkey;
    mutable name: text = "";
    mutable url: text = "";
    mutable active: boolean = false;
    mutable tier: provider_tier = provider_tier.DAPP_PROVIDER;
    /**
     * System provider can
     * - Add and remove signer nodes to clusters
     * - update the system cluster/voter set
     * - Add provider with any (non-system) privileges
     * - Propose removal of providers
     * - Update cluster configuration
     */
    mutable system: boolean = false;
}

// Type of quota for a provider
enum provider_quota_type {
    max_actions_per_day,
    max_nodes,
    max_containers
}

namespace provider_quota_defaults {
    val MAX_NODES = 1;
    val MAX_CONTAINERS = 2;
}

entity provider_quota {
    index tier: provider_tier, provider_quota_type;
    mutable value: integer;
}

// Limit how much stuff provider can do to prevent abuse,
// such as starting billion nodes, etc.
entity provider_rl_state {
    key provider;
    mutable points: integer;
    mutable last_update: integer;
}

struct provider_info {
	pubkey;
	name = "";
	url: text = "";
}</string>
                            </entry>
                            <entry key="model/region_model.rell">
                                <string>entity region {
    key name;
}

entity region_territory {
    key region, territory: text;
}
</string>
                            </entry>
                            <entry key="model/voter_set_model.rell">
                                <string>/**
 * A voter set is a set of providers who are part of a governance process.
 * The threshold determines how consensus is achieved.
 */
entity voter_set {
    key name;
    // special values for threshold:
    // 0: super-majority of voters, specifically  `n - (n - 1) / 3` (which is usually around 67%)
    // -1: simple majority
    // positive number: that many voters
    mutable threshold: integer = 0;
}

/**
 * Voter set governance determines who is allowed to change an voter set.
 */
entity voter_set_governance {
    key voter_set;
    mutable governor: voter_set;
}

entity voter_set_member {
    key voter_set, provider;
}</string>
                            </entry>
                            <entry key="nm_api/module.rell">
                                <string>module;

import common.*;
import model.*;
import roles.*;

/*
    NM API

    Version 1: Initial Managed Mode support
    Version 2: MustSyncUntil feature
    Version 3: Containers and Clusters
    Version 4: Blockchain.system property added (chain0, anchoring_chain are system chains)
    Version 5: Precise Configuration Update
    Version 6: Blockchain states
    Version 7: nm_get_management_chain() query added
    Version 8: Blockchain Configuration Options
    Version 9: IMPORTING blockchains added to the result set of nm_compute_blockchain_info_list() query and nm_get_blockchain_signers_in_latest_configuration() query added
    Version 10: nm_find_next_removed_blockchains() query added
    Version 11: nm_find_next_inactive_blockchains() query added
    Version 12: nm_get_container_for_blockchain_on_node() query added
    Version 13: nm_get_unarchiving_blockchain_node_info() query added
    Version 14: nm_get_blockchain_containers_for_node() query added
    Version 15: nm_get_migrating_blockchain_node_info() query added
    Version 16: migrating_blockchain_node_info.up_to_height renamed to .final_height
    Version 17: nm_find_next_removed_blockchains(), nm_get_unarchiving_blockchain_node_info(), nm_get_peer_list_version(), nm_compute_blockchain_list(), nm_get_blockchain_configuration_v5(), nm_get_container_for_blockchain_on_node() removed
    Version 18: nm_get_blockchain_configuration_v5() recovered, nm_get_blockchain_configuration_info() added
    Version 19: nm_compute_blockchain_list() recovered
*/

struct blockchain_info {
    rid: byte_array;
    system: boolean;
    state: blockchain_state;
}

struct inactive_blockchain_info {
    rid: byte_array;
    state: blockchain_state;
    height: integer;
}

struct migrating_blockchain_node_info {
    rid: byte_array;
    source_container: text;
    destination_container: text;
    is_source_node: boolean;
    is_destination_node: boolean;
    final_height: integer;
}

struct blockchain_configuration_info {
    base_config: byte_array;
    signers: list&lt;pubkey&gt;;
    config_hash: byte_array;
}
</string>
                            </entry>
                            <entry key="nm_api/nm_api.rell">
                                <string>/**
 * Returns NM API Version
 * NM API Version: 1
 */
query nm_api_version() = 19;

/**
 * Returns peer info list
 * NM API Version: 1
 * (API expects array, so we need suppress naming)
 */
query nm_get_peer_infos(): list&lt;(text, integer, pubkey, timestamp)&gt; {
    val peer_infos = list&lt;(text, integer, pubkey, timestamp)&gt;();
    peer_infos.add_all(node @* {} (_ = .host, _ = .port, _ = .pubkey, _ = .last_updated));
    peer_infos.add_all(importing_foreign_blockchain @* {} (_ = .host, _ = .port, _ = .pubkey, _ = 0));
    return peer_infos;
}

/**
 * Configuration updates are found in two tables. Both must be checked to get next configuration height.
 * NM API Version: 1
 */
query nm_find_next_configuration_height(blockchain_rid: byte_array, height: integer): integer? {
    val bc = blockchain @? { blockchain_rid };
    if (bc == null) return null;
    val conf_h = blockchain_configuration @? { bc, .height &gt; height } (@sort .height) limit 1;
    val sign_h = blockchain_configuration_signers @? { bc, .height &gt; height } (@sort .height) limit 1;
    if (conf_h == null) return sign_h;
    if (sign_h == null) return conf_h;
    return min(conf_h, sign_h);
}

/**
 * Merge content of blockchain_configuration and blockchain_configuration_signers
 * NM API Version: 1
 */
query nm_get_blockchain_configuration(blockchain_rid: byte_array, height: integer): byte_array? {
    val config = get_blockchain_configuration(blockchain_rid, height);
    if (config != null) {
        val full_config = map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(config.base_config));
        full_config["signers"] = config.signers.to_gtv();
        return full_config.to_gtv().to_bytes();
    } else {
        return null;
    }
}

/**
 * NM API Version: 5
 * @deprecated in NM API Version 18, use nm_get_blockchain_configuration_info() instead
 */
query nm_get_blockchain_configuration_v5(blockchain_rid: byte_array, height: integer):
    (base_config: byte_array, signers: list&lt;pubkey&gt;, config_hash: byte_array)?
        = get_blockchain_configuration(blockchain_rid, height);

/**
 * Returns blockchain configuration as a struct
 * NM API Version: 18
 */
query nm_get_blockchain_configuration_info(blockchain_rid: byte_array, height: integer): blockchain_configuration_info? {
    val bc_config = get_blockchain_configuration(blockchain_rid, height);
    if (bc_config == null) return null;

    return blockchain_configuration_info(
        base_config = bc_config.base_config,
        signers = bc_config.signers,
        config_hash = bc_config.config_hash
    );
}

/**
 * Returns list of blockchains to be launched
 * NM API Version: 1
 * @deprecated use nm_compute_blockchain_info_list
 */
query nm_compute_blockchain_list(node_id: pubkey): list&lt;byte_array&gt; {
    val res = list&lt;byte_array&gt;();
    for (blockchain in compute_blockchain_info_list(node_id)) {
        res.add(blockchain.rid);
    }
    return res;
}

/**
 * Returns a list of blockchains to be launched or to be initialized on a node (RUNNING, PAUSED, IMPORTING, UNARCHIVING)
 * NM API Version: 4
 * Compatibility:
 *  - NM API Version 4-8: Returns RUNNING or PAUSED blockchains
 *  - NM API Version 9-11: Returns RUNNING or PAUSED or IMPORTING blockchains
 */
query nm_compute_blockchain_info_list(node_id: pubkey): list&lt;blockchain_info&gt; {
    return compute_blockchain_info_list(node_id);
}

function compute_blockchain_info_list(node_id: pubkey): list&lt;blockchain_info&gt; {
    val node = node @? { node_id };
    if (exists(node)) {
        val res = set&lt;blockchain_info&gt;();
        res.add_all(get_mandatory_system_chains());
        res.add_all(get_cluster_node_blockchains(node));
        res.add_all(get_cluster_replica_node_blockchains(node));
        res.add_all(get_blockchains_replicated_by_node(node));
        return list(res);
    } else {
        return [blockchain_info(chain_context.blockchain_rid, true, blockchain_state.RUNNING)];
    }
}

/**
 * NM API Version: 4
 */
query nm_get_blockchain_replica_node_map(blockchain_rids: set&lt;byte_array&gt;): map&lt;byte_array, set&lt;byte_array&gt;&gt; {
    val res = map&lt;byte_array, set&lt;byte_array&gt;&gt;();
    for (brid in set(blockchain_rids)) {
        res.put(brid, get_blockchain_replica_nodes(brid));
    }
    return res;
}

/**
 * NM API Version: 3
 */
query nm_get_container_limits(name): map&lt;text, integer&gt; {
    require_container(name);

    var return_map = map&lt;text, integer&gt;();

    val container_limits = get_current_container_resource_limits(name);

    val container_units = container_limits[container_resource_limit_type.container_units];
    val max_blockchains = container_limits[container_resource_limit_type.max_blockchains];
    val extra_storage = container_limits[container_resource_limit_type.extra_storage];

    val cpu = _get_container_limit_or_default(standard_container_unit.cpu, container_units);
    val ram = _get_container_limit_or_default(standard_container_unit.ram, container_units);
    val io_read = _get_container_limit_or_default(standard_container_unit.io_read, container_units);
    val io_write = _get_container_limit_or_default(standard_container_unit.io_write, container_units);
    val storage = _get_container_limit_or_default(standard_container_unit.storage, container_units);

    return_map.put(container_resource_limit_type.container_units.name, container_units);
    return_map.put(container_resource_limit_type.max_blockchains.name, max_blockchains);
    return_map.put("storage", storage + extra_storage);
    return_map.put("cpu", cpu);
    return_map.put("ram", ram);
    return_map.put("io_read", io_read);
    return_map.put("io_write", io_write);

    return return_map;
}

function _get_container_limit_or_default(value: integer, container_units: integer): integer {
    return if (container_units == -1 or value == -1) -1 else container_units * value;
}

/**
 * Returns node containers
 * NM API Version: 3
 */
query nm_get_containers(pubkey): list&lt;text&gt; {
    val n = require_node(pubkey);
    val clusters = cluster_node @* { n } .cluster;
    val res = list&lt;text&gt;();
    for (cl in clusters) {
        val containers = container @* { cl, .state == container_state.RUNNING } .name;
        res.add_all(containers);
    }
    return res;
}

/**
 * NM API Version: 3
 */
query nm_get_blockchains_for_container(container_name: text): list&lt;byte_array&gt; {
    val container = container @? {container_name};
    val res = list&lt;byte_array&gt;();
    if (exists(container)) {
        val cluster = container.cluster;
        res.add_all(container_blockchain @* {container, .blockchain.state in [blockchain_state.RUNNING, blockchain_state.PAUSED]} .blockchain.rid);
    }
    return res;
}

/**
 * Returns container blockchain is running in.
 * NM API Version: 3
 */
query nm_get_container_for_blockchain(blockchain_rid: byte_array): text {
    val bc = require_blockchain(blockchain_rid);
    require(bc.state != blockchain_state.REMOVED, "Blockchain is removed: " + bc.rid);
    return container_blockchain @ { bc } .container.name;
}

/**
 * Returns a list of containers running on a node.
 * NM API Version: 14
 */
query nm_get_blockchain_containers_for_node(node_id: pubkey, blockchain_rid: byte_array): list&lt;text&gt; {
    val node = require_node(node_id);
    val bc = require_blockchain(blockchain_rid);

    val mbc = moving_blockchain @? { bc };
    if (exists(mbc)) {
        val res = list&lt;text&gt;();
        if (cluster_node @? { node, mbc.source.cluster } ??) res.add(mbc.source.name);
        if (cluster_node @? { node, mbc.destination.cluster } ??) res.add(mbc.destination.name);
        return res;
    }

    val unbc = unarchiving_blockchain @? { bc };
    if (exists(unbc)) {
        val res = list&lt;text&gt;();
        if (cluster_node @? { node, unbc.source.cluster } ??) res.add(unbc.source.name);
        if (cluster_node @? { node, unbc.destination.cluster } ??) res.add(unbc.destination.name);
        return res;
    }

    return [nm_get_container_for_blockchain(blockchain_rid)];
}

/**
 * Returns a list of brids with corresponding container that this blockchain is dependent on. At the given height.
 * NM API Version: 3
 */
query nm_get_blockchain_dependencies(blockchain, height: integer): list&lt;(byte_array, text)&gt; {
    val res = list&lt;(byte_array, text)&gt;();
    val conf_h = blockchain_configuration @? { blockchain, .height &lt;= height } (@sort .height) limit 1;
    if (conf_h != null) {
        val brids = blockchain_dependency @* {.me == blockchain, conf_h} .dependent_on.rid;
        for (brid in brids) {
            res.add((brid, container_blockchain @ { blockchain @{brid}} .container.name));
        }
    }
    return res;
}

/**
 * Precise Configuration Update: Returns list of pending configs with minimum_height &lt;= height
 * NM API Version: 5
 */
query nm_get_pending_blockchain_configuration(blockchain_rid: byte_array, height: integer):
        list&lt;(base_config: byte_array, signers: list&lt;pubkey&gt;, minimum_height: integer)&gt; {
    val bc = require_blockchain(blockchain_rid);
    return get_pending_blockchain_configuration(bc, height);
}

/**
 * Precise Configuration Update: Returns a pending blockchain config by config hash or null
 * NM API Version: 5
 */
query nm_get_pending_blockchain_configuration_by_hash(blockchain_rid: byte_array, config_hash: byte_array):
        (base_config: byte_array, signers: list&lt;pubkey&gt;, minimum_height: integer)? {
    val bc = require_blockchain(blockchain_rid);
    return get_pending_blockchain_configuration_by_hash(bc, config_hash);
}

/**
 * Precise Configuration Update: Returns a faulty blockchain config by height or null
 * NM API Version: 5
 */
query nm_get_faulty_blockchain_configuration(blockchain_rid: byte_array, height: integer): byte_array? {
    val bc = require_blockchain(blockchain_rid);
    return faulty_blockchain_configuration @? { bc, .reported_at_height == height }.config_hash;
}

/**
 * Returns current state of blockchain
 * NM API Version: 6
 */
query nm_get_blockchain_state(blockchain_rid: byte_array): text {
    val bc = require_blockchain(blockchain_rid);
    return bc.state.name;
}

/**
 * Returns Blockchain RID of management chain
 * NM API Version: 7
 */
query nm_get_management_chain() = blockchain @ { blockchains.directory_chain } (.rid);

/**
 * Returns blockchain_configuration_options structure for given blockchain and height or null
 * NM API Version 8
 */
query nm_get_blockchain_configuration_options(blockchain_rid: byte_array, height: integer)
    = blockchain_configuration_options @? { .blockchain.rid == blockchain_rid, .height &lt;= height }
    (@omit @sort_desc .height, $.to_struct()) limit 1;

/**
 * Returns the blockchain signers from the latest configuration
 * NM API Version 9
 */
query nm_get_blockchain_signers_in_latest_configuration(blockchain_rid: byte_array): list&lt;byte_array&gt; {
    val bc = require_blockchain(blockchain_rid);
    val signers_bytes = blockchain_configuration_signers @ { bc } (@omit @sort_desc .height, .signers) limit 1;
    return list&lt;byte_array&gt;.from_gtv(gtv.from_bytes(signers_bytes));
}

/**
 * Finds the next height blockchains were deactivated (removed, archived) at and returns found blockchain RIDs and heights.
 * Returns an empty list if there are no blockchains deactivated after `height`.
 * NM API Version 11
 */
query nm_find_next_inactive_blockchains(height: integer): list&lt;inactive_blockchain_info&gt; {
    val next_height = inactive_blockchain @? { .height &gt; height } (@sort .height) limit 1;
    return if (empty(next_height)) []
    else inactive_blockchain @* { next_height } ( inactive_blockchain_info(.blockchain.rid, .blockchain.state, .height) );
}

/**
 * Returns migrating (moving or unarchiving) blockchain info.
 * NM API Version 15
 */
query nm_get_migrating_blockchain_node_info(node_id: pubkey, blockchain_rid: byte_array): migrating_blockchain_node_info? {
    val node = node @? { node_id };
    if (node == null) return null;

    val bc = blockchain @? { blockchain_rid };
    if (bc == null) return null;

    val mvbc = moving_blockchain @? { bc } ($.to_struct());
    if (exists(mvbc)) {
        return migrating_blockchain_node_info(
            rid = [bc.rid.to_gtv(), mvbc.transaction.tx_rid.to_gtv()].hash(),
            source_container = mvbc.source.name,
            destination_container = mvbc.destination.name,
            is_source_node = exists(cluster_node @? { mvbc.source.cluster, node }),
            is_destination_node = exists(cluster_node @? { mvbc.destination.cluster, node }),
            final_height = mvbc.final_height
        );
    }

    val unbc = unarchiving_blockchain @? { bc } ($.to_struct());
    if (exists(unbc)) {
        return migrating_blockchain_node_info(
            rid = [bc.rid.to_gtv(), unbc.transaction.tx_rid.to_gtv()].hash(),
            source_container = unbc.source.name,
            destination_container = unbc.destination.name,
            is_source_node = exists(cluster_node @? { unbc.source.cluster, node }),
            is_destination_node = exists(cluster_node @? { unbc.destination.cluster, node }),
            final_height = unbc.final_height
        );
    }

    return null;
}

/**
 * Returns true if pubkey is allowed to deploy to blockchain, otherwise false.
 * NM API Version 19
 */
query nm_is_blockchain_provider(provider_pubkey: pubkey, blockchain_rid: byte_array): boolean {

    val provider = require_provider(provider_pubkey);
    val blockchain = require_blockchain(blockchain_rid);
    val container = container_blockchain @ { blockchain } .container;

    return roles.has_deploy_access(provider, container);
}
</string>
                            </entry>
                            <entry key="nm_api/nm_api_impl.rell">
                                <string>function get_cluster_node_blockchains(node): set&lt;blockchain_info&gt; {
    val blockchains = set(
        (cluster_node, container_blockchain) @* {
            cluster_node.node == node,
            cluster_node.cluster == container_blockchain.container.cluster,
            container_blockchain.blockchain.state in [blockchain_state.RUNNING, blockchain_state.PAUSED, blockchain_state.IMPORTING, blockchain_state.UNARCHIVING],
            container_blockchain.container.state == container_state.RUNNING or container_blockchain.container.state == container_state.MIGRATING
        } ( blockchain_info(container_blockchain.blockchain.rid, container_blockchain.blockchain.system, container_blockchain.blockchain.state) )
    );

    blockchains.add_all(
        signer_excluded_from_pending_configuration @* { .pubkey == node.pubkey }
            ( blockchain_info(.blockchain.rid, .blockchain.system, .blockchain.state) )
    );

    return blockchains;
}

function get_cluster_replica_node_blockchains(node) = set(
    (cluster_replica_node, container_blockchain) @* {
        cluster_replica_node.node == node,
        cluster_replica_node.cluster == container_blockchain.container.cluster,
        container_blockchain.blockchain.state in [blockchain_state.RUNNING, blockchain_state.PAUSED, blockchain_state.IMPORTING, blockchain_state.UNARCHIVING],
        container_blockchain.container.state == container_state.RUNNING or container_blockchain.container.state == container_state.MIGRATING
    } ( blockchain_info(container_blockchain.blockchain.rid, container_blockchain.blockchain.system, container_blockchain.blockchain.state) )
);

function get_blockchains_replicated_by_node(node) = set(
    blockchain_replica_node @* {
        node, .blockchain.state in [blockchain_state.RUNNING, blockchain_state.PAUSED, blockchain_state.IMPORTING, blockchain_state.UNARCHIVING]
    } (blockchain_info(.blockchain.rid, .blockchain.system, .blockchain.state))
);

function get_mandatory_system_chains(): set&lt;blockchain_info&gt; {
    val system_anchoring_blockchain = if (system_anchoring_chain.rid.empty()) null
        else blockchain @? { .rid == system_anchoring_chain.rid, .state in [blockchain_state.RUNNING, blockchain_state.PAUSED, blockchain_state.IMPORTING, blockchain_state.UNARCHIVING] };

    return if (system_anchoring_blockchain != null) set([blockchain_info(system_anchoring_chain.rid, true, system_anchoring_blockchain.state)])
        else set();
}

function get_blockchain_replica_nodes(blockchain_rid: byte_array): set&lt;byte_array&gt; {
    val replicas = set&lt;byte_array&gt;();

    // cluster_replica_node
    val replica_nodes = (crn: cluster_replica_node, cb: container_blockchain) @* {
        cb.blockchain.rid == blockchain_rid,
        cb.container.cluster == crn.cluster
    } (.node.pubkey);
    replicas.add_all(replica_nodes);

    // blockchain_replica_node
    replicas.add_all(blockchain_replica_node @* { .blockchain.rid == blockchain_rid } (.node.pubkey));

    return replicas;
}
</string>
                            </entry>
                            <entry key="price_oracle/module.rell">
                                <string>module;

import common.*;
import model.*;
import messaging.blockchain_rid.*;
import lib.icmf.*;

object price_oracle_chain {
    mutable rid: byte_array = x"";
}

operation init_price_oracle_chain(my_pubkey: pubkey, price_oracle_chain_config: byte_array) {
    require_is_provider_with_rate_limit(my_pubkey);
    require_is_system_provider(my_pubkey);

    require(price_oracle_chain.rid == x"", "Price oracle chain is already started");

    val nodes = cluster_node @* { system_cluster() } (@sort .node.pubkey);
    // do not write new configuration when size is 0 since it's impossible to recover from that
    require(nodes.size() &gt; 0, "System cluster must have at least one node");

    log("Adding price oracle chain to system container");
    val price_oracle_blockchain = add_blockchain(price_oracle_chain_config, nodes, blockchains.price_oracle_chain_name, system_container(), true);
    update price_oracle_chain(rid = price_oracle_blockchain.rid);

    send_message(blockchain_rid_topic, blockchain_rid(rid = price_oracle_blockchain.rid, name = price_oracle_blockchain.name).to_gtv());
}

query get_price_oracle_chain_rid(): byte_array? = if (price_oracle_chain.rid == x"") null else price_oracle_chain.rid;
</string>
                            </entry>
                            <entry key="proposal/module.rell">
                                <string>module;

import common.*;
import .voting.*;

enum proposal_type {
    configuration_at,
    configuration,
    bc,
    container_limits,
    cluster_limits,
    cluster_provider,
    cluster_remove,
    voter_set_update,
    provider_state,
    provider_is_system,
    provider_quota,
    provider_batch,
    blockchain_action,
    cluster_anchoring_configuration,
    container,
    container_remove,

    blockchain_import,
    configuration_import,
    finish_blockchain_import,

    foreign_blockchain_import,
    foreign_blockchain_blocks_import,

    blockchain_move_start,
    blockchain_move_cancel,
    blockchain_move_finish,
    force_configuration,
}

enum proposal_state {
    PENDING, APPROVED, REJECTED, REVOKED
}

entity proposal {
    index timestamp, proposal_type;
    proposed_by: provider;
    index voter_set;
    description: text = "";
    transaction = op_context.transaction;
    mutable state: proposal_state = proposal_state.PENDING;
}

enum blockchain_configuration_update_state {
    PENDING, SUCCESSFUL, FAILED
}

entity blockchain_configuration_update_attempt {
    key proposal, config_hash: byte_array;
    mutable applied_at_height: integer = -1;
    mutable state: blockchain_configuration_update_state = blockchain_configuration_update_state.PENDING;
}

operation revoke_proposal(my_pubkey: pubkey, proposal_rowid: rowid) {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    require_provider_auth_with_rate_limit(me);
    val prop = proposal @? { proposal_rowid };
    require(exists(prop), "Proposal not found: %d".format(proposal_rowid));
    require(prop!!.proposed_by == me, "It is only allowed to revoke own proposals");
    prop.state = proposal_state.REVOKED;
    delete_proposal(prop);
}

function create_proposal(proposal_type, proposed_by: provider, deployer: voter_set, description: text = ""): proposal {
    validate_metadata_text("description", description);
    return create proposal(op_context.last_block_time, proposal_type, proposed_by, deployer, description);
}

function delete_proposal(prop: proposal) {
    delete_proposal_handlers()[prop.proposal_type.name](prop);
}

@extendable function delete_proposal_handlers(): map&lt;text, (proposal) -&gt; unit&gt;;</string>
                            </entry>
                            <entry key="proposal/queries.rell">
                                <string>query get_proposals_range(from: timestamp, until: timestamp, only_pending: boolean) {
    return proposal @* {
        .timestamp &gt;= from,
        .timestamp &lt;= until,
        not(only_pending) or .state == proposal_state.PENDING
    } (.rowid, .proposal_type, .state);
}

query get_relevant_proposals(from: timestamp, until: timestamp, only_pending: boolean, my_pubkey: pubkey) {
    return (proposal, voter_set_member) @* {
        voter_set_member.voter_set == proposal.voter_set,
        voter_set_member.provider.pubkey == my_pubkey,
        proposal.timestamp &gt;= from,
        proposal.timestamp &lt;= until,
        not(only_pending) or proposal.state == proposal_state.PENDING
    } (proposal.rowid, proposal.proposal_type, proposal.state);
}

query get_proposal(id: rowid?): (id: rowid, timestamp: integer, type: proposal_type, proposed_by: pubkey, description: text, state: proposal_state)? {
    val result = if (id == null) proposal @? {} (@sort_desc @omit .rowid, $) limit 1 else proposal @? { id };
    if (result == null) return null;
    return (
        id = result.rowid,
        timestamp = result.timestamp,
        type = result.proposal_type,
        proposed_by = result.proposed_by.pubkey,
        description = result.description,
        state = result.state
    );
}

function require_proposal(rowid) = require(proposal @? { rowid }, "Proposal " + rowid + " not found");

function get_latest_proposal(rowid?, proposal_type) = if (rowid == null) proposal @ { proposal_type, proposal_state.PENDING } ( @max proposal ) else proposal @ { rowid };

struct proposal_voting_results {
    positive_votes: integer;
    negative_votes: integer;
    max_votes: integer;
    threshold: integer;
    voting_result;
}

query get_proposal_voting_results(rowid): proposal_voting_results {
    val proposal = require_proposal(rowid);
    require(proposal.state == proposal_state.PENDING, "This proposal is closed as %s. For info about voting use get_proposal_voter_info query.".format(proposal.state));
    val positive_votes = positive_votes(proposal);
    val negative_votes = negative_votes(proposal);
    val max_votes = max_votes(proposal.voter_set);
    val threshold = proposal.voter_set.threshold;
    val status = _compute_voting_result(positive_votes, negative_votes, max_votes, threshold);
    return proposal_voting_results(positive_votes, negative_votes, max_votes, threshold, status);
}

struct proposal_voter {
    provider: byte_array;
    provider_name: text;
    vote: boolean;
}

query get_proposal_voter_info(rowid): list&lt;proposal_voter&gt; {
    val proposal = require_proposal(rowid);
    return vote @* { .proposal.rowid == rowid } (proposal_voter(.provider.pubkey, .provider.name, .vote));
}

struct blockchain_configuration_attempt_data {
    state: blockchain_configuration_update_state;
    applied_at_height: integer;
}

query get_blockchain_configuration_update_attempt_state_by_tx_rid(tx_rid: byte_array): blockchain_configuration_attempt_data? =
    blockchain_configuration_update_attempt @? { .proposal.transaction.tx_rid == tx_rid } (blockchain_configuration_attempt_data(.state, .applied_at_height));

query get_blockchain_configuration_update_attempt_state_by_proposal(rowid): blockchain_configuration_attempt_data? =
    blockchain_configuration_update_attempt @? { .proposal.rowid == rowid } (blockchain_configuration_attempt_data(.state, .applied_at_height));

query get_proposal_ids_by_tx_rid(tx_rid: byte_array): list&lt;rowid&gt; {
    return proposal @* { .transaction.tx_rid == tx_rid } (.rowid);
}
</string>
                            </entry>
                            <entry key="proposal/voting/apply.rell">
                                <string>// NB: check authority before calling this function
function internal_vote(provider, proposal, vote: boolean) {
    require(voter_set_member @? { proposal.voter_set, provider }, provider.pubkey + " must be a member of the voter set");
    create vote(proposal, provider, vote);
    log("vote '%s' added for proposal %s".format(if (vote) "Yes" else "No", proposal_str(proposal)));
    try_to_apply_proposal(proposal);
}

function try_to_apply_proposal(prop: proposal) {
    val prop_str = proposal_str(prop);
    val results = get_proposal_voting_results(prop.rowid);
    when (results.voting_result) {
        pending -&gt; log("proposal is still under discussion:", prop_str);
        rejected -&gt; {
            log("proposal rejected:", prop_str);
            prop.state = proposal_state.REJECTED;
            delete_proposal(prop);
        }
        approved -&gt; {
            log("proposal approved:", prop_str);
            prop.state = proposal_state.APPROVED;
            apply_voting_result_handlers()[prop.proposal_type.name](prop);
            delete_proposal(prop);
        }
    }
}

@extendable function apply_voting_result_handlers(): map&lt;text, (proposal) -&gt; unit&gt;;

@extendable function proposal_str(prop: proposal): text? = "%s:%d".format(prop.proposal_type, prop.rowid);
</string>
                            </entry>
                            <entry key="proposal/voting/module.rell">
                                <string>module;

import ^.*;

entity vote {
    key proposal, provider;
    vote: boolean; // yes | no
}

function create_voter_set_internal(name, threshold: integer = 0, governor: voter_set? = null) {
    val vs = create voter_set(name, threshold);
    if (exists(governor)) {
        create voter_set_governance(voter_set = vs, governor = governor);
    } else {
        create voter_set_governance(voter_set = vs, governor = vs);
    }
    return vs;
}

/**
 * Governance semantics:
 * Anybody can create a new voter_set directly, but it consumes an action point.
 * voter_set can be updated. If voter_set_governance is defined, it will control voter set, otherwise, it is the voter set itself.
 */
operation create_voter_set(my_pubkey: pubkey, name, threshold: integer, initials: list&lt;pubkey&gt;?, governor: text?) {
    require_is_provider_with_rate_limit(my_pubkey);
    validate_entity_name(name);
    val governor_set = if (governor != null) require_voter_set(governor) else null;
    val vs = create_voter_set_internal(name, threshold, governor_set);
    if (exists(initials)) {
        for (prov in initials) {
            val p = require_provider(prov);
            create voter_set_member(voter_set = vs, p);
        }
    }
}

operation make_vote(my_pubkey: pubkey, proposal_id: integer, vote: boolean) {
    require_is_signer(my_pubkey);
    val provider = require_provider(my_pubkey);
    val prop = require_proposal(rowid(proposal_id));
    require(prop.state == proposal_state.PENDING, "The proposal is already closed as %s".format(prop.state));
    require_voter_set_member(prop.voter_set, provider);
    require(empty(vote @? {prop, provider}), "Only one vote per pubkey is allowed");
    internal_vote(provider, prop, vote);
}

operation retract_vote(my_pubkey: pubkey, proposal_id: integer) {
    require_is_signer(my_pubkey);
    val provider = require_provider(my_pubkey);
    val prop = require_proposal(rowid(proposal_id));
    require(prop.state == proposal_state.PENDING, "The proposal is already closed as %s".format(prop.state));
    require_voter_set_member(prop.voter_set, provider);

    if (exists(vote @? { prop, provider })) {
        delete vote @* { prop, provider };
        log("vote for proposal retracted:", proposal_str(prop));
        try_to_apply_proposal(prop);
    } else {
        log("no vote for proposal to retract:", proposal_str(prop));
    }
}
</string>
                            </entry>
                            <entry key="proposal/voting/queries.rell">
                                <string>query get_provider_votes(from: timestamp, until: timestamp, provider_key: pubkey) {
    return (proposal, vote) @* {
        vote.provider.pubkey == provider_key,
        vote.proposal == proposal,
        proposal.timestamp &gt;= from,
        proposal.timestamp &lt;= until
    } ( .proposal, .vote );
}</string>
                            </entry>
                            <entry key="proposal/voting/vote.rell">
                                <string>enum voting_result {
    pending,
    approved,
    rejected
}

namespace consensus_threshold {
    val majority = -1;
    val super_majority = 0;
}

function positive_votes(proposal) = vote @? { proposal, .vote == true } (@sum 1) ?: 0;
function negative_votes(proposal) = vote @? { proposal, .vote == false } (@sum 1) ?: 0;
function max_votes(voter_set) = voter_set_member @ { voter_set } (@sum 1);

function _compute_voting_result(yes: integer, no: integer, max: integer, threshold: integer): voting_result {
    require (threshold &gt;= -1 and threshold &lt;= max, "Invalid threshold, must be in range [%d, %d] but was: %d".format(-1, max, threshold));
    require (yes + no &lt;= max, "Too many votes");
    when (threshold) {
        -1 -&gt; return _compute_voting_result_majority(yes, no, max);
        0 -&gt; return _compute_voting_result_super_majority(yes, no, max);
        else -&gt; return _compute_voting_result_custom(yes, no, max, threshold);
    }
}

function _compute_voting_result_custom(yes: integer, no: integer, max: integer, threshold: integer): voting_result {
    return _voting_result(yes, no, max, threshold);
}

function _compute_voting_result_majority(yes: integer, no: integer, max: integer): voting_result {
    val required = max / 2 + 1;
    return _voting_result(yes, no, max, required);
}

function _compute_voting_result_super_majority(yes: integer, no: integer, max: integer): voting_result {
    val required = max - (max - 1) / 3;
    return _voting_result(yes, no, max, required);
}

function _voting_result(yes: integer, no: integer, max: integer, required: integer): voting_result {
    log("Votes - positive: %d, negative: %d, required: %d, maximum: %d".format(yes, no, required, max));
    return when {
        yes &gt;= required -&gt; voting_result.approved;
        no &gt; max - required -&gt; voting_result.rejected;
        else -&gt; voting_result.pending;
    };
}</string>
                            </entry>
                            <entry key="proposal_blockchain/module.rell">
                                <string>module;

import common.*;
import proposal.*;
import proposal_blockchain_move.*;
import model.*;
import .util.*;

@extend(on_configuration_updated)
function try_to_finalize_blockchain_unarchiving(message: configuration_updated) {
    val ubc = unarchiving_blockchain @? { .blockchain.rid == message.blockchain_rid, .final_height == message.height - 1 };
    if (exists(ubc)) {
        val src_nodes = cluster_node @* { ubc.source.cluster } ( .node );
        for (node in src_nodes) {
            delete blockchain_replica_node @? { ubc.blockchain, node };
        }
        if (not src_nodes.empty()) {
            log("blockchain_replica_node removed: %s".format(src_nodes @* {} ( .pubkey )));
        }

        ubc.blockchain.state = blockchain_state.RUNNING;

        log("Blockchain %s unarchiving from container %s to container %s was finished at height %s".format(
            ubc.blockchain.rid, ubc.source.name, ubc.destination.name, ubc.final_height)
        );

        delete ubc;
    }
}
</string>
                            </entry>
                            <entry key="proposal_blockchain/proposal_blockchain.rell">
                                <string>// Proposed bc:s are put here while waiting for enough positive votes.
entity pending_blockchain {
    key proposal;
    name;
    data: byte_array;
    container;
}

entity added_blockchain {
    key proposal;
    blockchain;
}

@extend(is_container_available_for_removal) function(container) = 
        if (exists(pending_blockchain @* { container }))
            "Container %s has pending proposals and can't be deleted. Resolve proposals first".format(container.name)
        else null;

/**
 * Proposes a new blockchain to a container.
 * NB: only the deployer voter set of the container can do this.
 */
operation propose_blockchain(my_pubkey: pubkey, config_data: byte_array, bc_name: text, container_name: text, description: text = "") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val container = require_running_container(container_name);
    require_container_deployer(container, me);
    validate_entity_name(bc_name);

    validate_blockchain_configuration(config_data, signers = true, header_hash = true, system_chain = false);

    require(empty(pending_blockchain @* { .data == config_data, .proposal.state == proposal_state.PENDING } limit 1), "Blockchain with the same config already proposed");
    val prop = create_proposal(proposal_type.bc, me, container.deployer, description);
    val bc = create pending_blockchain(prop, bc_name, data = config_data, container);
    validate_blockchain_proposal(bc);
    internal_vote(me, prop, true);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.bc.name: delete_pending_blockchain(*)];

function delete_pending_blockchain(proposal) {
    delete pending_blockchain @? { proposal };
}

@extend(apply_voting_result_handlers) function() = [proposal_type.bc.name: apply_blockchain_proposal(*)];

// Initial signers of new bc are the ones in cluster_node table.
function apply_blockchain_proposal(proposal) {
    val bc = pending_blockchain @? {proposal};
    if (bc == null) return;
    val nodes = validate_blockchain_proposal(bc);

    val blockchain = add_blockchain(bc.data, nodes, bc.name, bc.container);
    create added_blockchain(proposal, blockchain);
    log("Added blockchain", blockchain.rid);
}

query get_blockchain_proposal(rowid): (data:byte_array, container:text)? {
    return pending_blockchain @? { require_proposal(rowid) } ( data = decompress_configuration(.data), container = .container.name );
}

/**
 * tx_rid: transaction rid of blockchain proposal
 * @return blockchain RID, or null if proposal is not applied
 */
query find_blockchain_rid(tx_rid: byte_array): byte_array? { 
    val proposal = require(proposal @? { proposal_type.bc, .transaction.tx_rid == tx_rid }, "No blockchain proposal found in given transaction");
    return added_blockchain @? { proposal }.blockchain.rid;
}

function validate_blockchain_proposal(proposal_details: pending_blockchain) {

    val container = proposal_details.container;

    require_running_container(container.name);
    require(not container.system, "Proposing blockchain to a system container is not allowed");

    // Require container resource limits are honored
    require_container_is_not_full(container);

    val nodes = cluster_node @* { container.cluster } (@sort .node.pubkey);
    // do not write new configuration when size is 0 since it's impossible to recover from that
    require(nodes.size() &gt; 0, "Cluster must have at least one node");

    return nodes;
}</string>
                            </entry>
                            <entry key="proposal_blockchain/proposal_blockchain_action.rell">
                                <string>enum blockchain_action {
    pause,
    resume,
    remove,
    archive,
    unarchive
}

struct unarchive_args {
    destination_container: text;
    final_height: integer;
}

entity pending_blockchain_action {
    key proposal;
    key blockchain;
    action: blockchain_action;
    args: byte_array = x"";
}

@extend(proposal_str)
function proposal_blockchain_action_str(prop: proposal): text? =
    pending_blockchain_action @? { prop } ( "%s:%s:%d".format(prop.proposal_type, .action, prop.rowid) );

@extend(apply_voting_result_handlers) function() = [proposal_type.blockchain_action.name: apply_blockchain_action(*)];

function apply_blockchain_action(proposal) {
    val pba = pending_blockchain_action @? { proposal };
    if (pba == null) return;
    validate_blockchain_action_proposal(pba.blockchain, pba.action);
    when (pba.action) {
        pause -&gt; _apply_pause_blockchain(pba, proposal);
        resume -&gt; _apply_resume_blockchain(pba, proposal);
        remove -&gt; _apply_delete_blockchain(pba, proposal);
        archive -&gt; _apply_archive_blockchain(pba, proposal);
        unarchive -&gt; _apply_unarchive_blockchain(pba, proposal);
    }
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.blockchain_action.name: delete_pending_blockchain_action(*)];

function delete_pending_blockchain_action(proposal) {
    delete pending_blockchain_action @? { proposal };
}

// Stop block production by pausing
function _apply_pause_blockchain(action: pending_blockchain_action, proposal) {
    action.blockchain.state = blockchain_state.PAUSED;
}

// Restart block production
function _apply_resume_blockchain(action: pending_blockchain_action, proposal) {
    action.blockchain.state = blockchain_state.RUNNING;
}

// Delete everything about this bc except D1 information
function _apply_delete_blockchain(action: pending_blockchain_action, proposal) {
    val bc = action.blockchain;
    remove_blockchain(bc);
    delete pending_blockchain_action @ { proposal };
}

function _apply_archive_blockchain(action: pending_blockchain_action, proposal) {
    action.blockchain.state = blockchain_state.ARCHIVED;
    delete blockchain_replica_node @* { action.blockchain };
    create inactive_blockchain(action.blockchain, op_context.block_height);
}

function _apply_unarchive_blockchain(action: pending_blockchain_action, proposal) {
    action.blockchain.state = blockchain_state.UNARCHIVING;
    val args = unarchive_args.from_bytes(action.args);

    // keep src cluster/container nodes as replicas
    val src_container = container_blockchain @ { action.blockchain } (.container);
    val src_cluster_nodes_to_replicate_on = cluster_node @* {
        src_container.cluster, .node not in blockchain_replica_node @* { action.blockchain } (.node)
    } (.node);
    for (node in src_cluster_nodes_to_replicate_on) {
        create blockchain_replica_node(action.blockchain, node);
    }
    // remove bc from container
    delete container_blockchain @* { action.blockchain };

    // add bc to the dst container
    val dst_container = require_container(args.destination_container);
    require_container_is_not_full(dst_container);
    create container_blockchain(dst_container, action.blockchain);
    // remove dst cluster/container nodes from bc replicas
    val dst_nodes = cluster_node @* { dst_container.cluster } (@omit @sort .node.pubkey, .node);
    require(dst_nodes.size() &gt; 0, "Cluster %s must have at least one node".format(dst_container.cluster.name));
    for (node in dst_nodes) {
        delete blockchain_replica_node @* { action.blockchain, node };
    }

    // create a base_config and signers config at final_height (base_config needed if signers don't change)
    // FYI: PCU-based update (i.e. update_configuration_signers_regular()) can't be used here.
    require_height_is_greater_or_equal_to_last_config_height(action.blockchain, args.final_height);
    // base config
    val base_config = require(get_blockchain_configuration(action.blockchain.rid, args.final_height)?.base_config,
        "Can't find config for %s for block %s".format(action.blockchain.rid, args.final_height));
    compress_and_store_configuration(action.blockchain, args.final_height + 1, make_config_unique(base_config));
    // signers config
    val encoded_dst_nodes = (dst_nodes @* {} ( .pubkey )).to_gtv().to_bytes();
    create blockchain_configuration_signers(action.blockchain, args.final_height + 1, encoded_dst_nodes);

    // create unarchiving_blockchain state
    create unarchiving_blockchain(action.blockchain, source = src_container, destination = dst_container, args.final_height);
    delete inactive_blockchain @? { action.blockchain };
}

operation propose_blockchain_action(my_pubkey: pubkey, blockchain_rid: byte_array, action: blockchain_action, description: text = "") {
    require(action != blockchain_action.unarchive, "Use propose_blockchain_unarchive_action() for unarchive operation");
    propose_blockchain_action_impl(my_pubkey, blockchain_rid, action, x"", description);
}

operation propose_blockchain_unarchive_action(my_pubkey: pubkey, blockchain_rid: byte_array, destination_container: text, final_height: integer, description: text = "") {
    val args = unarchive_args(destination_container, final_height);
    propose_blockchain_action_impl(my_pubkey, blockchain_rid, blockchain_action.unarchive, args.to_bytes(), description);
}

function propose_blockchain_action_impl(my_pubkey: pubkey, blockchain_rid: byte_array, action: blockchain_action, args: byte_array, description: text = "") {
    val me = require_is_provider_with_rate_limit(my_pubkey);

    // blockchain state requirements
    val blockchain = require_blockchain(blockchain_rid);
    val pending_proposal = pending_blockchain_action @? { blockchain, .proposal.state == proposal_state.PENDING };
    require(empty(pending_proposal), "Blockchain action already proposed: " + pending_proposal?.proposal);

    validate_blockchain_action_proposal(blockchain, action);

    val container = container_blockchain @ { blockchain } .container;
    require_container_deployer(container, me);
    var deployer = container.deployer;

    // action requirements
    if (action == blockchain_action.remove) {
        require(empty(blockchain_dependency @* { .dependent_on == blockchain }), "Blockchain can't be removed since other blockchains depend on it: " + blockchain.rid);
    } else if (action == blockchain_action.archive) {
        require(not(blockchain.system), "Archiving of system chains is not allowed: " + blockchain.rid);
    } else if (action == blockchain_action.unarchive) {
        // unarchiving from a container to another container running on the same node is not allowed
        val args0 = unarchive_args.from_bytes(args);
        val dst_container = require_container(args0.destination_container);
        require_container_deployer(dst_container, me);
        deployer = dst_container.deployer;
        require_container_is_not_full(dst_container);
    }

    val prop = create_proposal(proposal_type.blockchain_action, me, deployer, description);
    val pba = create pending_blockchain_action(prop, blockchain, action, args);
    internal_vote(me, prop, true);
}

query get_blockchain_action_proposal(rowid) {
    val proposal = get_latest_proposal(rowid, proposal_type.blockchain_action);
    if (proposal == null) return null;
    val pba = pending_blockchain_action @ { proposal };
    return (
        blockchain = pba.blockchain.rid,
        blockchain_name = pba.blockchain.name,
        action = pba.action
    );
}

query get_blockchain_unarchive_action_proposal(rowid) {
    val proposal = get_latest_proposal(rowid, proposal_type.blockchain_action);
    if (proposal == null) return null;
    val pba = pending_blockchain_action @ { proposal };
    require(pba.action == blockchain_action.unarchive, "Proposal action is not 'unarchive'. Use get_blockchain_action_proposal() query");
    val args = unarchive_args.from_bytes(pba.args);
    val source_container = container_blockchain @ { pba.blockchain } ( .container.name );
    return (
        blockchain = pba.blockchain.rid,
        blockchain_name = pba.blockchain.name,
        action = pba.action,
        source_container = source_container,
        destination_container = args.destination_container,
        final_height = args.final_height
    );
}

function validate_blockchain_action_proposal(blockchain, action: blockchain_action) {

    // simple states
    when (blockchain.state) {
        blockchain_state.RUNNING -&gt; {
            require(action != blockchain_action.resume, "Blockchain is already running: " + blockchain.rid);
            require(action != blockchain_action.unarchive, "Running blockchain can't be unarchived: " + blockchain.rid);
        }
        blockchain_state.PAUSED -&gt; {
            require(action != blockchain_action.pause, "Blockchain is already paused: " + blockchain.rid);
            require(action != blockchain_action.unarchive, "Paused blockchain can't be unarchived: " + blockchain.rid);
        }
        blockchain_state.REMOVED -&gt; {
            require(false, "Removed blockchain can't be %sd: %s".format(action.name, blockchain.rid));
        }
        blockchain_state.IMPORTING -&gt; {
            require(false, "Importing blockchain can't be %sd: %s".format(action.name, blockchain.rid));
        }
        blockchain_state.ARCHIVED -&gt; {
            require(action != blockchain_action.pause, "Archived blockchain can't be paused: " + blockchain.rid);
            require(action != blockchain_action.resume, "Archived blockchain can't be resumed: " + blockchain.rid);
            require(action != blockchain_action.archive, "Blockchain is already archived: " + blockchain.rid);
        }
    }

    // complex state: moving
    if (exists(pending_blockchain_move @? { blockchain }) or exists(moving_blockchain @? { blockchain, .final_height == -1 })) {
        require(false, "Moving blockchain can't be %sd: %s".format(action.name, blockchain.rid));
    }

    // complex state: unarchiving
    if (exists(unarchiving_blockchain @? { blockchain })) {
        require(false, "Unarchiving blockchain can't be %sd: %s".format(action.name, blockchain.rid));
    }
}
</string>
                            </entry>
                            <entry key="proposal_blockchain/proposal_configuration.rell">
                                <string>entity pending_configuration {
    key proposal;
    blockchain;
    data: byte_array;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.configuration.name: apply_configuration(*)];

function apply_configuration(proposal) {
    val pc = pending_configuration @? { proposal };
    if (pc == null) return;
    val is_chain0 = pc.blockchain.rid == chain_context.blockchain_rid;

    if (is_chain0) {
        apply_configuration_chain0(pc);
    } else {
        apply_configuration_regular(pc, proposal);
    }
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.configuration.name: delete_pending_configuration(*)];

function delete_pending_configuration(proposal) {
    delete pending_configuration @? { proposal };
}

function apply_configuration_chain0(pc: pending_configuration) {
    val apply_at_height = op_context.block_height + 1; // NB: compute_blockchain_info_list()/get_cluster_node_blockchains() relies on this
    log("Base configuration update chain0 at height %d".format(apply_at_height));
    require(empty(blockchain_configuration @? { pc.blockchain, apply_at_height }), "Configuration at height %d already exists".format(apply_at_height));
    val unique_base_config = make_config_unique(pc.data);
    compress_and_store_configuration(pc.blockchain, apply_at_height, unique_base_config);
    add_dependencies(unique_base_config, pc.blockchain.rid, apply_at_height);
}

function apply_configuration_regular(pc: pending_configuration, proposal) {
    val last_configuration_height = (blockchain_configuration @? { pc.blockchain } (@sort_desc .height) limit 1) ?: 0;
    val last_pending_configuration = pending_blockchain_configuration @? { pc.blockchain } (@sort_desc @omit .minimum_height, $) limit 1;
    val (minimum_height, signers) = if (last_pending_configuration != null)
        (last_configuration_height.max(last_pending_configuration.minimum_height) + 1,
         last_pending_configuration.signers)
     else
        (last_configuration_height + 1,
         blockchain_configuration_signers @ { pc.blockchain } (@omit @sort_desc .height, .signers) limit 1);
    log("Base configuration update for chain %s at minimum height %d".format(pc.blockchain.rid, minimum_height));
    val unique_base_config = make_config_unique(pc.data);
    val config_hash = calculate_configuration_hash(unique_base_config, list&lt;byte_array&gt;.from_gtv(gtv.from_bytes(signers)));
    create pending_blockchain_configuration(
        pc.blockchain,
        minimum_height,
        config_hash,
        base_config = unique_base_config,
        signers
    );
    create blockchain_configuration_update_attempt(
        proposal,
        config_hash
    );
}

operation propose_configuration(my_pubkey: pubkey, blockchain_rid: byte_array, config_data: byte_array, description: text = "") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val blockchain = require_active_blockchain(blockchain_rid);
    val container = container_blockchain @ {blockchain} .container;
    require_container_deployer(container, me);
    val is_chain0 = blockchain.rid == chain_context.blockchain_rid;

    validate_blockchain_configuration(config_data, signers = not is_chain0, header_hash = not is_chain0, system_chain = blockchain.system);

    val prop = create_proposal(proposal_type.configuration, me, container.deployer, description);
    create pending_configuration(prop, blockchain, config_data);
    internal_vote(me, prop, true);
}

struct blockchain_configuration_data {
    height: integer;
    blockchain;
    data: byte_array;
}

struct pending_blockchain_configuration_data {
    proposal;
    blockchain;
    data: byte_array;
}

query get_configuration_proposal(rowid?): (
    current_conf:blockchain_configuration_data,
    proposed_conf:pending_blockchain_configuration_data
)? {
    val proposal = get_latest_proposal(rowid, proposal_type.configuration);
    if (proposal == null) return null;
    val config = pending_configuration @? { require_proposal(proposal.rowid) }
        (pending_blockchain_configuration_data(.proposal, .blockchain, decompress_configuration(.data)));
    if (config == null) return null;

    val current = get_latest_blockchain_configuration_data(config.blockchain);

    return (
        current_conf = blockchain_configuration_data(current.height, config.blockchain, current.config),
        proposed_conf = config
    );
}
</string>
                            </entry>
                            <entry key="proposal_blockchain/proposal_configuration_at.rell">
                                <string>entity pending_configuration_at {
    key proposal;
    blockchain;
    height: integer;
    force: boolean;
    data: byte_array;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.configuration_at.name: apply_configuration_at(*)];

function apply_configuration_at(proposal) {
    val pc = pending_configuration_at @? { proposal };
    if (pc == null) return;
    validate_configuration_at_proposal(pc);

    val height = pc.height;
    val is_chain0 = pc.blockchain.rid == chain_context.blockchain_rid;
    val config_data = make_config_unique(pc.data);
    compress_and_store_configuration(pc.blockchain, height, config_data, true);
    add_dependencies(config_data, pc.blockchain.rid, height);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.configuration_at.name: delete_pending_configuration_at(*)];

function delete_pending_configuration_at(proposal) {
    delete pending_configuration_at @? { proposal };
}

/*
 * Proposes a new configuration for a blockchain.
 * NB: Only the deployer of the container the blockchain is running in can perform operation
 */
operation propose_configuration_at(my_pubkey: pubkey, blockchain_rid: byte_array, config_data: byte_array, height: integer, force: boolean, description: text = "") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val blockchain = require_active_blockchain(blockchain_rid);
    val container = container_blockchain @ { blockchain }.container;
    require_container_deployer(container, me);
    val is_chain0 = blockchain.rid == chain_context.blockchain_rid or
        chain_context.blockchain_rid == x"0000000000000000000000000000000000000000000000000000000000000000"; // for unit tests
    require(is_chain0, "Proposing configuration at a specific height disallowed for chain: " + blockchain_rid);
    validate_blockchain_configuration(config_data, signers = not is_chain0, header_hash = false, system_chain = blockchain.system);

    require(height &gt;= 0, "height must be &gt;= 0");

    val pending_config = pending_configuration_at @? { blockchain, height, .proposal.state == proposal_state.PENDING } limit 1;
    require(empty(pending_config),
        "Pending configuration proposal already exists: blockchain_rid: %s, height: %d. Revoke the proposal %d first."
            .format(blockchain.rid, height, pending_config?.proposal)
    );

    val prop = create_proposal(proposal_type.configuration_at, me, container.deployer, description);
    val pc = create pending_configuration_at(prop, blockchain, height, force, data = config_data);
    validate_configuration_at_proposal(pc);
    internal_vote(me, prop, true);
}

struct pending_blockchain_configuration_at_data {
    proposal;
    blockchain;
    data: byte_array;
    height: integer;
    force: boolean;
}

query get_configuration_proposal_at(rowid?): (
    current_conf:blockchain_configuration_data,
    proposed_conf:pending_blockchain_configuration_at_data
)? {
    val proposal = get_latest_proposal(rowid, proposal_type.configuration_at);
    if (proposal == null) return null;
    val config = pending_configuration_at @? { require_proposal(proposal.rowid) }
        (pending_blockchain_configuration_at_data(.proposal, .blockchain, decompress_configuration(.data), .height, .force));
    if (config == null) return null;

    val current = get_latest_blockchain_configuration_data(config.blockchain);

    return (
        current_conf = blockchain_configuration_data(current.height, config.blockchain, current.config),
        proposed_conf = config
    );
}

function validate_configuration_at_proposal(proposal_details: pending_configuration_at) {
    require(empty(blockchain_configuration @? { proposal_details.blockchain, proposal_details.height } limit 1) or proposal_details.force, "Configuration at height %d already exists for blockchain %s, need to set force to override".format(proposal_details.height, proposal_details.blockchain.rid));
}
</string>
                            </entry>
                            <entry key="proposal_blockchain/proposal_forced_configuration.rell">
                                <string>entity proposed_forced_configuration {
    key proposal;
    blockchain;
    height: integer;
    config_data: byte_array;
}

entity forced_configuration {
    index blockchain;
    height: integer;
    config_hash: byte_array;
}

operation propose_forced_configuration(my_pubkey: pubkey, blockchain_rid: byte_array, config_data: byte_array, height: integer, description: text = "") {
    val provider = require_is_provider_with_rate_limit(my_pubkey);
    val blockchain = require_active_blockchain(blockchain_rid);
    val container = container_blockchain @ { blockchain }.container;
    require_container_deployer(container, provider);

    val is_chain0 = blockchain.rid == chain_context.blockchain_rid;
    require(not is_chain0, "Forcing configuration disallowed for directory chain: " + blockchain_rid);

    validate_blockchain_configuration(config_data, true, header_hash = true, system_chain = blockchain.system);

    val prop = create_proposal(proposal_type.force_configuration, provider, container.deployer, description);
    create proposed_forced_configuration(prop, blockchain, height, config_data);
    internal_vote(provider, prop, true);
}

@extend(apply_voting_result_handlers) function() = [proposal_type.force_configuration.name: apply_proposed_forced_configuration(*)];

function apply_proposed_forced_configuration(proposal) {
    val pfc = proposed_forced_configuration @? { proposal };
    if (pfc == null) return;

    val blockchain = require_active_blockchain(pfc.blockchain.rid);
    require(blockchain.state == blockchain_state.PAUSED, "Forcing configuration only allowed on PAUSED blockchains. Blockchain rid: %s, state: %s".format(blockchain.rid, blockchain.state));

    val cluster = container_blockchain @ { blockchain }  ( .container ) .cluster;
    val cluster_signers = cluster_node @* { cluster } (@sort .node.pubkey);
    val signers = cluster_signers.to_gtv().to_bytes();

    val config_data = make_config_unique(pfc.config_data);
    compress_and_store_configuration(blockchain, pfc.height, config_data, true);

    val blockchain_configuration_signers = blockchain_configuration_signers @? { blockchain, pfc.height};
    if (blockchain_configuration_signers == null) {
        create blockchain_configuration_signers(blockchain, pfc.height, signers);
    } else {
        blockchain_configuration_signers.signers = signers;
    }
    create forced_configuration(blockchain, pfc.height, config_data.hash());
    delete pending_blockchain_configuration @* {blockchain};
    delete signer_excluded_from_pending_configuration @* {blockchain};

    add_dependencies(config_data, blockchain.rid, pfc.height);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.force_configuration.name: delete_proposed_forced_configuration(*)];

function delete_proposed_forced_configuration(proposal) {
    delete proposed_forced_configuration @? { proposal };
}

struct proposed_forced_configuration_data {
    proposal;
    blockchain;
    height: integer;
    config_data: byte_array;
}

struct forced_configuration_data {
    blockchain;
    height: integer;
    config_hash: byte_array;
}

query get_proposed_forced_configuration(rowid?): (
    current_conf: blockchain_configuration_data,
    forced_conf: proposed_forced_configuration_data
)? {
    val proposal = get_latest_proposal(rowid, proposal_type.force_configuration);
    if (proposal == null) return null;
    val config = proposed_forced_configuration @? { require_proposal(proposal.rowid) }
        (proposed_forced_configuration_data(.proposal, .blockchain, .height, decompress_configuration(.config_data)));
    if (config == null) return null;

    val current = get_latest_blockchain_configuration_data(config.blockchain);

    return (
        current_conf = blockchain_configuration_data(current.height, config.blockchain, current.config),
        forced_conf = config
    );
}

query get_forced_configuration(blockchain_rid: byte_array): list&lt;forced_configuration_data&gt; {
    val blockchain = require_blockchain(blockchain_rid);
    return forced_configuration @* { blockchain } (
        forced_configuration_data(
            .blockchain,
            .height,
            .config_hash
        )
    );
}
</string>
                            </entry>
                            <entry key="proposal_blockchain/util/module.rell">
                                <string>module;

import model.*;
import common.*;

struct module_args {
    max_config_path_depth: integer;
    max_config_size: integer;
    max_block_size: integer;
    min_fast_revolt_status_timeout: integer;
    min_inter_block_interval: integer;
    allowed_dapp_chain_gtx_modules: gtv;
    allowed_dapp_chain_sync_exts: gtv;
    require_revolt_when_should_build_block: boolean = true;
    require_add_primary_key_to_header: boolean = true;
}

function validate_blockchain_configuration(config_data: byte_array, signers: boolean, header_hash: boolean, system_chain: boolean) {
    val config_map = map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(config_data));
    if (signers) {
        require("signers" not in config_map, "Configuration must not contain \"signers\"");
    }
    if (header_hash) {
        require("config_consensus_strategy" in config_map,
            "Configuration must contain \"config_consensus_strategy\"=\"HEADER_HASH\"");
        require(config_map["config_consensus_strategy"] == "HEADER_HASH".to_gtv(),
            "Configuration must contain \"config_consensus_strategy\"=\"HEADER_HASH\"");
    }

    if (chain_context.args.require_add_primary_key_to_header) {
        val config_add_primary_key_to_header = config_map.get_or_null("add_primary_key_to_header");
        require(config_add_primary_key_to_header, "Blockchain configuration property 'add_primary_key_to_header' must be configured");
        require(
            boolean.from_gtv(config_add_primary_key_to_header),
            "Configured value 'false' is not allowed for 'add_primary_key_to_header' property"
        );
    }

    val block_strategy_config = config_map.get_or_null("blockstrategy");
    require(block_strategy_config, "blockstrategy configuration must not be empty");
    validate_block_strategy_config(map&lt;text, gtv&gt;.from_gtv(block_strategy_config));

    val revolt_config = config_map.get_or_null("revolt");
    require(revolt_config, "revolt configuration must not be empty");
    validate_revolt_config(map&lt;text, gtv&gt;.from_gtv(revolt_config));

    // Check modules and extensions for non system chains
    if (not system_chain) {
        val configured_gtx = config_map.get_or_null("gtx");
        if (configured_gtx != null) {
            validate_gtx_modules(map&lt;text, gtv&gt;.from_gtv(configured_gtx));
        }

        validate_sync_exts(config_map);
    }

    validate_compression(config_data);
}

function validate_block_strategy_config(block_strategy_config: map&lt;text, gtv&gt;) {
    val config_max_block_size = block_strategy_config.get_or_null("maxblocksize");
    if (config_max_block_size != null) {
        require(
            integer.from_gtv(config_max_block_size) &lt;= chain_context.args.max_block_size,
            "Configured max block size exceeds maximum allowed size %s".format(chain_context.args.max_block_size)
        );
    }
    val config_min_inter_block_interval = block_strategy_config.get_or_null("mininterblockinterval");
    require(config_min_inter_block_interval, "mininterblockinterval configuration must be set");
    require(
        integer.from_gtv(config_min_inter_block_interval) &gt;= chain_context.args.min_inter_block_interval,
        "Configured min inter block interval is lower than minimum allowed %s"
            .format(chain_context.args.min_inter_block_interval)
    );
}

function validate_revolt_config(revolt_config: map&lt;text, gtv&gt;) {
    val config_fast_revolt_status_timeout = revolt_config.get_or_null("fast_revolt_status_timeout");
    if (config_fast_revolt_status_timeout != null) {
        val config_fast_revolt_status_timeout_int = integer.from_gtv(config_fast_revolt_status_timeout);
        if (config_fast_revolt_status_timeout_int != -1) {
            require(
                config_fast_revolt_status_timeout_int &gt;= chain_context.args.min_fast_revolt_status_timeout,
                "Configured fast revolt status timeout is lower than minimum allowed %s"
                    .format(chain_context.args.min_fast_revolt_status_timeout)
            );
        }
    }
    if (chain_context.args.require_revolt_when_should_build_block) {
        val config_revolt_when_should_build_block = revolt_config.get_or_null("revolt_when_should_build_block");
        require(config_revolt_when_should_build_block, "Blockchain configuration property 'revolt_when_should_build_block' must be configured");
        require(
            boolean.from_gtv(config_revolt_when_should_build_block),
            "Configured value 'false' is not allowed for 'revolt_when_should_build_block' property"
        );
    }
}

function validate_gtx_modules(gtx_config: map&lt;text, gtv&gt;) {
    val allowed_gtx_modules = set&lt;text&gt;.from_gtv(chain_context.args.allowed_dapp_chain_gtx_modules);
    val configured_gtx_modules = gtx_config.get_or_null("modules");
    require(
        configured_gtx_modules == null or allowed_gtx_modules.contains_all(list&lt;text&gt;.from_gtv(configured_gtx_modules)),
        "Some of the configured GTX modules are not in the allowed set %s".format(allowed_gtx_modules)
    );
}

function validate_sync_exts(config_map: map&lt;text, gtv&gt;) {
    val allowed_sync_exts = set&lt;text&gt;.from_gtv(chain_context.args.allowed_dapp_chain_sync_exts);
    val configured_sync_exts = config_map.get_or_null("sync_ext");
    require(
        configured_sync_exts == null or allowed_sync_exts.contains_all(list&lt;text&gt;.from_gtv(configured_sync_exts)),
        "Some of the configured synchronization infrastructure extensions are not in the allowed set %s".format(allowed_sync_exts)
    );
}

function validate_compression(config_data: byte_array) {
    val config_map = map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(config_data));
    var total_config_size = 0;

    if (config_map.contains(COMPRESSED_ROOTS_CONFIG_KEY)) {
        val compressed_roots = list&lt;compressed_root&gt;.from_gtv_pretty(config_map[COMPRESSED_ROOTS_CONFIG_KEY]);
        for (root in compressed_roots) {
            val path_depth = root.path.size();
            require(path_depth &gt; 0, "Compression at configuration root level is not allowed");
            require(
                path_depth &lt;= chain_context.args.max_config_path_depth,
                "Compression path depth %d exceeds maximum allowed depth %d".format(path_depth, chain_context.args.max_config_path_depth)
            );

            val config_path = extract_config_path(config_map, root.path);
            val hashes_to_decompress = set&lt;byte_array&gt;();
            for (compressed_key in root.compressed_keys) {
                if (config_path != null and config_path.contains(compressed_key.content_key)) {
                    require(
                        compressed_key.content_hash == config_path[compressed_key.content_key].hash(),
                        "Ambiguous compression detected for path %s, key '%s' is defined with different values in compression root and actual configuration".format(
                            root.path, compressed_key.content_key
                        )
                    );
                }
                hashes_to_decompress.add(compressed_key.content_hash);
            }

            val compressed_content_sizes = map&lt;byte_array, integer&gt;();
            for ((hash, size) in compressed_blockchain_configuration_part @* { .hash in hashes_to_decompress } (.hash, .data.size())) {
                compressed_content_sizes.put(hash, size);
            }
            for (compressed_key in root.compressed_keys) {
                total_config_size += compressed_content_sizes[compressed_key.content_hash];
            }

            // Check if we can break early
            require(
                total_config_size &lt;= chain_context.args.max_config_size,
                "Configuration exceeds maximum allowed size %d".format(chain_context.args.max_config_size)
            );
        }
        config_map.remove(COMPRESSED_ROOTS_CONFIG_KEY);
    }

    total_config_size += config_map.to_gtv().to_bytes().size();
    require(
        total_config_size &lt;= chain_context.args.max_config_size,
        "Configuration exceeds maximum allowed size %d".format(chain_context.args.max_config_size)
    );
}

function extract_config_path(config_map: map&lt;text, gtv&gt;, path: list&lt;text&gt;): map&lt;text, gtv&gt;? {
    var config_path = config_map;
    for (p in path) {
        if (config_path.contains(p)) {
            config_path = map&lt;text, gtv&gt;.from_gtv(config_path[p]);
        } else {
            return null;
        }
    }
    return config_path;
}
</string>
                            </entry>
                            <entry key="proposal_blockchain_import/module.rell">
                                <string>module;

import common.*;
import proposal.*;
import model.*;
import .util.*;

@extend(on_configuration_updated)
function try_to_finalize_foreign_blockchain_import(message: configuration_updated) {
    val ifb = importing_foreign_blockchain @? { .blockchain_rid == message.blockchain_rid, .final_height == message.height - 1 };
    if (exists(ifb)) {
        delete ifb;
        delete blockchain_configuration_options @* { .blockchain.rid == message.blockchain_rid };
        log("Foreign blockchain import was finished at height %s".format(message.height - 1));
    }
}
</string>
                            </entry>
                            <entry key="proposal_blockchain_import/proposal_blockchain_import.rell">
                                <string>entity pending_blockchain_import {
    key proposal;
    name;
    blockchain_rid: byte_array;
    config_data: byte_array;
    container;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.blockchain_import.name: apply_blockchain_import(*)];
@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.blockchain_import.name: delete_pending_blockchain_import(*)];

operation propose_import_blockchain(my_pubkey: pubkey, config_data: byte_array, blockchain_rid: byte_array, bc_name: text, container_name: text,
                                    description: text = "Propose import blockchain") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val container = require_container(container_name);
    require_container_deployer(container, me);
    validate_entity_name(bc_name);

    require("signers" in map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(config_data)), "No signers in configuration");

    require(empty(pending_blockchain_import @* { .blockchain_rid == blockchain_rid, .proposal.state == proposal_state.PENDING } limit 1), "Already proposed");
    val prop = create_proposal(proposal_type.blockchain_import, me, container.deployer, description);
    val pbi = create pending_blockchain_import(prop, bc_name, blockchain_rid, config_data, container);
    validate_blockchain_import_proposal(pbi);
    internal_vote(me, prop, true);
}

function apply_blockchain_import(proposal) {
    val pbi = pending_blockchain_import @? { proposal };
    if (pbi == null) return;
    validate_blockchain_import_proposal(pbi);

    val blockchain = create blockchain(pbi.blockchain_rid, pbi.name, system = false, state = blockchain_state.IMPORTING);
    create container_blockchain(pbi.container, blockchain);

    add_configuration_with_signers(blockchain, 0, pbi.config_data);

    log("Blockchain import started: %s / %s".format(pbi.name, pbi.blockchain_rid));
}

function delete_pending_blockchain_import(proposal) {
    delete pending_blockchain_import @? { proposal };
}

query get_blockchain_import_proposal(rowid?):
    (name: text, blockchain_rid: byte_array, config_data: byte_array, container: text)?
{
    val proposal = get_latest_proposal(rowid, proposal_type.blockchain_import);
    if (proposal == null) return null;
    return pending_blockchain_import @ { proposal } (
        name = .name,
        blockchain_rid = .blockchain_rid,
        config_data = .config_data,
        container = .container.name
    );
}

query get_blockchain_import_proposal_id(blockchain_rid: byte_array): rowid? =
        pending_blockchain_import @? { blockchain_rid } ( .proposal.rowid );

function validate_blockchain_import_proposal(proposal_details: pending_blockchain_import) {
    require(empty(blockchain @? { proposal_details.blockchain_rid }), "Blockchain with RID %s already exists".format(proposal_details.blockchain_rid));
    // Require container resource limits are honored
    require_container_is_not_full(proposal_details.container);

    val nodes = cluster_node @* { proposal_details.container.cluster } (@sort .node.pubkey);
    // do not write new configuration when size is 0 since it's impossible to recover from that
    require(nodes.size() &gt; 0, "Cluster must have at least one node");
}
</string>
                            </entry>
                            <entry key="proposal_blockchain_import/proposal_configuration_import.rell">
                                <string>entity pending_configuration_import {
    key proposal;
    blockchain;
    height: integer;
    config_data: byte_array;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.configuration_import.name: apply_configuration_import(*)];
@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.configuration_import.name: delete_pending_configuration_import(*)];

operation propose_import_configuration(my_pubkey: pubkey, blockchain_rid: byte_array, height: integer, config_data: byte_array,
                                       description: text = "Propose import configuration") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val blockchain = require_blockchain(blockchain_rid);

    val container = container_blockchain @ { blockchain }.container;
    require_container_deployer(container, me);

    require("signers" in map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(config_data)), "No signers in configuration");

    require(empty(pending_configuration_import @* { blockchain, .height == height, .proposal.state == proposal_state.PENDING } limit 1), "Already proposed");
    val prop = create_proposal(proposal_type.configuration_import, me, container.deployer, description);
    val pci = create pending_configuration_import(prop, blockchain, height, config_data);
    validate_configuration_import_proposal(pci);
    internal_vote(me, prop, true);
}

function apply_configuration_import(proposal) {
    val pci = pending_configuration_import @? { proposal };
    if (pci == null) return;
    validate_configuration_import_proposal(pci);

    add_configuration_with_signers(pci.blockchain, pci.height, pci.config_data);

    log("Configuration imported: %d / %s".format(pci.height, pci.blockchain.rid));
}

function delete_pending_configuration_import(proposal) {
    delete pending_configuration_import @? { proposal };
}

query get_configuration_import_proposal(rowid?):
    (blockchain_rid: byte_array, height: integer, config_data: byte_array)?
{
    val proposal = get_latest_proposal(rowid, proposal_type.configuration_import);
    if (proposal == null) return null;
    return pending_configuration_import @ { proposal } (
        blockchain_rid = .blockchain.rid,
        height = .height,
        config_data = .config_data
    );
}

function validate_configuration_import_proposal(proposal_details: pending_configuration_import) {

    val blockchain = proposal_details.blockchain;

    require(blockchain.state == blockchain_state.IMPORTING, "Blockchain must be in %s state to import configurations".format(blockchain_state.IMPORTING));
    require(empty(blockchain_configuration @? { blockchain, proposal_details.height } limit 1),
        "Configuration at height %d already exists for blockchain %s".format(proposal_details.height, blockchain.rid));

    val container = container_blockchain @ { blockchain }.container;
    val nodes = cluster_node @* { container.cluster } (@sort .node.pubkey);
    // do not write new configuration when size is 0 since it's impossible to recover from that
    require(nodes.size() &gt; 0, "Cluster must have at least one node");
}</string>
                            </entry>
                            <entry key="proposal_blockchain_import/proposal_finish_blockchain_import.rell">
                                <string>entity pending_finish_blockchain_import {
    key proposal;
    blockchain: blockchain;
    final_height: integer;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.finish_blockchain_import.name: apply_finish_blockchain_import(*)];
@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.finish_blockchain_import.name: delete_pending_finish_blockchain_import(*)];

operation propose_finish_import_blockchain(my_pubkey: pubkey, blockchain_rid: byte_array, final_height: integer, description: text = "Propose finish import blockchain") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val blockchain = require_blockchain(blockchain_rid);

    val container = container_blockchain @ { blockchain }.container;
    require_container_deployer(container, me);

    require(empty(pending_finish_blockchain_import @* { blockchain, .proposal.state == proposal_state.PENDING } limit 1), "Already proposed");
    val prop = create_proposal(proposal_type.finish_blockchain_import, me, container.deployer, description);
    val pfbi = create pending_finish_blockchain_import(prop, blockchain, final_height);
    validate_finish_blockchain_import_proposal(pfbi);
    internal_vote(me, prop, true);
}

function apply_finish_blockchain_import(proposal) {
    val pfbi = pending_finish_blockchain_import @? { proposal };
    if (pfbi == null) return;
    val nodes = validate_finish_blockchain_import_proposal(pfbi);

    update blockchain @ { pfbi.blockchain.rid } (.state = blockchain_state.RUNNING);
    /*
        FYI: PCU based solution can't be used here at the moment,
        since PCU doesn't support replacing all signers per one config update:

        update_configuration_signers_regular(pfbi.blockchain, nodes, null);
    */
    create blockchain_configuration_signers(pfbi.blockchain, pfbi.final_height + 1, nodes.to_gtv().to_bytes());

    log("Blockchain import finished: %s".format(pfbi.blockchain.rid));
}

function delete_pending_finish_blockchain_import(proposal) {
    delete pending_finish_blockchain_import @? { proposal };
}

query get_finish_blockchain_import_proposal(rowid?):
    (blockchain_rid: byte_array, final_height: integer)?
{
    val proposal = get_latest_proposal(rowid, proposal_type.finish_blockchain_import);
    if (proposal == null) return null;
    return pending_finish_blockchain_import @ { proposal } (
        blockchain_rid = .blockchain.rid,
        final_height = .final_height
    );
}

function validate_finish_blockchain_import_proposal(proposal_details: pending_finish_blockchain_import) {
    val container = container_blockchain @ { proposal_details.blockchain }.container;
    val nodes = cluster_node @* { container.cluster } (@sort .node.pubkey);
    require(nodes.size() &gt; 0, "Cluster must have at least one node");
    require(proposal_details.blockchain.state == blockchain_state.IMPORTING, "Blockchain must be in %s state to finish import".format(blockchain_state.IMPORTING));

    return nodes;
}
</string>
                            </entry>
                            <entry key="proposal_blockchain_import/proposal_foreign_blockchain_blocks_import.rell">
                                <string>entity pending_foreign_blockchain_blocks_import {
    key proposal;
    blockchain: blockchain;
    final_height: integer;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.foreign_blockchain_blocks_import.name: apply_foreign_blockchain_blocks_import(*)];
@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.foreign_blockchain_blocks_import.name: delete_pending_foreign_blockchain_blocks_import(*)];

operation propose_foreign_blockchain_blocks_import(
    my_pubkey: pubkey, blockchain_rid: byte_array, final_height: integer, description: text = "Propose import foreign blockchain blocks"
) {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val blockchain = require_blockchain(blockchain_rid);

    val container = container_blockchain @ { blockchain }.container;
    require_container_deployer(container, me);

    require(empty(pending_foreign_blockchain_blocks_import @* { blockchain, .proposal.state == proposal_state.PENDING } limit 1), "Already proposed");

    val prop = create_proposal(proposal_type.foreign_blockchain_blocks_import, me, container.deployer, description);
    val proposal_details = create pending_foreign_blockchain_blocks_import(prop, blockchain, final_height);
    validate_foreign_blockchain_blocks_import_proposal(proposal_details);
    internal_vote(me, prop, true);
}

function apply_foreign_blockchain_blocks_import(proposal) {
    val pfbbi = pending_foreign_blockchain_blocks_import @? { proposal };
    if (pfbbi == null) return;
    val (fbi, nodes) = validate_foreign_blockchain_blocks_import_proposal(pfbbi);

    val container = container_blockchain @ { pfbbi.blockchain }.container;

    update blockchain @ { pfbbi.blockchain.rid } (.state = blockchain_state.RUNNING);
    create blockchain_configuration_signers(pfbbi.blockchain, pfbbi.final_height + 1, nodes.to_gtv().to_bytes());
    fbi.final_height = pfbbi.final_height;
    create blockchain_configuration_options(
        pfbbi.blockchain, pfbbi.final_height + 1, suppress_special_transaction_validation = false
    );

    log("Foreign blockchain blocks import started: %s / %s".format(pfbbi.blockchain.name, pfbbi.blockchain.rid));
}

function delete_pending_foreign_blockchain_blocks_import(proposal) {
    delete pending_foreign_blockchain_blocks_import @? { proposal };
}

query get_foreign_blockchain_blocks_import_proposal(rowid?): (blockchain_rid: byte_array, final_height: integer)? {
    val proposal = get_latest_proposal(rowid, proposal_type.foreign_blockchain_blocks_import);
    if (proposal == null) return null;
    return pending_foreign_blockchain_blocks_import @ { proposal } (
        blockchain_rid = .blockchain.rid,
        final_height = .final_height
    );
}

function validate_foreign_blockchain_blocks_import_proposal(proposal_details: pending_foreign_blockchain_blocks_import) {

    val blockchain = proposal_details.blockchain;

    require(blockchain.state == blockchain_state.IMPORTING, "Blockchain must be in %s state to start importing blocks".format(blockchain_state.IMPORTING));

    val container = container_blockchain @ { proposal_details.blockchain }.container;
    val nodes = cluster_node @* { container.cluster } (@sort .node.pubkey);
    require(nodes.size() &gt; 0, "Cluster must have at least one node");
    require_height_is_greater_or_equal_to_last_config_height(proposal_details.blockchain, proposal_details.final_height);
    val fbi = require(importing_foreign_blockchain @? { .blockchain_rid == proposal_details.blockchain.rid },
        "Can't find foreign blockchain being imported: %s".format(proposal_details.blockchain.rid)
    );
    require(proposal_details.final_height != -1,
        "A final_height (%s) already proposed for foreign blockchain import: %s".format(proposal_details.final_height, proposal_details.blockchain.rid)
    );

    return (fbi, nodes);
}

</string>
                            </entry>
                            <entry key="proposal_blockchain_import/proposal_foreign_blockchain_import.rell">
                                <string>entity pending_foreign_blockchain_import {
    key proposal;
    pubkey;
    host: text;
    port: integer;
    api_url: text;
    chain0_rid: byte_array;
    blockchain_name: text;
    blockchain_rid: byte_array;
    initial_config_data: byte_array;
    container;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.foreign_blockchain_import.name: apply_foreign_blockchain_import(*)];
@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.foreign_blockchain_import.name: delete_pending_foreign_blockchain_import(*)];

operation propose_foreign_blockchain_import(
    my_pubkey: pubkey,
    node_pubkey: pubkey,
    host: text,
    port: integer,
    api_url: text,
    chain0_rid: byte_array,
    blockchain_name: text,
    blockchain_rid: byte_array,
    initial_config_data: byte_array,
    container_name: text,
    description: text = "Propose import foreign blockchain"
) {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val container = require_container(container_name);
    require_container_deployer(container, me);
    validate_entity_name(blockchain_name);
    validate_host(host);
    validate_url(api_url);

    require("signers" in map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(initial_config_data)), "No signers in configuration");
    require(empty(pending_foreign_blockchain_import @* { .blockchain_rid == blockchain_rid, .proposal.state == proposal_state.PENDING } limit 1), "Already proposed");

    val prop = create_proposal(proposal_type.foreign_blockchain_import, me, container.deployer, description);
    val pending_foreign_blockchain_import = create pending_foreign_blockchain_import(
        prop,
        pubkey = node_pubkey, host, port, api_url,
        chain0_rid,
        blockchain_name, blockchain_rid, initial_config_data,
        container);
    validate_foreign_blockchain_import_proposal(pending_foreign_blockchain_import);
    internal_vote(me, prop, true);
}

function apply_foreign_blockchain_import(proposal) {
    val pfbi = pending_foreign_blockchain_import @? { proposal };
    if (pfbi == null) return;
    validate_foreign_blockchain_import_proposal(pfbi);

    val blockchain = create blockchain(pfbi.blockchain_rid, pfbi.blockchain_name, system = false, state = blockchain_state.IMPORTING);
    create container_blockchain(pfbi.container, blockchain);
    add_configuration_with_signers(blockchain, 0, pfbi.initial_config_data);
    create importing_foreign_blockchain(
        pubkey = pfbi.pubkey,
        host = pfbi.host,
        port = pfbi.port,
        api_url = pfbi.api_url,
        chain0_rid = pfbi.chain0_rid,
        blockchain_rid = pfbi.blockchain_rid
    );
    create blockchain_configuration_options(
        blockchain, 0, suppress_special_transaction_validation = true
    );

    log("Foreign blockchain import started: %s / %s".format(pfbi.blockchain_name, pfbi.blockchain_rid));
}

function delete_pending_foreign_blockchain_import(proposal) {
    delete pending_foreign_blockchain_import @? { proposal };
}

query get_foreign_blockchain_import_proposal(rowid?) {
    val proposal = get_latest_proposal(rowid, proposal_type.blockchain_import);
    if (proposal == null) return null;
    return pending_foreign_blockchain_import @ { proposal } (
        foreign_node = .pubkey,
        host = .host,
        port = .port,
        api_url = .api_url,
        chain0_rid = .chain0_rid,
        blockchain_name = .blockchain_name,
        blockchain_rid = .blockchain_rid,
        container = .container.name
    );
}

function validate_foreign_blockchain_import_proposal(proposal_details: pending_foreign_blockchain_import) {
    require(empty(blockchain @? { proposal_details.blockchain_rid }), "Blockchain with RID %s already exists".format(proposal_details.blockchain_rid));
    // Require container resource limits are honored
    require_container_is_not_full(proposal_details.container);

    val nodes = cluster_node @* { proposal_details.container.cluster } (@sort .node.pubkey);
    require(nodes.size() &gt; 0, "Cluster must have at least one node");
}
</string>
                            </entry>
                            <entry key="proposal_blockchain_import/util/module.rell">
                                <string>module;

import model.*;
import common.*;

function add_configuration_with_signers(blockchain, height: integer, config_data: byte_array) {
    val config_map = map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(config_data));
    val signers = require(config_map.remove_or_null("signers"), "No signers in configuration");
    val base_config = config_map.to_gtv();

    compress_and_store_configuration(blockchain, height, base_config.to_bytes());
    create blockchain_configuration_signers(blockchain, height, signers.to_bytes());
    add_dependencies(config_data, blockchain.rid, height);
}
</string>
                            </entry>
                            <entry key="proposal_blockchain_move/container_blockchains_move.rell">
                                <string>import lib.icmf.*;
import messaging.anchoring_api.*;

function move_container_blockchains_impl(
    src_container_name: text,
    destination_cluster_name: text,
    anchored_blockchains: map&lt;byte_array, integer&gt;,
    destination_container_units: integer,
    destination_container_extra_storage: integer
): (container_name: text, cluster_name: text) {
    // require src container
    val src_container = require_container(src_container_name);
    val src_cluster = src_container.cluster;
    val src_deployer = src_container.deployer;

    // require dst cluster
    val dst_cluster = require_cluster(destination_cluster_name);
    val dst_nodes = cluster_node @* { dst_cluster } ( .node );
    require(dst_nodes.size() &gt; 0, "Cluster %s must have at least one node".format(dst_cluster.name));

    // require blockchains
    val bcs = container_blockchain @* { src_container } ( .blockchain );
    require(anchored_blockchains.keys().contains_all(bcs @* {} ( .rid )),
        "Last anchored height for some of cluster %s blockchains not found".format(src_cluster.name));

    // require blockchain states
    for (bc in bcs) {
        require(bc.state == blockchain_state.PAUSED,
            "Only blockchains in PAUSED states can be moved; current state of blockchain %s is: %s".format(bc.rid, bc.state)
        );
    }

    // require blockchain is not being moved right now
    for (bc in bcs) {
        require(empty(moving_blockchain @? { bc.rid }), "Blockchain %s is being moved".format(bc.rid));
    }

    // require there no pending blockchain actions (pause, resume, remove, archive, unarchive)
    for (bc in bcs) {
        val pba = pending_blockchain_action @? { bc.rid };
        require(empty(pba), "Blockchain %s cannot be moved due to a pending proposal %s related to it".format(bc.rid, pba?.proposal));
    }

    // require there are no pending blockchains to be added to container
    val pb = pending_blockchain @? { src_container };
    require(empty(pb), "Container bcs cannot be moved due to a pending proposal %s related to it".format(pb?.proposal));

    // create dst container
    val new_name = src_container.name + "_new";
    val new_deployers = voter_set_member @* { src_container.deployer } (.provider.pubkey);
    val dst_container = create_container_impl(
        src_container.proposed_by, new_name, dst_cluster,
        src_container.deployer.threshold, new_deployers,
        destination_container_units,
        standard_container_defaults.max_blockchains,
        destination_container_extra_storage
    );

    // moving
    val src_nodes = cluster_node @* { .cluster == src_cluster }.node;
    for (bc in bcs) {
        val final_height = anchored_blockchains[bc.rid];
        create moving_blockchain(bc, source = src_container, destination = dst_container, final_height/*, remove_on_nodes*/);

        for (node in src_nodes) {
            if (not exists(blockchain_replica_node @? { bc, node })) create blockchain_replica_node(bc, node);
        }
        // remove bc from src_container
        delete container_blockchain @* { bc };
        // add bc to the dst_container
        create container_blockchain(dst_container, bc);
        // remove dst cluster/container nodes from bc replicas if any exists
        for (node in dst_nodes) {
            delete blockchain_replica_node @* { bc, node };
        }
        // FYI: PCU-based update (i.e. update_configuration_signers_regular()) can't be used here.
        require_height_is_greater_or_equal_to_last_config_height(bc, final_height);
        // base config
        val base_config = require(get_blockchain_configuration(bc.rid, final_height)?.base_config,
            "Can't find config for %s for block %s".format(bc.rid, final_height));
        compress_and_store_configuration(bc, final_height + 1, make_config_unique(base_config));
        // signers config
        val encoded_dst_nodes = (dst_nodes @* {} ( @sort .pubkey )).to_gtv().to_bytes();
        create blockchain_configuration_signers(bc, final_height + 1, encoded_dst_nodes);
    }

    if (bcs.empty()) {
        remove_container_impl(src_container);
    } else {
        src_container.state = container_state.MIGRATING;
    }

    return (container_name = dst_container.name, cluster_name = dst_cluster.name);
}
</string>
                            </entry>
                            <entry key="proposal_blockchain_move/module.rell">
                                <string>module;

import common.*;
import proposal.*;
import proposal_blockchain.*;
import model.*;

entity remove_moved_blockchain_on_original_node {
    key blockchain, node;
}

@extend(on_configuration_updated)
function try_to_finalize_blockchain_move(message: configuration_updated) {
    val moving_bc = moving_blockchain @? { .blockchain.rid == message.blockchain_rid, .final_height == message.height - 1 };
    if (exists(moving_bc)) {
        val src_nodes = cluster_node @* { moving_bc.source.cluster } ( .node );
        for (node in src_nodes) {
            delete blockchain_replica_node @? { moving_bc.blockchain, node };
            create remove_moved_blockchain_on_original_node(moving_bc.blockchain, node);
        }
        if (not src_nodes.empty()) {
            log("blockchain_replica_node removed: %s".format(src_nodes @* {} ( .pubkey )));
        }

        log("Blockchain %s moving from container %s to container %s was finished at height %s".format(
            moving_bc.blockchain.rid, moving_bc.source.name, moving_bc.destination.name, moving_bc.final_height)
        );

        val src_container = moving_bc.source;
        delete moving_bc;

        // If we are migrating all bcs in source container and they all have finished then we can now delete it
        if (src_container.state == container_state.MIGRATING and (moving_blockchain @* { .source == src_container }).empty()) {
            remove_container_impl(src_container);
        }
    }
}
</string>
                            </entry>
                            <entry key="proposal_blockchain_move/proposal_blockchain_move.rell">
                                <string>import common.*;

entity pending_blockchain_move {
    key proposal;
    blockchain;
    destination: container;
}

operation propose_blockchain_move(my_pubkey: pubkey, blockchain_rid: byte_array, destination_container: text, description: text = "") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val blockchain = require(blockchain @? { blockchain_rid }, "Unknown blockchain " + blockchain_rid);
    // require blockchain is not system
    require(not(blockchain.system), "Moving of system chains is not allowed: " + blockchain_rid);

    val dst_container = validate_blockchain_move_proposal(blockchain, me, destination_container);

    val prop = create_proposal(proposal_type.blockchain_move_start, me, dst_container.deployer, description);
    val pending_blockchain_move = create pending_blockchain_move(prop, blockchain, dst_container);
    internal_vote(me, prop, true);
}

query get_blockchain_move_proposal(rowid?):
        (blockchain_rid: byte_array, blockchain_name: text, container: text, cluster: text)? {
    val proposal = get_latest_proposal(rowid, proposal_type.blockchain_import);
    if (proposal == null) return null;
    return pending_blockchain_move @ { proposal } (
        blockchain_rid = .blockchain.rid,
        blockchain_name = .blockchain.name,
        container = .destination.name,
        cluster = .destination.cluster.name
    );
}

@extend(apply_voting_result_handlers) function() = [proposal_type.blockchain_move_start.name: apply_blockchain_move(*)];
@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.blockchain_move_start.name: delete_pending_blockchain_move(*)];

function apply_blockchain_move(proposal) {
    val pending_blockchain_move = pending_blockchain_move @ { proposal };
    val pbm = pending_blockchain_move.to_struct();
    validate_blockchain_move_proposal(
        pending_blockchain_move.blockchain,
        pending_blockchain_move.proposal.proposed_by,
        pending_blockchain_move.destination.name);

    // keep src cluster/container nodes as replicas
    val src_container = container_blockchain @ { pbm.blockchain } (.container);
    val src_cluster_nodes_to_replicate_on = cluster_node @* {
        src_container.cluster, .node not in blockchain_replica_node @* { pbm.blockchain } (.node)
    } (.node);
    for (node in src_cluster_nodes_to_replicate_on) {
        create blockchain_replica_node(pbm.blockchain, node);
    }
    // remove bc from container
    delete container_blockchain @* { pbm.blockchain };

    // add bc to the dst container
    create container_blockchain(pbm.destination, pbm.blockchain);
    // remove dst cluster/container nodes from bc replicas
    val dst_nodes = cluster_node @* { pbm.destination.cluster } (@omit @sort .node.pubkey, .node);
    require(dst_nodes.size() &gt; 0, "Cluster %s must have at least one node".format(pbm.destination.cluster.name));
    for (node in dst_nodes) {
        delete blockchain_replica_node @* { pbm.blockchain, node };
    }

    create moving_blockchain(pbm.blockchain, source = src_container, destination = pbm.destination);
}

function delete_pending_blockchain_move(proposal) {
    delete pending_blockchain_move @? { proposal };
}

function validate_blockchain_move_proposal(blockchain, proposed_by: provider, destination_container: text) {

    // require blockchain state
    require(blockchain.state in [blockchain_state.RUNNING, blockchain_state.PAUSED],
        "Only blockchains in RUNNING | PAUSED states can be moved; current state of blockchain %s is: %s".format(blockchain.rid, blockchain.state)
    );
    
    // require blockchain is not being moved right now
    require(empty(moving_blockchain @? { blockchain }), "Blockchain %s is being moved".format(blockchain.rid));

    // require there no pending blockchain actions (pause, resume, remove, archive, unarchive)
    val pba = pending_blockchain_action @? { blockchain };
    require(empty(pba), "Blockchain %s cannot be moved due to a pending proposal %s related to it".format(blockchain.rid, pba?.proposal));

    // src container
    val src_container = container_blockchain @ { blockchain } .container;
    require_container_deployer(src_container, proposed_by);

    // dst container
    val dst_container = require_container(destination_container);
    require_container_deployer(dst_container, proposed_by);
    require_container_is_not_full(dst_container);

    require(
        get_voter_set_members_sorted(src_container.deployer.name) == get_voter_set_members_sorted(dst_container.deployer.name),
        "Deployers of source and destination container must be identical");

    return dst_container;
}
</string>
                            </entry>
                            <entry key="proposal_blockchain_move/proposal_blockchain_move_finish.rell">
                                <string>import common.*;

entity pending_blockchain_move_finish {
    key proposal;
    blockchain;
    destination: container;
    final_height: integer;
}

operation propose_blockchain_move_finish(my_pubkey: pubkey, blockchain_rid: byte_array, final_height: integer, description: text = "") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val blockchain = require_active_blockchain(blockchain_rid);
    val mv_bc = validate_blockchain_move_finish_proposal(blockchain);

    val prop = create_proposal(proposal_type.blockchain_move_finish, me, mv_bc.destination.deployer, description);
    create pending_blockchain_move_finish(prop, blockchain, mv_bc.destination, final_height);
    internal_vote(me, prop, true);
}

query get_blockchain_move_finish_proposal(rowid?):
        (blockchain_rid: byte_array, blockchain_name: text, cluster: text, container: text, final_height: integer)? {
    val proposal = get_latest_proposal(rowid, proposal_type.blockchain_import);
    if (proposal == null) return null;
    return pending_blockchain_move_finish @ { proposal } (
        blockchain_rid = .blockchain.rid,
        blockchain_name = .blockchain.name,
        cluster = .destination.cluster.name,
        container = .destination.name,
        final_height = .final_height
    );
}

@extend(apply_voting_result_handlers) function() = [proposal_type.blockchain_move_finish.name: apply_blockchain_move_finish(*)];
@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.blockchain_move_finish.name: delete_pending_blockchain_move_finish(*)];

function apply_blockchain_move_finish(proposal) {
    val pbmf = pending_blockchain_move_finish @ { proposal } ( $.to_struct() );
    val mv_bc = validate_blockchain_move_finish_proposal(pbmf.blockchain);

    // create a base_config and signers config at final_height (base_config needed if signers don't change)
    // FYI: PCU-based update (i.e. update_configuration_signers_regular()) can't be used here.
    // base config
    val base_config = require(get_blockchain_configuration(pbmf.blockchain.rid, pbmf.final_height)?.base_config,
        "Can't find config for %s for block %s".format(pbmf.blockchain.rid, pbmf.final_height));
    compress_and_store_configuration(pbmf.blockchain, pbmf.final_height + 1, make_config_unique(base_config));
    // signers config
    val encoded_dst_nodes = (cluster_node @* { pbmf.destination.cluster } (@sort .node.pubkey)).to_gtv().to_bytes();
    create blockchain_configuration_signers(pbmf.blockchain, pbmf.final_height + 1, encoded_dst_nodes);

    // update moving_blockchain state
    mv_bc.final_height = pbmf.final_height;
    mv_bc.remove_on_nodes = (cluster_node @* { mv_bc.source.cluster } (@sort .node.pubkey )).to_gtv().to_bytes();
}

function delete_pending_blockchain_move_finish(proposal) {
    delete pending_blockchain_move_finish @? { proposal };
}

function validate_blockchain_move_finish_proposal(blockchain) {

    val proposal_details = require(moving_blockchain @? { blockchain }, "Blockchain %s is not being moved".format(blockchain.rid));

    require(
        get_voter_set_members_sorted(proposal_details.source.deployer.name) == get_voter_set_members_sorted(proposal_details.destination.deployer.name),
        "Deployers of source and destination container must be identical");

    return proposal_details;
}
</string>
                            </entry>
                            <entry key="proposal_cluster/module.rell">
                                <string>module;

import common.*;
import model.*;
import proposal.*;</string>
                            </entry>
                            <entry key="proposal_cluster/proposal_cluster_limits.rell">
                                <string>entity pending_cluster_limits {
    key proposal;
    key cluster;
    cluster_units: integer;
    extra_storage: integer;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.cluster_limits.name: apply_cluster_limits(*)];

function apply_cluster_limits(proposal) {
    val pps = pending_cluster_limits @? { proposal };
    if (pps == null) return;
    validate_cluster_limits_proposal(pps);
    pps.cluster.cluster_units = pps.cluster_units;
    pps.cluster.extra_storage = pps.extra_storage;
    after_cluster_updated(pps.cluster);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.cluster_limits.name: delete_pending_cluster_limits(*)];

function delete_pending_cluster_limits(proposal) {
    delete pending_cluster_limits @? { proposal };
}

// The operation is mostly needed to make rell-maven-plugin generate code for enum `cluster_resource_limit_type`
operation propose_cluster_limits(my_pubkey: pubkey, cluster_name: text, cluster_units: integer? = null, extra_storage: integer? = null, description: text = "") {
    val me = require_provider(my_pubkey);
    // check that provider authority and that it is a cluster governor
    require_provider_auth_with_rate_limit(me);
    val cluster = require_cluster(cluster_name);
    require_cluster_governor(cluster, me);
    val prop = create_proposal(proposal_type.cluster_limits, me, cluster.governance, description);
    val pending_cluster_limits = create pending_cluster_limits(
        prop,
        cluster,
        cluster_units = cluster_units ?: cluster.cluster_units,
        extra_storage = extra_storage ?: cluster.extra_storage
    );
    validate_cluster_limits_proposal(pending_cluster_limits);
    internal_vote(me, prop, true);
}

query get_cluster_limits_proposal(rowid) {
    val proposal = get_latest_proposal(rowid, proposal_type.cluster_limits);
    if (proposal == null) return null;
    val pcl = pending_cluster_limits @ { proposal };
    return (
        cluster = pcl.cluster.name,
        cluster_units = pcl.cluster_units,
        extra_storage = pcl.extra_storage
    );
}

function validate_cluster_limits_proposal(proposal_details: pending_cluster_limits) {

    _require_cluster_units(proposal_details.cluster, proposal_details.cluster_units);
    _require_extra_storage(proposal_details.cluster, proposal_details.extra_storage);
}

function _require_cluster_units(cluster, new_cluster_units: integer) {
    require(new_cluster_units &gt; 0, "Cluster must consist of at least 1 cluster_unit");
    val minimum_cluster_units = get_minimum_cluster_units_for_current_container_units(cluster);
    require(new_cluster_units &gt;= minimum_cluster_units,
        "Can't propose cluster limits since cluster_units is too low for current usage of containers. Minimum cluster_units is %d".format(minimum_cluster_units));
    _require_new_cluster_units_for_current_nodes(cluster, new_cluster_units);
}

function _require_new_cluster_units_for_current_nodes(cluster, new_cluster_units: integer) {
    val needed_cluster_units = new_cluster_units - cluster.cluster_units;
    if (needed_cluster_units &gt; 0) {
        val nodes = set&lt;node&gt;();
        nodes.add_all(cluster_node @* { cluster }.node);
        nodes.add_all(cluster_replica_node @* { cluster }.node);
        val too_small_nodes = set&lt;pubkey&gt;();
        for (node in nodes) {
            if (get_available_cluster_units_for_node(node) &lt; needed_cluster_units) too_small_nodes.add(node.pubkey);
        }
        require(empty(too_small_nodes),
            "Can't propose cluster limits since nodes %s does not have room for another %d cluster_units".format(too_small_nodes.to_text(), needed_cluster_units));
    }
}

function _require_extra_storage(cluster, new_extra_storage: integer) {
    require(new_extra_storage &gt;= 0, "Extra storage must not be negative");
    val minimum_extra_storage = get_used_extra_storage_for_cluster(cluster);
    require(new_extra_storage &gt;= minimum_extra_storage,
        "Can't propose extra storage since extra_storage is too low for current usage of containers. Minimum extra_storage is %d".format(minimum_extra_storage));
    _require_new_extra_storage_for_current_nodes(cluster, new_extra_storage);
}

function _require_new_extra_storage_for_current_nodes(cluster, new_extra_storage: integer) {
    val needed_extra_storage = new_extra_storage - cluster.extra_storage;
    if (needed_extra_storage &gt; 0) {
        val nodes = set&lt;node&gt;();
        nodes.add_all(cluster_node @* { cluster }.node);
        nodes.add_all(cluster_replica_node @* { cluster }.node);
        val too_small_nodes = set&lt;pubkey&gt;();
        for (node in nodes) {
            if (get_available_extra_storage_for_node(node) &lt; needed_extra_storage) too_small_nodes.add(node.pubkey);
        }
        require(empty(too_small_nodes),
            "Can't propose extra storage since nodes %s does not have room for another %d MiB".format(too_small_nodes.to_text(), needed_extra_storage));
    }
}</string>
                            </entry>
                            <entry key="proposal_cluster/proposal_cluster_provider.rell">
                                <string>entity pending_cluster_provider {
    key proposal;
    key cluster;
    provider;
    add: boolean;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.cluster_provider.name: apply_cluster_provider(*)];

function apply_cluster_provider(proposal) {
    val pps = pending_cluster_provider @? { proposal };
    if (pps == null) return;
    validate_cluster_provider_proposal(pps.provider, pps.add, pps.cluster);
    if (pps.add) {
        create cluster_provider(pps.cluster, pps.provider);
    } else {
        delete cluster_provider @ {pps.cluster, pps.provider};
        check_operational(pps.cluster);
    }
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.cluster_provider.name: delete_pending_cluster_provider(*)];

function delete_pending_cluster_provider(proposal) {
    delete pending_cluster_provider @? { proposal };
}

operation propose_cluster_provider(my_pubkey: pubkey, cluster_name: text, provider_pubkey: pubkey, add: boolean, description: text = "") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    val provider = require_provider(provider_pubkey);
    // Check that provider authority and that it is a cluster governor
    val cluster = require_cluster(cluster_name);
    require(cluster.name != clusters.system, "Cannot add provider to system cluster manually");
    require_cluster_governor(cluster, me);

    validate_cluster_provider_proposal(provider, add, cluster);
    val prop = create_proposal(proposal_type.cluster_provider, me, cluster.governance, description);
    val pending_cluster_provider = create pending_cluster_provider(prop, cluster, provider, add);
    internal_vote(me, prop, true);
}

query get_cluster_provider_proposal(rowid?) {
    val proposal = get_latest_proposal(rowid, proposal_type.cluster_provider);
    if (proposal == null) return null;
    val pcp = pending_cluster_provider @ { proposal };
    return (
        cluster = pcp.cluster.name,
        provider = pcp.provider.pubkey,
        add = pcp.add
    );
}

function validate_cluster_provider_proposal(provider, add: boolean, cluster) {
    require_node_access(provider);
    if (add) {
        require(
            empty(cluster_provider @? { cluster, provider }),
            "%s is already a provider in cluster %s".format(provider.pubkey, cluster.name)
        );
    } else {
        require(
            exists(cluster_provider @? { cluster, provider }),
            "%s is not a provider in cluster %s".format(provider.pubkey, cluster.name)
        );
    }
}
</string>
                            </entry>
                            <entry key="proposal_cluster/proposal_cluster_remove.rell">
                                <string>entity pending_remove_cluster {
    key proposal;
    key cluster;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.cluster_remove.name: apply_remove_cluster(*)];

function apply_remove_cluster(proposal) {
    val cl = pending_remove_cluster @? { proposal } .cluster;
    if (cl == null) return;
    delete pending_remove_cluster @ { proposal };
    remove_cluster_impl(cl);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.cluster_remove.name: delete_pending_cluster_remove(*)];

function delete_pending_cluster_remove(proposal) {
    delete pending_remove_cluster @? { proposal };
}

operation propose_remove_cluster(my_pubkey: pubkey, name, description: text = "") {
    val me = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(me);
    val c = require(cluster @? { name }, "Unknown cluster %s".format(name));
    require_cluster_governor(c, me);
    require_cluster_available_for_removal(c);

    val prop = create_proposal(proposal_type.cluster_remove, me, c.governance, description);
    create pending_remove_cluster(prop, c);
    internal_vote(me, prop, true);
}

query get_cluster_remove_proposal(rowid) {
    val proposal = get_latest_proposal(rowid, proposal_type.cluster_remove);
    if (proposal == null) return null;
    return pending_remove_cluster @ { proposal } .cluster.name;
}</string>
                            </entry>
                            <entry key="proposal_cluster_anchoring/module.rell">
                                <string>module;

import proposal.*;
import model.*;
</string>
                            </entry>
                            <entry key="proposal_cluster_anchoring/proposal_cluster_anchoring.rell">
                                <string>entity pending_cluster_anchoring_configuration {
    key proposal;
    data: byte_array;  // map&lt;string, gtv&gt;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.cluster_anchoring_configuration.name: apply_cluster_anchoring_configuration(*)];

function apply_cluster_anchoring_configuration(proposal) {
    val pcac = pending_cluster_anchoring_configuration @? { proposal };
    if (pcac == null) return;
    set_cluster_anchoring_config(pcac.data);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.cluster_anchoring_configuration.name: delete_pending_cluster_anchoring_configuration(*)];

function delete_pending_cluster_anchoring_configuration(proposal) {
    delete pending_cluster_anchoring_configuration @? { proposal };
}

operation propose_cluster_anchoring_configuration(my_pubkey: pubkey, config_data: byte_array) {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    require_is_system_provider(my_pubkey);

    val new_config = map&lt;text, gtv&gt;.from_gtv(gtv.from_bytes(config_data)); // Validate that config is a map
    require(new_config.size() &gt; 0, "Configuration must not be empty");

    val prop = create_proposal(proposal_type.cluster_anchoring_configuration, me, system_p_voter_set());
    create pending_cluster_anchoring_configuration(prop, config_data);
    validate_cluster_anchoring_proposal();
    internal_vote(me, prop, true);
}

query get_cluster_anchoring_configuration_proposal(rowid?) {
    val proposal = get_latest_proposal(rowid, proposal_type.cluster_anchoring_configuration);
    if (proposal == null) return null;
    val pcac = decompress_configuration(pending_cluster_anchoring_configuration @ { proposal }.data);
    val current_conf = decompress_configuration(cluster_anchoring_config.raw_config);
    return (
        current_conf = current_conf,
        proposed_conf = pcac
    );
}

function validate_cluster_anchoring_proposal() {
    val current_config = map&lt;text,gtv&gt;.from_gtv(gtv.from_bytes(cluster_anchoring_config.raw_config));
    require(current_config.size() &gt; 0, "Cluster anchoring is disabled");
}</string>
                            </entry>
                            <entry key="proposal_container/module.rell">
                                <string>module;

import common.*;
import model.*;
import proposal.*;
import .proposal_container_limits.*;</string>
                            </entry>
                            <entry key="proposal_container/proposal_container.rell">
                                <string>entity pending_container {
    key proposal;
    key name;
    cluster;
    deployer: voter_set;
    container_units: integer;
    max_blockchains: integer;
    extra_storage: integer;
    proposed_by: provider;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.container.name: apply_container(*)];

function apply_container(proposal) {
    val pps = pending_container @? { proposal };
    if (pps == null) return;
    validate_container_proposal(pps);
    create_container_with_limits(
        pps.proposed_by,
        pps.name,
        pps.cluster,
        pps.deployer,
        pps.container_units,
        pps.max_blockchains,
        pps.extra_storage
    );
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.container.name: delete_pending_container(*)];

function delete_pending_container(proposal) {
    delete pending_container @? { proposal };
}

// Who can create a new container? Cluster deployers' voter set. (They can also update container limits.)
operation propose_container(my_pubkey: pubkey, cluster_name: text, name, deployer_name: text, description: text = "") {
    // check that provider authority and that it is cluster's deployer
    val me = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(me);
    val cluster = require_cluster(cluster_name);
    val deployer = require_voter_set(deployer_name);
    require_cluster_governor(cluster, me);

    validate_entity_name(name);

    val container_units = standard_container_defaults.container_units;
    val max_blockchains = standard_container_defaults.max_blockchains;
    val extra_storage = standard_container_defaults.extra_storage;

    val prop = create_proposal(proposal_type.container, me, cluster.governance, description);
    val pending_container = create pending_container(
        prop, name, cluster, deployer, container_units, max_blockchains, extra_storage, me
    );
    validate_container_proposal(pending_container);
    internal_vote(me, prop, true);
}

query get_container_proposal(rowid) {
    val proposal = get_latest_proposal(rowid, proposal_type.container);
    if (proposal == null) return null;
    val pcwl = pending_container @ { proposal };
    return (
        container = pcwl.name,
        container_units = pcwl.container_units,
        max_blockchains = pcwl.max_blockchains,
        extra_storage = pcwl.extra_storage
    );
}

function validate_container_proposal(pending_container) {
    require(empty(container @? { pending_container.name }), "Container with name %s already exists".format(pending_container.name));
    require_provider_quota(pending_container.proposed_by, provider_quota_type.max_containers);
    require_cluster_quotas(pending_container.cluster, pending_container.container_units, pending_container.extra_storage);
}

</string>
                            </entry>
                            <entry key="proposal_container/proposal_container_limits.rell">
                                <string>module;

import ^.*;
import ^^.model.*;

entity pending_container_limits {
    key proposal;
    key container;
    container_units: integer;
    max_blockchains: integer;
    extra_storage: integer;
}

@extend(is_container_available_for_removal) function(container) = 
if (exists(pending_container_limits @* { container })) 
    "Container %s has pending proposals and can't be deleted. Resolve proposals first".format(container.name) 
else null;

@extend(apply_voting_result_handlers) function() = [proposal_type.container_limits.name: apply_container_limits(*)];

function apply_container_limits(proposal) {
    val pps = pending_container_limits @? { proposal };
    if (pps == null) return;
    validate_container_limits_proposal(pps);
    upgrade_container(pps.container, pps.container_units, pps.extra_storage, pps.max_blockchains);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.container_limits.name: delete_pending_container_limits(*)];

function delete_pending_container_limits(proposal) {
    delete pending_container_limits @? { proposal };
}

operation propose_container_limits(my_pubkey: pubkey, container_name: text, limits: map&lt;container_resource_limit_type, integer&gt;, description: text = "") {
    if (limits.contains(container_resource_limit_type.container_units)) {
        val proposed_container_units = limits[container_resource_limit_type.container_units];
        require(proposed_container_units &gt; 0 or proposed_container_units == -1, "Invalid value for container units: %d, must be -1 or greater than 0".format(proposed_container_units));
    }
    if (limits.contains(container_resource_limit_type.extra_storage)) {
        val proposed_extra_storage = limits[container_resource_limit_type.extra_storage];
        require(proposed_extra_storage &gt;= 0, "Invalid value for extra_storage: %d, must be greater or equal to 0".format(proposed_extra_storage));
    }

    val limits_map = get_current_container_resource_limits(container_name);
    limits_map.put_all(limits);

    // check that provider authority and that it is cluster's deployer
    val me = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(me);
    val container = require_container(container_name);
    require_cluster_governor(container.cluster, me);
    
    val prop = create_proposal(proposal_type.container_limits, me, container.cluster.governance, description);
    val pcl = create pending_container_limits(
        prop,
        container,
        container_units = limits_map[container_resource_limit_type.container_units],
        max_blockchains = limits_map[container_resource_limit_type.max_blockchains],
        extra_storage = limits_map[container_resource_limit_type.extra_storage]
    );
    validate_container_limits_proposal(pcl);
    internal_vote(me, prop, true);
}

query get_container_limits_proposal(rowid) {
    val proposal = get_latest_proposal(rowid, proposal_type.container_limits);
    if (proposal == null) return null;
    val pcl = pending_container_limits @ { proposal };
    return (
        container = pcl.container.name,
        container_units = pcl.container_units,
        max_blockchains = pcl.max_blockchains,
        extra_storage = pcl.extra_storage
    );
}

@extend(before_remove_container) function remove_all_container_proposals(container) {
    val pcls = pending_container_limits @* { container };
    for (pcl in pcls) {
        pcl.proposal.state = proposal_state.REVOKED;
        delete pcl;
    }
}

function validate_container_limits_proposal(proposal_details: pending_container_limits) {

    require_container_units(proposal_details.container, proposal_details.container_units);
    require_max_blockchains(proposal_details.container, proposal_details.max_blockchains);
    require_extra_storage(proposal_details.container, proposal_details.extra_storage);
}
</string>
                            </entry>
                            <entry key="proposal_container/proposal_remove_container.rell">
                                <string>entity pending_remove_container {
    key proposal;
    key container;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.container_remove.name: apply_container_remove(*)];

function apply_container_remove(proposal) {
    val cont = pending_remove_container @? { proposal } .container;
    if (cont == null) return;
    delete pending_remove_container @ { proposal };
    remove_container_impl(cont);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.container_remove.name: delete_pending_remove_container(*)];

function delete_pending_remove_container(proposal) {
    delete pending_remove_container @? { proposal };
}

operation propose_remove_container(my_pubkey: pubkey, name, description: text = "") {
    val me = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(me);
    val cont = require_container(name);
    require_cluster_governor(cont.cluster, me);
    require_container_available_for_removal(cont);

    val prop = create_proposal(proposal_type.container_remove, me, cont.cluster.governance, description);
    create pending_remove_container(prop, cont);
    internal_vote(me, prop, true);
}

query get_container_remove_proposal(rowid?) {
    val proposal = get_latest_proposal(rowid, proposal_type.container_remove);
    if (proposal == null) return null;
    return pending_remove_container @ { proposal } .container.name;
}

@extend(before_remove_container) function remove_all_container_proposals(container) {
    val prcs = pending_remove_container @* { container };
    for (prc in prcs) {
        prc.proposal.state = proposal_state.REVOKED;
        delete prc;
    }
}
</string>
                            </entry>
                            <entry key="proposal_provider/module.rell">
                                <string>module;

import proposal.*;
import model.*;</string>
                            </entry>
                            <entry key="proposal_provider/proposal_provider_batch.rell">
                                <string>entity pending_provider_batch {
    key proposal;
    provider_infos: byte_array; // list&lt;provider_info&gt;.to_gtv().to_bytes()
    provider_tier;
    system: boolean;
    active: boolean;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.provider_batch.name: apply_provider_batch(*)];

function apply_provider_batch(proposal) {
    val ppb = pending_provider_batch @? { proposal };
    if (ppb == null) return;
    val providers = list&lt;provider_info&gt;.from_gtv(gtv.from_bytes(ppb.provider_infos));
    for (pi in providers) {
        if (empty(provider @? { pi.pubkey })) {
            register_and_enable_provider(pi, ppb.provider_tier, null, null, ppb.active);
            val provider = provider @ { pi.pubkey };
            when {
                ppb.system -&gt; {
                    enroll.system(provider);
                    if (not(ppb.active)) {
                        update_provider_state(provider, false);
                    } else {
                        after_provider_updated(provider);
                    }
                }
                _is_node_provider(ppb.provider_tier) -&gt; {
                    enroll.node(provider @ { pi.pubkey });
                    after_provider_updated(provider);
                }
            }
        } else {
            log("Warning: provider already exists: " + pi.pubkey);
        }
    }
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.provider_batch.name: delete_pending_provider_batch(*)];

function delete_pending_provider_batch(proposal) {
    delete pending_provider_batch @? { proposal };
}

operation propose_providers(my_pubkey: pubkey, provider_infos: list&lt;provider_info&gt;, tier: provider_tier, system: boolean, active: boolean, description: text = "") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    require_is_system_provider(my_pubkey);
    if (system) {
        require(tier == provider_tier.NODE_PROVIDER, "Only NODE_PROVIDER can be marked as system provider");
    }
    require(not(empty(provider_infos)), "Proposed provider key list is empty");

    val providers_map = map&lt;pubkey, provider_info&gt;();
    for (pi in provider_infos) {
        providers_map[pi.pubkey] = pi;
    }
    val providers = providers_map.values();
    for (p in providers) {
        require_pubkey(p.pubkey);
        require(empty(provider @? { p.pubkey }), "Provider already exists: " + p.pubkey);
        validate_metadata_text("name", p.name);
        validate_url(p.url);
    }

    val already_proposed = set&lt;pubkey&gt;();
    for (ppb in pending_provider_batch @* {}) {
        already_proposed.add_all(
            list&lt;provider_info&gt;.from_gtv(gtv.from_bytes(ppb.provider_infos)) @* {} ( .pubkey )
        );
    }
    for (p in providers) {
        require(not(p.pubkey in already_proposed), "Provider is already proposed: " + p.pubkey);
    }

    val prop = create_proposal(proposal_type.provider_batch, me, system_p_voter_set(), description);
    create pending_provider_batch(prop, providers.to_gtv().to_bytes(), tier, system, active);
    internal_vote(me, prop, true);
}

query get_provider_batch_proposal(rowid) {
    val proposal = get_latest_proposal(rowid, proposal_type.provider_batch);
    if (proposal == null) return null;
    val ppb = pending_provider_batch @ { proposal };
    return (
        provider_infos = list&lt;provider_info&gt;.from_gtv(gtv.from_bytes(ppb.provider_infos)),
        tier = ppb.provider_tier,
        system = ppb.system,
        active = ppb.active
    );
}</string>
                            </entry>
                            <entry key="proposal_provider/proposal_provider_is_system.rell">
                                <string>// Proposed promotion/demotion of provider to/from being system provider
entity pending_provider_is_system {
    key proposal;
    provider;
    system: boolean;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.provider_is_system.name: apply_provider_is_system(*)];

// For promotion and demotion of system providers
function apply_provider_is_system(proposal) {
    val pps = pending_provider_is_system @? { proposal };
    if (pps == null) return;
    // If promotion, update SYSTEM_P voter set
    if (pps.system) {
        enroll.system(pps.provider);
    } else {
        revoke.system(pps.provider);
    }
    after_provider_updated(pps.provider);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.provider_is_system.name: delete_pending_provider_is_system(*)];

function delete_pending_provider_is_system(proposal) {
    delete pending_provider_is_system @? { proposal };
}

operation propose_provider_is_system(my_pubkey: pubkey, provider_pubkey: pubkey, promote: boolean, description: text = "") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    require_is_system_provider(my_pubkey);
    val other_prov = require_provider(provider_pubkey);

    require(empty(pending_provider_is_system @* { .provider == other_prov, .system == promote, .proposal.state == proposal_state.PENDING } limit 1), "Already proposed");
    val prop = create_proposal(proposal_type.provider_is_system, me, system_p_voter_set(), description);
    create pending_provider_is_system(prop, other_prov, .system = promote);
    internal_vote(me, prop, true);
}

query get_system_provider_proposal(rowid) {
    val proposal = get_latest_proposal(rowid, proposal_type.provider_is_system);
    if (proposal == null) return null;
    val pis = pending_provider_is_system @ { proposal };
    return (
        provider = pis.provider.pubkey,
        add = pis.system
    );
}</string>
                            </entry>
                            <entry key="proposal_provider/proposal_provider_quota.rell">
                                <string>entity pending_provider_quota {
    key proposal;
    provider_tier;
    provider_quota_type;
    value: integer;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.provider_quota.name: apply_provider_quota(*)];

function apply_provider_quota(proposal) {
    val ppq = pending_provider_quota @? { proposal };
    if (ppq == null) return;
    update provider_quota @? { ppq.provider_tier, ppq.provider_quota_type } (ppq.value);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.provider_quota.name: delete_pending_provider_quota(*)];

function delete_pending_provider_quota(proposal) {
    delete pending_provider_quota @? { proposal };
}

operation propose_provider_quota(my_pubkey: pubkey, tier: provider_tier, provider_quota_type, value: integer, description: text = "") {
    val me = require_is_provider_with_rate_limit(my_pubkey);
    require_is_system_provider(my_pubkey);

    // quota value requirements
    when (provider_quota_type) {
        max_actions_per_day -&gt; require(value &gt; 0, "Proposed max_actions_per_day quota value must be &gt; 0: " + value);
        max_nodes -&gt; {
            require(value &gt; 0, "Proposed max_nodes quota value must be &gt; 0: " + value);
        }
        max_containers -&gt; {
            require(value &gt;= -1, "Proposed max_containers quota value must be &gt;= -1: " + value);
            require(tier != provider_tier.DAPP_PROVIDER, "Proposing max_containers quota is not allowed for DAPP_PROVIDER");
        }
    }

    val prop = create_proposal(proposal_type.provider_quota, me, system_p_voter_set(), description);
    create pending_provider_quota(prop, tier, provider_quota_type, value);
    internal_vote(me, prop, true);
}

query get_provider_quota_proposal(rowid) {
    val proposal = get_latest_proposal(rowid, proposal_type.provider_quota);
    if (proposal == null) return null;
    val ppq = pending_provider_quota @ { proposal };
    return (
        tier = ppq.provider_tier,
        quota_type = ppq.provider_quota_type,
        value = ppq.value
    );
}</string>
                            </entry>
                            <entry key="proposal_provider/proposal_provider_state.rell">
                                <string>// Proposed enabling/disabling of providers are put here while waiting for enough positive votes.
entity pending_provider_state {
    key proposal;
    provider;
    active: boolean;
}

@extend(apply_voting_result_handlers) function() = [proposal_type.provider_state.name: apply_provider_state(*)];

// For both enabling and disabling of providers:
function apply_provider_state(proposal) {
    val pps = pending_provider_state @? { proposal };
    if (pps == null) return;
    update_provider_state(pps.provider, pps.active);
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.provider_state.name: delete_pending_provider_state(*)];

function delete_pending_provider_state(proposal) {
    delete pending_provider_state @? { proposal };
}

/*
    The provider with type in line can (+) / can not (-) change the state of the other one in the column:

         | DP | NP  | SP
    -----|-----|-----|-----
     DP |  -  |  -  |  -
     NP  |  +  |  -  |  -
     SP  |  +  |  +  |  +
*/
operation propose_provider_state(my_pubkey: pubkey, provider_pubkey: pubkey, active: boolean, description: text = "") {
    val me = require_provider(my_pubkey);
    require_provider_auth_with_rate_limit(me);

    val other_prov = require_provider(provider_pubkey);

    // Only SP and NP can enable/disable providers
    require_node_access(me);

    if (roles.has_node_access(me) and other_prov.tier == provider_tier.DAPP_PROVIDER) {
        update_provider_state(other_prov, active);
    } else {
        require_system_access(me);
        require(empty(pending_provider_state @* { .provider == other_prov, .active == active, .proposal.state == proposal_state.PENDING } limit 1), "Already proposed");
        require(other_prov.active != active, "Provider is already %s".format(if (active) "active" else "not active"));
        val prop = create_proposal(proposal_type.provider_state, me, system_p_voter_set(), description);
        create pending_provider_state(prop, other_prov, .active = active);
        internal_vote(me, prop, true);
    }
}

function update_provider_state(provider, active: boolean) {
    if (active == false) {
        disable_provider(provider);
    } else { // enable
        enable_provider(provider);
    }
}

query get_provider_state_proposal(rowid) {
    val proposal = get_latest_proposal(rowid, proposal_type.provider_state);
    if (proposal == null) return null;
    val pps = pending_provider_state @ { proposal };
    return (
        provider = pps.provider.pubkey,
        provider_name = pps.provider.name,
        active = pps.active
    );
}</string>
                            </entry>
                            <entry key="proposal_voter_set/module.rell">
                                <string>module;

import ^.proposal.*;
import ^.model.*;</string>
                            </entry>
                            <entry key="proposal_voter_set/proposal_voter_set.rell">
                                <string>entity pending_voter_set_update {
    key proposal, voter_set;
}

namespace voter_set_update {
    entity threshold {
        key pending_voter_set_update;
        threshold: integer;
    }

    entity governor {
        key pending_voter_set_update;
        governor: voter_set;
    }

    entity new_member {
        index pending_voter_set_update;
        provider;
    }

    entity remove_member {
        index pending_voter_set_update;
        provider;
    }
}

@extend(apply_voting_result_handlers) function() = [proposal_type.voter_set_update.name: apply_voter_set_update(*)];

function apply_voter_set_update(proposal) {
    val vsp = pending_voter_set_update @? { proposal };
    if (vsp == null) return;
    val threshold = voter_set_update.threshold @? { vsp }.threshold;
    if (threshold != null) vsp.voter_set.threshold = threshold;
    val governor = voter_set_update.governor @? { vsp }.governor;
    if (governor != null) update voter_set_governance @ { .voter_set == vsp.voter_set } (.governor = governor);
    val new_member = voter_set_update.new_member @* { vsp }.provider;
    for (m in new_member) {
        create voter_set_member(vsp.voter_set, m);
    }
    val remove_member = voter_set_update.remove_member @* { vsp }.provider;
    for (m in remove_member) {
        delete voter_set_member @ { vsp.voter_set, m };
    }
}

@extend(delete_proposal_handlers) function(): map&lt;text, (proposal) -&gt; unit&gt; = [proposal_type.voter_set_update.name: delete_pending_voter_set_update(*)];

function delete_pending_voter_set_update(proposal) {
    val p = pending_voter_set_update @? { proposal };
    if (p == null) return;
    delete voter_set_update.threshold @? { p };
    delete voter_set_update.governor  @? { p };
    delete voter_set_update.new_member  @* { p };
    delete voter_set_update.remove_member  @* { p };
    delete p;
}

operation propose_update_voter_set(my_pubkey: pubkey, voter_set_name: text, new_threshold: integer?, new_governor: name?, new_member: list&lt;pubkey&gt;, remove_member: list&lt;pubkey&gt;, description: text = "") {
    val provider = require_is_provider_with_rate_limit(my_pubkey);
    val voter_set = require_voter_set(voter_set_name);
    require(voter_set.name != voter_sets.system_p, "Cannot update system voter set. Update this by proposing system provider role");
    require_voter_set_governor(voter_set, provider);
    require(empty(proposal @* { voter_set, proposal_state.PENDING }), "Cannot have more than one pending proposal involving this voter set.");

    val prop = create_proposal(
        proposal_type.voter_set_update,
        provider,
        voter_set_governance @ { voter_set }.governor,
        description
    );
    val update_prop = create pending_voter_set_update(prop, voter_set);
    val threshold_after_update = new_threshold ?: voter_set.threshold;
    require(threshold_after_update &gt;= -1 and threshold_after_update &lt;= (voter_set_member @* { voter_set }).size() + new_member.size() - remove_member.size(),
        "Invalid threshold level, must be in range [-1, voter_set.size()]");
    if (new_threshold != null) {
        create voter_set_update.threshold(update_prop, new_threshold);
    }
    if (new_governor != null) {
        val g = require_voter_set(new_governor);
        create voter_set_update.governor(update_prop, g);
    }
    for (m in new_member) {
        val p = require_provider(m);
        require(p.active, "Provider %s is not enabled".format(m));
        require(
            empty(voter_set_member @? { voter_set, .provider == p }),
            "%s is already a member of voter set %s".format(m, voter_set_name)
        );
        create voter_set_update.new_member(update_prop, p);
    }
    for (m in remove_member) {
        val p = require_provider(m);
        create voter_set_update.remove_member(update_prop, p);
    }
    internal_vote(provider, prop, true);
}

query get_voter_set_update_proposal(id: integer) {
    val p = proposal @? { rowid(id) };
    if (p == null) return null;
    val pvsu = pending_voter_set_update @? { p };
    if (pvsu == null) return null;
    return (
        voter_set = pvsu.voter_set.name,
        threshold = voter_set_update.threshold @? { pvsu }.threshold,
        governor = voter_set_update.governor @? { pvsu } .governor.name,
        add_member = voter_set_update.new_member @* { pvsu } .provider.pubkey,
        remove_member = voter_set_update.remove_member @* { pvsu } .provider.pubkey
    );
}
</string>
                            </entry>
                            <entry key="roles/module.rell">
                                <string>module;
</string>
                            </entry>
                            <entry key="roles/roles.rell">
                                <string>
import model.*;

namespace roles {

    function has_system_access(provider) = provider.active and provider.system;

    function has_node_access(provider) = provider.active and provider.tier == provider_tier.NODE_PROVIDER;

    function has_deploy_access(provider, container) = provider.active and exists(voter_set_member @? { provider, container.deployer });

}

function require_system_access(provider) = require(roles.has_system_access(provider), "Provider " + provider.pubkey + " must have system priviliges");
function require_node_access(provider) = require(roles.has_node_access(provider), "Provider " + provider.pubkey + " must have permissions to deploy nodes");
function require_deploy_access(provider, container) = require(roles.has_deploy_access(provider, container), "Provider " + provider.pubkey + " must have permissions to deploy in container " + container.name);

namespace enroll {
    function system(provider) {
        provider.active = true;
        provider.system = true;
        node(provider);
        create voter_set_member(provider, system_p_voter_set());
        create cluster_provider(provider, system_cluster());
    }

    function node(provider) {
        provider.tier = provider_tier.NODE_PROVIDER;
    }

}

namespace revoke {
    function system(provider) {
        provider.system = false;
        delete voter_set_member @ { provider, system_container().deployer};
        delete cluster_provider @ { provider, system_cluster() };
    }

    function node(provider) {
        require(roles.has_system_access(provider) != true, "Cannot revoke node access for a system provider. Revoke system access");
        provider.tier = provider_tier.DAPP_PROVIDER;
        // TODO inactivate all nodes?
    }

}
</string>
                            </entry>
                            <entry key="signer_list_update/module.rell">
                                <string>import lib.icmf.*;
import messaging.signer_list_update.*;

val signer_list_update_event_name = "signer_list_update_event";

object signer_list_update {
    mutable serial: integer = 1;
}

struct signer_list_update_event {
    serial: integer;
    blockchain_rid: byte_array;
    signers: list&lt;byte_array&gt;;
}

function signal_signer_list_update(blockchain_rid: byte_array, updated_signers: byte_array) {
    val update_message = signer_list_update_message(
        serial = signer_list_update.serial,
        blockchain_rid = blockchain_rid,
        signers = updated_signers,
        confirmed_in_directory_at_height = op_context.block_height
    ).to_gtv();
    send_message(signer_list_update_topic, update_message);

    op_context.emit_event(signer_list_update_event_name, signer_list_update_event(
        signer_list_update.serial,
        blockchain_rid,
        list&lt;byte_array&gt;.from_gtv(gtv.from_bytes(updated_signers))
    ).to_gtv());
    signer_list_update.serial++;
}
</string>
                            </entry>
                            <entry key="version.rell">
                                <string>module;

// Increment this everytime a query/op in directory chain is updated or changed
query api_version(): integer = 55;
</string>
                            </entry>
                        </dict>
                    </entry>
                    <entry key="strictGtvConversion">
                        <int>1</int>
                    </entry>
                    <entry key="version">
                        <string>0.13.13</string>
                    </entry>
                </dict>
            </entry>
        </dict>
    </entry>
    <entry key="icmf">
        <dict>
            <entry key="receiver">
                <dict>
                    <entry key="anchoring">
                        <dict>
                            <entry key="topics">
                                <array>
                                    <string>G_configuration_updated</string>
                                    <string>G_configuration_failed</string>
                                    <string>G_last_anchored_heights</string>
                                </array>
                            </entry>
                        </dict>
                    </entry>
                    <entry key="global">
                        <dict>
                            <entry key="topics">
                                <array>
                                    <string>G_create_cluster</string>
                                    <string>G_create_container</string>
                                    <string>G_upgrade_container</string>
                                    <string>G_stop_container</string>
                                    <string>G_restart_container</string>
                                    <string>G_register_dapp_provider</string>
                                    <string>G_change_dapp_providers_state</string>
                                </array>
                            </entry>
                        </dict>
                    </entry>
                </dict>
            </entry>
        </dict>
    </entry>
    <entry key="revolt">
        <dict>
            <entry key="fast_revolt_status_timeout">
                <int>2000</int>
            </entry>
            <entry key="revolt_when_should_build_block">
                <int>1</int>
            </entry>
        </dict>
    </entry>
    <entry key="signers">
        <array>
            <bytea>037434C8D4F2B7B7DE44E80486A814676DC3D898FD4488E10E1940B1C4C5837200</bytea>
        </array>
    </entry>
    <entry key="sync_ext">
        <array>
            <string>net.postchain.d1.icmf.IcmfReceiverSynchronizationInfrastructureExtension</string>
        </array>
    </entry>
</dict>
